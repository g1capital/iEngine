#LyX 1.6.2 created this file. For more info see http://www.lyx.org/
\lyxformat 345
\begin_document
\begin_header
\textclass article
\begin_preamble



\usepackage{amsfonts}
\end_preamble
\use_default_options false
\language english
\inputencoding auto
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\paperfontsize 10
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 0
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\author "" 
\author "" 
\end_header

\begin_body

\begin_layout Title
Motivated Decision-Making using Transformation Invariant Probability Estimates
\end_layout

\begin_layout Author
Ryan Michael
\begin_inset Newline newline
\end_inset

 
\family typewriter
kerinin@gmail.com
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Subsection
Existing Work
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Expectation-maximization_algorithm
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Stationary_process
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Ergodicity
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Mixing_(mathematics)
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Lyapunov_exponent
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Recurrence_quantification_analysis
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Recurrence_plot
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Autoregressive_fractionally_integrated_moving_averag
e
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Autocorrelation
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
url{http://en.wikipedia.org/wiki/Linear_discriminant_analysis}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Hidden Markov Model
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% cannot account for future states - only capable of prediction
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% weak, short-term memory
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Box-Jenkins
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% intended for simplistic processes with well-understood stationarity and
 periodicity
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% http://en.wikipedia.org/wiki/Box-Jenkins
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Spectral Analysis
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% assumes some type of frequency-domain decomposition.
  Frequency-domain signal representations do not do a very good job predicting
 time-domain values.
 
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Shrinking-
\begin_inset Formula $\epsilon$
\end_inset

 SVM Regression
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% theoretical foundation weak; only compensates for the relevance of recent
 data.
  See Markhov problem
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
General Overview
\end_layout

\begin_layout Standard
The goal is to create a method of statistical inference capable of processing
 multiplle sources of data which are both multi-variate and exhibits transformat
ion-invariant behaviors.
 This type of data is common, and developing a robust method of analysis
 has applications in many domains.
 
\end_layout

\begin_layout Standard
The most basic operation used in this system is the estimation of probability
 densities.
 Based on a set of observations drawn from some random process, we generate
 an estimate of the underlying probability distribution.
 This estimate tells us the probability of each point in the input space
 being observed.
 Areas of the input space in which a dense set of observations are observed
 are given high probability, while areas of the input space with few observation
s are given low probability.
\end_layout

\begin_layout Standard
We assume that observations are pulled from multiple independent sources,
 all of which respond to some underlying phenomena.
 For instance one set of observations could be from a microphone and another
 from a light detector.
 We do not know how the inputs are related internally, between inputs, or
 to what extent their behavior changes over time.
 For example one input could be a steady sine wave and another could be
 based on the decay of a radioactive isotope.
 In the former case previous observations of the source are useful, in the
 latter they're not.
 Alternately two inputs could be light detectors in the same room or they
 could be on different continents; in the former their input would be highly
 similar, in the latter not as much.
\end_layout

\begin_layout Section
Problem Setting
\end_layout

\begin_layout Standard
Our analysis take place in the context of a set of 
\begin_inset Formula $\ell$
\end_inset

 vector observations 
\begin_inset Formula $\vec{x}_{i}$
\end_inset

 of a hidden random variables 
\begin_inset Formula $X$
\end_inset

 with a real-valued event space of arbitrary dimension:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
X & =(\Omega,\mathcal{F},\mathcal{P})\\
\Omega & \in\mathbb{R}^{d}\\
X & =[\vec{x}_{1},...,\vec{x}_{\ell}]\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Our objective is to develop an estimate 
\begin_inset Formula $\varphi(x:\ X)$
\end_inset

 of the probability of a given point 
\begin_inset Formula $x$
\end_inset

 given a set of observations 
\begin_inset Formula $X$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\varphi(x:\ X) & \longmapsto\Pr(x|X)\pm\xi\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We assume that our estimate's error is upper-bounded by 
\begin_inset Formula $\xi$
\end_inset

.
 Methods exist to determine 
\begin_inset Formula $\xi$
\end_inset

; this is left as an exercise for the reader.
\end_layout

\begin_layout Section
Transformation Invariant Parzen Windows
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
General discussion of Parzen Windows
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Parzen Windows for i.i.d.
 Data
\end_layout

\begin_layout Standard
Our task is to estimate the probability of a given point 
\begin_inset Formula $\vec{x}$
\end_inset

 in the abstract space 
\begin_inset Formula $\Omega$
\end_inset

 based on a set of observations 
\begin_inset Formula $X$
\end_inset

.
 One method of accomplishing this is by using the Parzen Window method.
 We choose the Parzen Window method because it allows us to estimate probabiliti
es of unordered sets, provided they have an addition operation and a kernel
 function exists to provide a distance metric.
 The basic operation of the Parzen Window method is to estimate the probability
 of a point based on the sum of the distance from that point to each point
 in a set of prior observations.
 The Parzen Window approach to PDF estimation is as follows; given a set
 of prior observations 
\begin_inset Formula $X$
\end_inset

, a kernel function 
\begin_inset Formula $K_{\gamma}(\cdot,\cdot)$
\end_inset

 with width parameter 
\begin_inset Formula $\gamma$
\end_inset

, the probability of a point 
\begin_inset Formula $x$
\end_inset

 is determined by :
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\varphi(\vec{x}:\ X) & =\sum_{i}^{\ell}\frac{1}{\ell}K_{\gamma}(\vec{x},\vec{x}_{i})\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Multiple kernel functions exist, in this paper we will use the Radial Basis
 Function (RBF) kernel:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
K_{\gamma}\left(\vec{x},\vec{y}\right) & =\prod_{\upsilon=1}^{d}\frac{1}{\gamma\sqrt{2\pi}}e^{-\frac{1}{\gamma}\|x^{\nu},y^{\nu}\|^{2}}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset Formula $\|\cdot,\cdot\|$
\end_inset

 is some metric, for instance the 
\begin_inset Formula $L^{2}$
\end_inset

 distance:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\|x,y\| & =\left(x-y\right)^{2}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
This approach is useful in the context of a set of observations drawn i.i.d.
 from a statid probability density function (PDF), however it is unable
 to address transformation invariance.
 Transformation invariance (TI) refers to datasets in which one or multiple
 PDF's occur in various transformed states within the dataset, for example
 a given sound (described by a known PDF) could be repeated multiple times
 in an audio recording, or a given image could appear in multiple locations
 in an image.
 While the Parzen Window method can generate an estimate in the presence
 of data generated by transformed PDF's, its inability to recognize multiple
 instances of a single PDF limits its power.
 
\end_layout

\begin_layout Subsection
Contextual Estimation
\end_layout

\begin_layout Standard
The crucial distinction between i.i.d data from a static PDF and data which
 exhibits transformational invariance is that when estimating the probability
 of a point, you must consider to context of the point as well as the location
 of the point in 
\begin_inset Formula $\Omega$
\end_inset

.
 In this paper we will establish the context of 
\begin_inset Formula $x$
\end_inset

 by defining a 'neighborhood' of points around 
\begin_inset Formula $x$
\end_inset

.
 We will do so through the use of a windowing kernel function 
\begin_inset Formula $\omega(\cdot)$
\end_inset

 which returns a multiplier to describe the distance between an arbitrary
 point and 
\begin_inset Formula $x$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\omega_{\alpha}(x_{i},x_{j}) & \Rightarrow[0,\infty)\\
\int_{-\infty}^{\infty}\omega_{\alpha}(x_{i},x_{j})dx_{i} & =1\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We can use any kernel function for 
\begin_inset Formula $\omega(\cdot)$
\end_inset

, however we assume that the windowing function has a peak at 
\begin_inset Formula $\Delta=0$
\end_inset

 and that it drops off as 
\begin_inset Formula $\Delta\rightarrow\pm\infty$
\end_inset

.
\end_layout

\begin_layout Subsection
Parzen Windows for Transformation-Invariant Data
\end_layout

\begin_layout Standard
Because we must establish a context for TI data, we must think of the set
 of observations 
\begin_inset Formula $X$
\end_inset

 as part of both the problem definition and the solution.
 This means that we can no longer simply calculate the kernel distance between
 
\begin_inset Formula $x$
\end_inset

 and each of the observations in 
\begin_inset Formula $X$
\end_inset

 independantly - we must compare the 
\emph on
context
\emph default
 of 
\begin_inset Formula $x$
\end_inset

 with 
\begin_inset Formula $X$
\end_inset

.
 This requires a kernel function capable of comparing two 
\emph on
sets of points
\emph default
.
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
\begin_inset Formula \begin{align*}
K_{\gamma}\left(X_{i},X_{j}\right) & =\prod_{\upsilon=1}^{d}\frac{1}{\gamma\sqrt{2\pi}}e^{-\frac{1}{\gamma}\|X_{i}^{\nu},X_{j}^{\nu}\|}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We can simplify this equation by observing that we are not actually dealing
 with two sets; we're dealing with the a set of observations 
\begin_inset Formula $X$
\end_inset

, a point whose probability we'd like to estimate 
\begin_inset Formula $x$
\end_inset

, and a neighborhood defined by 
\begin_inset Formula $x$
\end_inset

 and some parameter 
\begin_inset Formula $\alpha$
\end_inset

:
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
\begin_inset Formula \begin{align*}
K_{\gamma}\left(\vec{x}_{n},\vec{x}_{m},X\right) & =\prod_{\upsilon=1}^{d}\frac{1}{\gamma\sqrt{2\pi}}e^{-\frac{1}{\gamma}\|x_{n}^{\nu},x_{m}^{\nu},X\|}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Notice we've added 
\begin_inset Formula $\alpha$
\end_inset

 to the distance metric to allow each point in 
\begin_inset Formula $X$
\end_inset

 to contribute based on it's proximity to the neighborhood of 
\begin_inset Formula $x$
\end_inset

 rather than attempting to explicitly define the neighborhood as a discrete
 set.
 This raises the question not only of how each point's influence is modified
 but of how we define the distance between two sets of observations.
 There are multiple divergence measures available which can be used to define
 a 'distance' between two sets.
 We will later show the importance of the distance metric 
\begin_inset Formula $\|\cdot,\cdot\|$
\end_inset

 being integrable; for this reason we will use the Pearson Divergence as
 our distance metric when comparing sets.
 In order to acommodate TI comparisons, we add a linear transform 
\begin_inset Formula $(\mathbf{A},\mathbf{b})$
\end_inset

 to the windowed set.
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\|X^{\nu},Y^{\nu}\| & =\sum_{x^{\nu}\in\{X\cup Y\}}\left(\frac{\varphi(x^{\nu}:\ \mathbf{A}^{\nu}X^{\nu}+\mathbf{b}^{\nu})}{\varphi(x^{\nu}:\ Y^{\nu})}-1\right)^{2}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Notice that we are transforming the windowed set of data points 
\emph on
but
\emph default
 
\emph on
not
\emph default
 
\begin_inset Formula $x^{v}$
\end_inset

.
 What we're trying to do is match transformed points in the windowed set
 to existing points in the full set - this tells us the 'distance' between
 the windowed set and the full set.
 We also want to establish how adding the point 
\begin_inset Formula $x^{\nu}$
\end_inset

 to the windowed set affects the correlation between the two sets (this
 is how we determine the probability of 
\begin_inset Formula $x^{\nu}$
\end_inset

).
 We're only testing a discrete subset possible values of 
\begin_inset Formula $x^{\nu}$
\end_inset

 (we can't integrate over all possible points because resulting equation
 isn't integrable), and we want to make sure that when we calculate the
 difference measure we use a 'good' set of points .
 Since we're assuming the windowed data contains all the points in 
\begin_inset Formula $X$
\end_inset

 this shouldn't be a problem, and using the points in 
\begin_inset Formula $X$
\end_inset

 to test the distance between the windowed set and 
\begin_inset Formula $X$
\end_inset

 should give us an accurate measure of the correlation between the two.
\end_layout

\begin_layout Standard
Because the Pearson Divergence is a summation, we can control the influence
 of each point by scaling it using the windowing function:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\|x_{n}^{\nu},x_{m}^{\nu},X\| & =\sum_{i=1}^{\ell}\omega_{\alpha}(x_{n}^{\nu},x_{i}^{\nu})\left(\frac{\varphi(x_{i}^{\nu}:\ x_{m}^{\nu},\mathbf{A}X+\mathbf{b})}{\varphi(x_{i}^{\nu}:\ x_{m}^{\nu},X^{\nu})}-1\right)^{2}\\
\varphi(x^{\nu}:\ x_{n}^{\nu},X) & =\sum_{i=1}^{\ell}\frac{\omega_{\alpha}(x_{i}^{\nu},x_{n}^{\nu})}{\sum_{j=1}^{\ell}\omega_{\alpha}(x_{j}^{\nu},x_{n}^{\nu})}K_{\gamma}(x^{\nu},x_{i}^{\nu})\\
 & =\sum_{i=1}^{\ell}\ddot{\omega}_{\alpha}(x_{i}^{\nu},x_{n}^{\nu},X)\ K_{\gamma}(x^{\nu},x_{i}^{\nu})\end{align*}

\end_inset


\end_layout

\begin_layout Standard
In both of these equations we have used 
\begin_inset Formula $(\mathbf{A},\mathbf{b})$
\end_inset

 without commenting on their definition.
 These two matrix transformations are used to shift, scale, rotate, shear
 or mirror 
\begin_inset Formula $\{x\cup X\}$
\end_inset

 prior to comparing it to 
\begin_inset Formula $X$
\end_inset

.
 The first transformation matrix 
\begin_inset Formula $\mathbf{A}$
\end_inset

 is an 
\begin_inset Formula $(\bar{d}+d_{x})\times(\bar{d}+d_{x})$
\end_inset

 matrix.
 The linear operator 
\begin_inset Formula $\mathbf{A}X$
\end_inset

 allows us to scale, rotate, shear, and mirror 
\begin_inset Formula $X$
\end_inset

 depending on the matrix values of 
\begin_inset Formula $\mathbf{A}$
\end_inset

.
 The second transformation amtrix 
\begin_inset Formula $\mathbf{b}$
\end_inset

 is a 
\begin_inset Formula $(\bar{d}+d_{x})\times1$
\end_inset

 matrix; adding these terms together allows us to shift 
\begin_inset Formula $X$
\end_inset

 along any axis based on the values of 
\begin_inset Formula $\mathbf{b}$
\end_inset

.
 One approach to these transformations is to explicitly determine values
 for the two matrices and to determine the distance 
\begin_inset Formula $\|\cdot,\cdot\|$
\end_inset

 using these values.
 A more robust approach is to integrate over all possible values of 
\begin_inset Formula $(\mathbf{A},\mathbf{b}):$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\|x_{n}^{\nu},x_{m}^{\nu},X:\ \alpha\| & \sum_{x_{i}^{\nu}\in\{x\cup X\}}\omega_{\alpha}(x_{n}^{\nu},x_{i}^{\nu})\int\int_{-\infty}^{\infty}\left(\frac{\varphi(x_{i}^{\nu}:\ x_{m}^{\nu},\mathbf{A}X+\mathbf{b})}{\varphi(x_{i}^{\nu}:\ x_{m}^{\nu},X^{\nu})}-1\right)^{2}d\mathbf{A}d\mathbf{b}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
This approach allows us to compare the distance between any linear transform
 of 
\begin_inset Formula $\{x\cup X\}$
\end_inset

 and 
\begin_inset Formula $X$
\end_inset

 - a far more powerful and less computationally demanding approach.
 Doing so requires that we calculate the following integral (in which we
 assume 
\begin_inset Formula $x\in\mathbb{R}^{1}$
\end_inset

 and 
\begin_inset Formula $X\rightarrow[|X|,1]$
\end_inset

] ):
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\int\int_{-\infty}^{\infty}\left(\frac{\varphi(x_{i}^{\nu}:\ x_{m}^{\nu},\mathbf{A}X+\mathbf{b})}{\varphi(x_{i}^{\nu}:\ x_{m}^{\nu},X^{\nu})}-1\right)^{2}d\mathbf{A}d\mathbf{b}\]

\end_inset


\end_layout

\begin_layout Standard
Because 
\begin_inset Formula $(\mathbf{A},\mathbf{b})$
\end_inset

 is a pair of matrix transformation, we must integrate the previous equation
 element-wise:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\int\int_{-\infty}^{\infty}\left(\frac{\varphi(x^{\nu}:\ x_{n}^{\nu},\mathbf{A}X+\mathbf{b})}{\varphi(x^{\nu}:\ x_{n}^{\nu},X)}-1\right)^{2}d\mathbf{A}d\mathbf{b} & =\int...\int\left(\frac{\varphi\left(x^{\nu}:\ x_{n}^{\nu},\left[\mathbf{b}_{\nu}+\sum_{n=1}^{d}\mathbf{A}_{\nu,n}x_{i}^{n}|\ x_{i}\in X\right]\right)}{\varphi(x^{\nu}:\ x_{n}^{\nu},X)}-1\right)^{2}d\mathbf{A}_{0}...d\mathbf{A}_{d}d\mathbf{b}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
 & =\int...\int\left(\frac{\sum_{i=1}^{\ell}\ddot{\omega}_{\alpha}(x_{i}^{\nu},x_{n}^{\nu},X)\ K_{\gamma}\left(x^{\nu},\left(\mathbf{b}_{\nu}+\sum_{n=1}^{d}\sum_{i=1}^{\ell}\mathbf{A}_{\nu,n}x_{i}^{n}\right)\right)}{\sum_{i}^{\ell}\ddot{\omega}_{\alpha}(x_{i}^{\nu},x_{n}^{\nu},X)\ K_{\gamma}(x^{\nu},X)}-1\right)^{2}d\mathbf{A}_{0}...d\mathbf{A}_{d}d\mathbf{b}\\
 & =\int...\int\left(\frac{\sum_{i=1}^{\ell}\ddot{\omega}_{\alpha}(x_{i}^{\nu},x_{n}^{\nu},X)}{\sum_{i=1}^{\ell}\ddot{\omega}_{\alpha}(x_{i}^{\nu},x_{n}^{\nu},X)\ \exp\left(-\frac{1}{\gamma}\left(x^{\nu}-x_{i}^{\nu}\right)^{2}\right)}\ \exp\left(-\frac{1}{\gamma}\left(x^{\nu}-\left(\mathbf{b}_{\nu}+\sum_{n=1}^{d}\sum_{i=1}^{\ell}\mathbf{A}_{\nu,n}x_{i}^{n}\right)\right)^{2}\right)-1\right)^{2}d\mathbf{A}_{0}...d\mathbf{A}_{d}d\mathbf{b}\\
 & =\int...\int\left(\frac{\sum_{i=1}^{\ell}\ddot{\omega}_{\alpha}(x_{i}^{\nu},x_{n}^{\nu},X)}{\sum_{i=1}^{\ell}\ddot{\omega}_{\alpha}(x_{i}^{\nu},x_{n}^{\nu},X)\ \exp\left(-\frac{1}{\gamma}\left(x^{\nu}-x_{i}^{\nu}\right)^{2}\right)}\ \exp\left(-\frac{1}{\gamma}\left(\left(x^{\nu}\right)^{2}-2x^{\nu}\left(\mathbf{b}_{\nu}+\sum_{n=1}^{d}\sum_{i=1}^{\ell}\mathbf{A}_{\nu,n}x_{i}^{n}\right)+\left(\mathbf{b}_{\nu}+\sum_{n=1}^{d}\sum_{i=1}^{\ell}\mathbf{A}_{\nu,n}x_{i}^{n}\right)^{2}\right)\right)-1\right)^{2}d\mathbf{A}_{0}...d\mathbf{A}_{d}d\mathbf{b}\\
 & =\int...\int\left(\frac{\sum_{i=1}^{\ell}\ddot{\omega}_{\alpha}(x_{i}^{\nu},x_{n}^{\nu},X)}{\sum_{i=1}^{\ell}\ddot{\omega}_{\alpha}(x_{i}^{\nu},x_{n}^{\nu},X)\ \exp\left(-\frac{1}{\gamma}\left(x^{\nu}-x_{i}^{\nu}\right)^{2}\right)}\ \exp\left(-\frac{1}{\gamma}\left(\left(x^{\nu}\right)^{2}-2x^{\nu}\mathbf{b}_{\nu}+2x^{\nu}\sum_{n=1}^{d}\sum_{i=1}^{\ell}\mathbf{A}_{\nu,n}x_{i}^{n}+\mathbf{b}_{\nu}^{2}+\mathbf{b}_{\nu}\sum_{n=1}^{d}\sum_{i=1}^{\ell}\mathbf{A}_{\nu,n}x_{i}^{n}+\sum_{\begin{array}{c}
n=1\\
m=1\end{array}}^{d}\sum_{\begin{array}{c}
i=1\\
j=1\end{array}}^{\ell}\mathbf{A}_{\nu,n}x_{i}^{n}\mathbf{A}_{\nu,m}x_{j}^{m}\right)\right)-1\right)^{2}d\mathbf{A}_{0}...d\mathbf{A}_{d}d\mathbf{b}\\
 & =\int...\int\left(\frac{\sum_{i=1}^{\ell}\ddot{\omega}_{\alpha}(x_{i}^{\nu},x_{n}^{\nu},X)}{\sum_{i=1}^{\ell}\ddot{\omega}_{\alpha}(x_{i}^{\nu},x_{n}^{\nu},X)\ \exp\left(-\frac{1}{\gamma}\left(x^{\nu}-x_{i}^{\nu}\right)^{2}\right)}\ \exp\left(-\frac{1}{\gamma}\left(\sum_{\begin{array}{c}
n=1\\
m=1\end{array}}^{d}\sum_{\begin{array}{c}
i=1\\
j=1\end{array}}^{\ell}\mathbf{A}_{\nu,n}x_{i}^{n}\mathbf{A}_{\nu,m}x_{j}^{m}+\left(x^{\nu}\right)^{2}+\sum_{n=1}^{d}\sum_{i=1}^{\ell}(2x^{\nu}+\mathbf{b}_{\nu})\mathbf{A}_{\nu,n}x_{i}^{n}-2x^{\nu}\mathbf{b}_{\nu}+\mathbf{b}_{\nu}^{2}\right)\right)-1\right)^{2}d\mathbf{A}_{0}...d\mathbf{A}_{d}d\mathbf{b}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Finish this up
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Extend this to integrate over a subset of 
\begin_inset Formula $d$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Multiple Data Sources
\end_layout

\begin_layout Standard
Consider the case where data from multiple data sources contributes to our
 set of observations 
\begin_inset Formula $X$
\end_inset

, for instance the data drawn from a microphone and a video camera.
 Let us assume that both input sources are timestamped, but the sampling
 frequency is different for each source and that the vector data points
 have different dimensionality, we will refer to observations of the former
 as 
\begin_inset Formula $X=(\Omega^{X},\mathcal{F}^{X},\mathcal{P}^{X})$
\end_inset

 and the latter as 
\begin_inset Formula $Y=(\Omega^{Y},\mathcal{P}^{Y},\mathcal{F}^{Y})$
\end_inset

.
 In this situation we can treat the timestamp as a 'shared' dimension, but
 all other dimensions of the two vectors are independant.
 It is clear that if we intend to establish a PDF of the joint probability
 space 
\begin_inset Formula $(\Omega^{Y,X},\mathcal{P}^{Y,X},\mathcal{F}^{Y,X})$
\end_inset

, we must treat each dimension in 
\begin_inset Formula $\Omega^{X}$
\end_inset

 and 
\begin_inset Formula $\Omega^{Y}$
\end_inset

 as orthonormal to each other.
 The fact that both event spaces share a time dimension means that a TI
 analysis over the time dimension is likely to produce useful results.
 
\end_layout

\begin_layout Standard
The most straightforward way to handle this situation is to assume that
 vector observations constitute sparse matrices; for any given dimension
 of an observation 
\begin_inset Formula $\vec{x}$
\end_inset

, the value can either be a real number or null:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\vec{x}^{v} & \in[\mathbb{R},\textrm{Ø}]\end{align*}

\end_inset

 In this case we only include in our analysis operations between real-valued
 dimensions.
 We can easily accomodate the shared time dimension by including the two
 time dimensions in our TI analysis.
 This allows us to consider not only the situation where both inputs are
 timestamped with accurate clocks, but the situation where the two clocks
 are independantly inaccurate.
\end_layout

\begin_layout Subsection
Single Channel Summary
\end_layout

\begin_layout Standard
We have developed a PDF estimation technique which allows us to take advantage
 of TI data.
 The solution uses a novel distance metric to compare the similarity between
 two sets of points in the context of arbitrary linear transformations of
 one set.
 The solution proposed is restricted to observations with shared dimensionality,
 however it imposes no restrictions on which dimensions are TI.
 The types of transformations considered by the proposed solution are restricted
 to linear matrix transformations shifts.
 In the next section we will extend the solution to cases where multiple
 'channels' of data are present allowing us to handle TI between observations
 with non-shared dimensions.
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\varphi(\vec{x}:\ X) & =\prod_{\upsilon=1}^{d}\sum_{i=1}^{|X|}\frac{1}{|X|}\ K_{\gamma}(x^{\nu},x_{i}^{\nu},X^{\nu})\\
K_{\gamma}\left(x_{n}^{\nu},x_{m}^{\nu},X^{\nu}\right) & =\frac{1}{\gamma\sqrt{2\pi}}e^{-\frac{1}{\gamma}\|x_{n}^{\nu},x_{m}^{\nu},X^{\nu}\|}\\
\|x_{n}^{\nu},x_{m}^{\nu},X^{\nu}\| & =\sum_{i=1}^{\ell}\omega_{\alpha}(x^{\nu},x_{i}^{\nu})\qquad???\\
\ddot{\omega}_{\alpha}(x_{i}^{\nu},x_{n}^{\nu},X) & =\frac{\omega_{\alpha}(x_{i}^{\nu},x_{n}^{\nu})}{\sum_{j=1}^{\ell}\omega_{\alpha}(x_{j}^{\nu},x_{n}^{\nu})}\end{align*}

\end_inset


\end_layout

\begin_layout Section
Support Vector Optimizations
\end_layout

\begin_layout Standard
The Parzen Window method is neither sparse nor computationally efficient,
 and as the number of observations grows, these deficiencies quickly become
 prohibitive.
 We now investigate the use of Support Vector Machines to generate estimates.
\end_layout

\begin_layout Subsection
Generalized Parzen-SVM 
\end_layout

\begin_layout Standard
Support Vector Machines are often used
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
ref?
\end_layout

\end_inset

 to estimate probability distributions by solving the related problem of
 estimating the cumulative distribution function of the random variable
 in question.
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
This isn't really the essence of SVM - put more verbage in the intro regarding
 what SVM's are, why they work, and why they're superior to other approaches
 (say, neural networks)
\end_layout

\end_inset

 This reduces the problem to one of estimating a non-linear mapping from
 observations to cumulative distribution values, which can be formulated
 as an optimization problem over a linear operator equation.
 Unfortunately, these methods depend on the ability to calculate an empirical
 distribution for each observation:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
F_{\ell}(x)=\frac{1}{\ell}\sum_{i}\theta(x-x_{i})\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\theta(x)$
\end_inset

 is the indicator function.
 To evaluate this function, the abstract space 
\begin_inset Formula $\Omega^{\mathcal{S}}$
\end_inset

 must be ordered.
 While we have described a distance metric over 
\begin_inset Formula $\Omega^{\mathcal{S}}$
\end_inset

, it is not clear what a meaningful ordering relation would be.
\end_layout

\begin_layout Standard
Rather than calculating the cumulative probability distribution of 
\begin_inset Formula $X$
\end_inset

, we begin with the assumption that the Parzen Window estimate of the probabilit
y distribution is acceptably accurate and attempt to minimize the difference
 between the Support Vector estimate and the Parzen Window estimate.
 In this spirit, we will use a modification of the Parzen Window estimator
 which substitutes a set of weights 
\begin_inset Formula $\beta$
\end_inset

 for the normalizing contant 
\begin_inset Formula $\frac{1}{|X|}$
\end_inset

 in the Parzen Window equation:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\varphi(\vec{x}:\ \beta,X) & =\prod_{\upsilon=1}^{d}\sum_{i=1}^{\ell}\beta_{i}K_{\gamma}(x^{\nu},x_{i}^{\nu},X^{\nu})\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The support vector approach requires that we define an optimization problem
 
\begin_inset Formula $W(\beta)$
\end_inset

 to determine the value of 
\begin_inset Formula $\beta$
\end_inset

.
 This optimization problem will determine which observations (or as we shall
 see, windows) will be used in estimations and which can be discarded as
 redundant or irrelevant information; the result of the optimization problem
 will be that a substantial number of multipliers 
\begin_inset Formula $\beta_{i}$
\end_inset

 will be 
\begin_inset Formula $0$
\end_inset

, allowing us to omit the windows defined by these points to be ignored
 in the prediction phase.
 We define the optimization problem as minimizing the square loss between
 the Support Vector and Parzen Window estimates over some set of observations
 
\begin_inset Formula $X$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
W(\beta:\ X) & \longmapsto\min_{\beta}\sum_{i=1}^{\ell}\left(\varphi(\vec{x}_{i}:\ X)-\varphi(\vec{x}_{i}:\ \beta,X)\right)^{2}+\beta\Omega(\lambda,X)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The optimization equation compares the results of Parzen estimation to the
 results of SVM estimation using a portion of the training data as synthetic
 testing data.
 For the optimization problem, we check the difference between the two estimates
 at windows defined by the observations 
\begin_inset Formula $X$
\end_inset

.
 The set of weights used in the Support Vector estimation must have a discrete
 number of elements; for simplicity we choose to assign a weight to each
 window defined by the time value of an observation in the training set
 and the constant parameter 
\begin_inset Formula $\alpha$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\sum_{i=1}^{\ell}\left(\varphi(x_{i}:\ X)-\varphi(x_{i}:\ \beta,X)\right)^{2} & =\sum_{i=1}^{\ell}\left(\varphi(x_{i}:\ X)\right)^{2}-2\left(\varphi(x_{i}:\ X)\varphi(x_{i}:\ \beta,X)\right)+\left(\varphi(x_{i}:\ \beta,X)\right)^{2}\\
 & =\sum_{i=1}^{\ell}\left(\left(\sum_{j=1}^{\ell}\frac{1}{\ell}K_{\gamma}(x_{i},x_{j},X)\right)^{2}-2\left(\sum_{j=1}^{\ell}\frac{1}{\ell}K_{\gamma}(x_{i},x_{j},X)\right)\left(\sum_{j=1}^{\ell}\beta_{j}K_{\gamma}(x_{i},x_{j},X)\right)+\left(\sum_{j=1}^{\ell}\beta_{j}K_{\gamma}(x_{i},x_{j},X)\right)^{2}\right)\\
 & =\sum_{i=1}^{\ell}\left(\sum_{\begin{array}{c}
j=1\\
k=1\end{array}}^{\ell}\frac{1}{\ell^{2}}K_{\gamma}(x_{i},x_{j},X)K_{\gamma}(x_{i},x_{k},X)-2\sum_{\begin{array}{c}
j=1\\
k=1\end{array}}^{\ell}\frac{1}{\ell}K_{\gamma}(x_{i},x_{j},X)\beta_{j}K_{\gamma}(x_{i},x_{k},X)+\sum_{\begin{array}{c}
j=1\\
k=1\end{array}}^{\ell}\beta_{j}\beta_{k}K_{\gamma}(x_{i},x_{j},X)K_{\gamma}(x_{i},x_{k},X)\right)\\
 & =\sum_{\begin{array}{c}
i=1\\
j=1\\
k=1\end{array}}^{\ell}\beta_{j}\beta_{k}K_{\gamma}(x_{i},x_{j},X)K_{\gamma}(x_{i},x_{k},X)-\sum_{\begin{array}{c}
i=1\\
j=1\\
k=1\end{array}}^{\ell}\frac{2\beta_{j}}{\ell}K_{\gamma}(x_{i},x_{j},X)K_{\gamma}(x_{i},x_{k},X)+\sum_{\begin{array}{c}
i=1\\
j=1\\
k=1\end{array}}^{\ell}\frac{1}{\ell^{2}}K_{\gamma}(x_{i},x_{j},X)K_{\gamma}(x_{i},x_{k},X)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Because this is an minimization problem we can eliminate the last term (changing
 
\begin_inset Formula $\beta$
\end_inset

 won't affect its value).
 Substituting our optimization problem becomes:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
W(\beta:\ X) & \longmapsto\min_{\beta}\sum_{\begin{array}{c}
i=1\\
j=1\\
k=1\end{array}}^{\ell}\beta_{j}\beta_{k}K_{\gamma}(x_{i},x_{j},X)K_{\gamma}(x_{i},x_{k},X)-\sum_{\begin{array}{c}
i=1\\
j=1\\
k=1\end{array}}^{\ell}\frac{2\beta_{j}}{\ell}K_{\gamma}(x_{i},x_{j},X)K_{\gamma}(x_{i},x_{k},X)+\lambda\Omega(\beta,X)\\
\text{subject to} & \beta_{i}\ge0,\ \sum\beta=1\end{align*}

\end_inset


\end_layout

\begin_layout Subsection
Quadratic Optimization Problem
\end_layout

\begin_layout Subsection
Support Vector Decomposition 
\end_layout

\begin_layout Standard
Due to the simplicity of the constraints in the optimization problem, it
 is possible to use the decomposition method of Osuna to reduce the memory
 requirements of the Parzen-SVM algorithm.
\end_layout

\begin_layout Subsubsection
 Sub-Problem Definition 
\end_layout

\begin_layout Standard
The decomposition algorithm breaks 
\begin_inset Formula $X$
\end_inset

 into two working sets 
\begin_inset Formula $B,N$
\end_inset

, and attempts to optimize 
\begin_inset Formula $B$
\end_inset

 while keeping 
\begin_inset Formula $N$
\end_inset

 fixed.
 This results in the following iterative optimization problem where 
\begin_inset Formula $\boldsymbol{\beta}^{k}$
\end_inset

 denotes the result of the previous iteration:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
W(\boldsymbol{\beta}_{B}) & =\frac{1}{2}\begin{bmatrix}\boldsymbol{\beta}_{B}^{T} & (\boldsymbol{\beta}_{N}^{k})^{T}\end{bmatrix}\begin{bmatrix}P_{BB} & P_{BN}\\
P_{NB} & P_{NN}\end{bmatrix}\begin{bmatrix}\boldsymbol{\beta}_{B}\\
\boldsymbol{\beta}_{N}^{k}\end{bmatrix}-\begin{bmatrix}q_{B}^{T} & q_{N}^{T}\end{bmatrix}\begin{bmatrix}\boldsymbol{\beta}_{B}\\
\boldsymbol{\beta}_{N}^{k}\end{bmatrix}\\
 & =\frac{1}{2}\boldsymbol{\beta}_{B}^{T}P_{BB}\boldsymbol{\beta}_{B}-(-q_{B}+P_{BN}\boldsymbol{\beta}_{N}^{k})^{T}\boldsymbol{\beta}_{B}\\
 & =\frac{1}{2}\begin{bmatrix}\beta_{i} & \beta_{j}\end{bmatrix}\begin{bmatrix}P_{ii} & P_{ij}\\
P_{ij} & P_{jj}\end{bmatrix}\begin{bmatrix}\beta_{i}\\
\beta_{j}\end{bmatrix}-(-q_{B}+P_{BN}\boldsymbol{\beta}_{N}^{k})^{T}\begin{bmatrix}\beta_{i}\\
\beta_{j}\end{bmatrix}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\text{subject to}\quad0\le\beta_{i},\beta_{j},\quad\beta_{i}+\beta_{j}=1-\mathbf{1}^{T}\boldsymbol{\beta}_{N}^{k}\end{equation}

\end_inset


\end_layout

\begin_layout Subsubsection
 Working Set Selection 
\end_layout

\begin_layout Standard
Select
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align}
 & i\in\text{arg}\max_{t}\left\{ -\nabla f(\boldsymbol{\beta}^{k})_{t}\ |\quad t\in I(\boldsymbol{\beta}^{k})\right\} \\
 & j\in\text{arg}\min_{t}\left\{ -\frac{b_{it}^{2}}{a_{it}}\ |\quad t\in I(\boldsymbol{\beta}^{k}),\quad-\nabla f(\boldsymbol{\beta}^{k})_{t}<-\nabla f(\boldsymbol{\beta}^{k})_{i}\right\} \end{align}

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% This needs to be checked for the new optimization scenario
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align}
I(\boldsymbol{\beta}) & \equiv\{t\ |\quad\beta_{t}<1\quad\text{or}\quad\beta_{t}>0\}\\
 & a_{it}=P_{ii}+P_{tt}-2P_{it}\\
 & \bar{a}_{it}=\begin{cases}
a_{it} & \text{if}\ a_{it}>0\\
\delta & \text{otherwise}\end{cases}\\
 & b_{it}=-\nabla f(\boldsymbol{\beta}^{k})_{i}+\nabla f(\boldsymbol{\beta}^{k})_{t}\\
\nabla f(\boldsymbol{\beta})_{i} & \equiv P_{i}\boldsymbol{\beta}-q_{i}\end{align}

\end_inset


\end_layout

\begin_layout Subsubsection
 Stopping Condition 
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\max_{i\in I(\boldsymbol{\alpha}^{k})}-\nabla f(\boldsymbol{\alpha})_{i}+\min_{j\in I(\boldsymbol{\alpha}^{k})}\nabla f(\boldsymbol{\alpha})_{j}\le\epsilon\end{equation}

\end_inset


\end_layout

\begin_layout Section
Transformation Invariance
\end_layout

\begin_layout Standard
Some datasets do not lend themselves to TI analysis, these datasets exhibit
 a single PDF with no similarity between the distribution at different locations
 along any axis.
 In order to understand when a TI analysis is useful we must develop a rigorous
 understanding of the problems TI solves and a method to quantify that problem
 for a given dataset.
\end_layout

\begin_layout Standard
Transformation invariance assumes that the context of an observation carries
 information relevant to the PDF of 
\begin_inset Formula $X$
\end_inset

 within that context.
 As such, we can say that the higher the expected information carried by
 a context, the more useful a TI analysis will be.
 This raises the question of how we determine the amount of information
 a given context contains 
\emph on
about the PDF within that context
\emph default
.
 In order for a context to contain information about itself, it must be
 similar to other contexts - we must have examples of similar behaviors
 from which we can generalize.
 So on the one hand, the more information we can extract from different
 contexts, the more likely we are to benefit from a TI analysis.
 On the other hand, if there are a relatively small number of divergent
 contexts, we can assume that the PDF of 
\begin_inset Formula $X$
\end_inset

 is not strongly contextualized, and therefore a TI analysis (based on contextua
l information) won't be particularly beneficial.
 These are the two factors which influence the relevance of a TI analysis;
 the variety of contexts and the overlap between contexts.
 In order to evaluate the relevance of TI analysis, we must develop orthogonal
 descriptions of these two factors.
 
\end_layout

\begin_layout Standard
The variety of contexts within a dataset can be described as the expected
 divergence between adjacent contexts:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
H_{\varepsilon}^{V}(X) & =\sum_{\begin{array}{c}
i=1\\
j=1\end{array}}^{\ell}\frac{1}{\ell^{2}}\omega_{\varepsilon}(x_{i},x_{j})\omega_{\alpha}(x_{i},x_{j})\|x_{i},x_{j},X\|\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The overlap between contexts can be described as the expected divergence
 between all possible contexts.
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
H_{\varepsilon}^{O}(X) & =\sum_{\begin{array}{c}
i=1\\
j=1\end{array}}^{\ell}\frac{1}{\ell^{2}}\omega_{\alpha}(x_{i},x_{j})\|x_{i},x_{j},X\|\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We would like the TI value to increase as either of these factors increase.
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
H_{\varepsilon}(X) & =H_{\varepsilon}^{V}(X)+H_{\varepsilon}^{O}(X)\\
 & =\sum_{\begin{array}{c}
i=1\\
j=1\end{array}}^{\ell}\frac{1}{\ell^{2}}\omega_{\varepsilon}(x_{i},x_{j})\omega_{\alpha}(x_{i},x_{j})\|x_{i},x_{j},X\|+\frac{1}{\ell^{2}}\omega_{\alpha}(x_{i},x_{j})\|x_{i},x_{j},X\|\\
 & =\sum_{\begin{array}{c}
i=1\\
j=1\end{array}}^{\ell}\frac{1}{\ell^{2}}\omega_{\alpha}(x_{i},x_{j})\|x_{i},x_{j},X\|\left(\omega_{\varepsilon}(x_{i},x_{j})+1\right)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
This can be extended to multi-dimensional data by observing that the window
 used previously is defined along a single dimension; we can just as easily
 consider a multi-dimensional window.
 In this case, we simply need to compare the expected divergence between
 adjacent windows in multiple dimensions and the expected divergence between
 all possible windows in multiple dimensions (here we use the 
\begin_inset Formula $L^{2}$
\end_inset

 metric):
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
H_{\varepsilon}^{V}(X) & =\sum_{\begin{array}{c}
i=1\\
j=1\end{array}}^{\ell}\sqrt{\sum_{\nu=1}^{d}\left(\frac{1}{\ell^{2}}\omega_{\varepsilon}(x_{i}^{\nu},x_{j}^{\nu})\omega_{\alpha}(x_{i}^{\nu},x_{j}^{\nu})\|x_{i}^{\nu},x_{j}^{\nu},X\|\right)^{2}}\\
H_{\varepsilon}^{O}(X) & =\sum_{\begin{array}{c}
i=1\\
j=1\end{array}}^{\ell}\sqrt{\sum_{\nu=1}^{d}\left(\frac{1}{\ell^{2}}\omega_{\alpha}(x_{i}^{\nu},x_{j}^{\nu})\|x_{i}^{\nu},x_{j}^{\nu},X\|\right)^{2}}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We can refine these to apply only to a subset of 
\begin_inset Formula $d$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
H_{\varepsilon}^{V}(X,D) & =\sum_{\begin{array}{c}
i=1\\
j=1\end{array}}^{\ell}\sqrt{\sum_{\nu\in D}\left(\frac{1}{\ell^{2}}\omega_{\varepsilon}(x_{i}^{\nu},x_{j}^{\nu})\omega_{\alpha}(x_{i}^{\nu},x_{j}^{\nu})\|x_{i}^{\nu},x_{j}^{\nu},X,D\|\right)^{2}}\\
H_{\varepsilon}^{O}(X,D) & =\sum_{\begin{array}{c}
i=1\\
j=1\end{array}}^{\ell}\sqrt{\sum_{\nu\in D}\left(\frac{1}{\ell^{2}}\omega_{\alpha}(x_{i}^{\nu},x_{j}^{\nu})\|x_{i}^{\nu},x_{j}^{\nu},X,D\|\right)^{2}}\\
D & =[d_{n},...,d_{m}],\ 1\le d_{i}<d\end{align*}

\end_inset


\end_layout

\begin_layout Section
Ensemble System
\end_layout

\begin_layout Standard
There are limits to the predictive ability of the current architecture.
 Most significantly, the context which influences a prediction is limited
 by the nature of 
\begin_inset Formula $\omega(\cdot,\cdot)$
\end_inset

.
 A more robust architecture would allow the context of a prediction to be
 more flexible, for example responding to two discrete windows while ignoring
 data outside those windows.
 A simple example of this is delayed causality; recognizing the relationship
 between an action and a delayed result requires that the system be capable
 of ignoring the intervening data.
 One solution to this limitation is to include multiple windows in the context
 of an estimate.
 In this case we would compare data drawn from multiple windows to data
 in a similar set of windows in 
\begin_inset Formula $X$
\end_inset

.
 
\end_layout

\begin_layout Standard
Another perspective in this limitation is that the estimates we generate
 (as well as the contexts from which we make the estimates) is limited by
 the nature of 
\begin_inset Formula $\omega(\cdot,\cdot)$
\end_inset

; it would be helpful for our estimates to make predictions about the probabilit
y of values in multiple windows of the abstract space 
\begin_inset Formula $\Omega^{X}$
\end_inset

.
 Taken together, we can see that a superior architecture would be capable
 of making estimates in the form of multiple windows using on observations
 
\emph on
based
\emph default
 on multiple windows.
 This can be accomplished by treating windows over 
\begin_inset Formula $X$
\end_inset

 as independant observations of the random variable 
\begin_inset Formula $Y$
\end_inset

.
 For any such window defined by a point 
\begin_inset Formula $D_{n}$
\end_inset

, we define the value of the point 
\begin_inset Formula $y_{m}^{n}$
\end_inset

 in the abstract space 
\begin_inset Formula $\Omega^{Y}$
\end_inset

as the divergence between 
\begin_inset Formula $X$
\end_inset

 in the window defined by 
\begin_inset Formula $D_{n}$
\end_inset

and the window defined by 
\begin_inset Formula $D_{m}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
y_{m}^{n} & =g(x_{m}:\ X)\\
g(x_{m}:\ X) & =\|x_{n},x_{m},X,D\|\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The dimensionality of 
\begin_inset Formula $\Omega^{Y}$
\end_inset

 is determined by the number of windows over 
\begin_inset Formula $X$
\end_inset

 which are used as observations of 
\begin_inset Formula $Y$
\end_inset

.
 We can define the value of each dimension of 
\begin_inset Formula $Y$
\end_inset

 at any point 
\begin_inset Formula $D_{i}$
\end_inset

 by determining the divergence between that dimension's corresponding window
 over 
\begin_inset Formula $X$
\end_inset

 and the window over 
\begin_inset Formula $X$
\end_inset

 defined by 
\begin_inset Formula $D_{i}$
\end_inset

.
 Because observations of 
\begin_inset Formula $Y$
\end_inset

 are derived from 
\begin_inset Formula $X$
\end_inset

, we must establish some method of choosing values of 
\begin_inset Formula $D$
\end_inset

 for which to derive observations; the nature of this method will depend
 on the nature of the data being analyzed and the computational resources
 available.
 We leave this for future investigation.
\end_layout

\begin_layout Standard
The generation of estimates of 
\begin_inset Formula $\mathcal{P}^{Y}$
\end_inset

 in a given window 
\begin_inset Formula $D$
\end_inset

 can be accomplished using the techniques already set forth, and in this
 sense the revised architecture is recursive.
 The estimator for each abstract space is referred to as a 'layer'; depending
 on the way estimates are used, the system can be seein as either a hierarchical
 system or a feed-forward system.
 We define a layer as such:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\varphi^{i+1}(g(x:\ X^{i}):\ X^{i+1}) & \longmapsto\Pr(g(x:\ X^{i})|X^{i+1})\pm\xi\\
X^{i+1} & =[g(x_{1}:\ X^{i}),...,g(x_{\ell^{i+1}}:\ X^{i})]\\
g(x:\ X^{i}) & =\left[\|x_{1},x,X^{i},D\|,...,\|x_{d^{i+1}},x,X^{i},D\|\right]\\
g(x:\ X^{i+1})^{j+1} & =g(g(x:\ X^{i})^{j}:\ X^{i+1})\\
g(x:\ X^{i})^{0} & =\|x_{n},x_{m},X,D\|\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We must therefore develop a method of translating estimates in the abstract
 space 
\begin_inset Formula $\Omega^{Y}$
\end_inset

 into estimates in the abstract space 
\begin_inset Formula $\Omega^{X}$
\end_inset

.
 One method is to use a feed-forward system, transforming a prediction 
\begin_inset Formula $y_{i}$
\end_inset

 into a PDF in 
\begin_inset Formula $\Omega^{X}$
\end_inset

.
 Because each dimension of 
\begin_inset Formula $y_{i}$
\end_inset

 is defined as a PDF in 
\begin_inset Formula $\Omega^{X}$
\end_inset

 and combining these PDF's is a trivial task.
 There are two weakness in this approach; it requries that the all the informati
on in a window 
\begin_inset Formula $D_{n}$
\end_inset

 over 
\begin_inset Formula $X$
\end_inset

 be used in generating values of 
\begin_inset Formula $y^{n}$
\end_inset

 and our predictions are limited by the sampling rate of 
\begin_inset Formula $Y$
\end_inset

.
 A more robust approach would allow the full use of the information contained
 in 
\begin_inset Formula $X$
\end_inset

 regardless of the amount of information transferred from 
\begin_inset Formula $X$
\end_inset

 to 
\begin_inset Formula $Y$
\end_inset

 (determined by the sampling rate and method of generating observations).
 A better approach is to use a hierarchical system in which we use some
 combination of predictions drawn from each layer of the architecture:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\varphi(x:\ X^{1},...,X^{\mu}) & =f\left(\varphi^{1}(x:\ X^{1}),...,\varphi^{\mu}(g(x:\ X^{\mu-1})^{\mu-1}:\ X^{\mu}):\ \lambda\right)\\
\int_{-\infty}^{\infty}f(x)dx & =1\\
0\le f(x)\le1 & \forall x\end{align*}

\end_inset


\end_layout

\begin_layout Standard
One way of doing this would be to simply take the product of each estimate
 weighted by some parameter to control the influence of different layers:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\varphi(x:\ X^{1},...,X^{\mu}) & =\prod_{n=1}^{\mu}\lambda^{n}\varphi^{n}(x:\ X^{n})\end{align*}

\end_inset


\end_layout

\begin_layout Standard
This system is not strictly hierarchical; we can determine the influence
 of each layer individually (potentially favoring lower layers over higher
 ones), however we refer to it as such because successive layers have an
 increased scope of data from which to make predictions, and each of these
 is eventually combined in making estimates.
 
\end_layout

\begin_layout Section
Motivated Decision-Making
\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Section
Old Work
\end_layout

\begin_layout Standard
We begin by considering the case where only one channel exists, so for now
 we will omit the superscript and refer to 
\begin_inset Formula $X^{n}=(\Omega^{n},\mathcal{F}^{n},\mathcal{P}^{n})$
\end_inset

 as 
\begin_inset Formula $X=(\Omega,\mathcal{F},\mathcal{P})$
\end_inset

.
 Given a set of training data 
\begin_inset Formula $Y$
\end_inset

, our task is to estimate a probability distribution based on a set of test
 data 
\begin_inset Formula $Z$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
Y & =\left[\vec{y}_{1},...,\vec{y}_{\ell}\right]\\
Z & =\left[\vec{z}_{1},...,\vec{z}_{\eta}\right]\end{align*}

\end_inset

For each observation in the training data we define a subset of 
\begin_inset Formula $Y$
\end_inset

 which contains all the observations which fall in a time window starting
 at the training observation and of duration greater than or equal to the
 duration of the test data and, shifted to the interval 
\begin_inset Formula $t\in[0,\theta)$
\end_inset

:
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
elaborate on what the time windows are and how they're denoted
\end_layout

\end_inset


\begin_inset Formula \begin{align}
\theta & \ge\max_{t}Z-\min_{t}Z\nonumber \\
S_{i} & =\left[(y_{j},t_{j}-t_{i})|\quad\mathbf{y}\in Y,t_{i}\le t_{j}<t_{j}+\theta\right]\label{eq:sSingle}\\
\mathcal{S} & =\left[S_{1},...,S_{\ell}\right]\nonumber \end{align}

\end_inset


\end_layout

\begin_layout Standard
We treat 
\begin_inset Formula $\mathcal{S}$
\end_inset

 as a random process:
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\mathcal{S}=[\Omega^{\mathcal{S}},\mathcal{F}^{\mathcal{S}},\mathcal{P}^{\mathcal{S}}]\]

\end_inset


\end_layout

\begin_layout Standard
and observe that 
\begin_inset Formula $Z$
\end_inset

 can be treated as an observation of 
\begin_inset Formula $\mathcal{S}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
S_{Z}=\left[\left(z_{i},z_{i}-\min_{t}Z\right)|\quad(x_{i},t_{i})\in Z\right]\label{eq:SySingle}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
We can now frame the task of estimating the probability distribution of
 
\begin_inset Formula $Z$
\end_inset

 as a task of estimating a probability density function 
\begin_inset Formula $\varphi$
\end_inset

 for the random process 
\begin_inset Formula $\mathcal{S}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\Pr(X=\mathbf{x}\ |\ Y\cup Z)\mapsto\ \Pr(\mathcal{S}=\{S_{Z}\cup\mathbf{x}\}|\quad Z)\ \simeq\ \varphi(\mathbf{x},S_{Z}:\ \mathcal{S})\label{eq:PrS}\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
Multiple Channel Setting
\end_layout

\begin_layout Standard
In order to extend this result to settings in which multiple channels exist,
 we return to 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:sSingle"

\end_inset

 and extend the scope to multiple channels:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
S_{i}=\left[(y_{j}^{n},t_{j}-t_{i})|\quad\mathbf{y}^{n}\in\mathbf{Y},t_{i}\le t_{j}<t_{j}+\theta\right]\label{eq:Smultiple}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
In this case, we treat each channel as an orthonormal basis of the abstract
 space 
\begin_inset Formula $\Omega^{\mathcal{S}}$
\end_inset

.
 Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:SySingle"

\end_inset

 is likewise extended in the same manner:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
S_{Z}=\left[\left(z_{i}^{n},z_{i}-\min_{t}Z\right)|\quad\mathbf{z}^{n}\in\mathbf{Z}\right]\label{eq:SyMultiple}\end{equation}

\end_inset


\end_layout

\begin_layout Section
Parzen Window Estimation
\end_layout

\begin_layout Standard
One method of evaluating 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:PrS"

\end_inset

 is by using the Parzen Window method.
 We choose the Parzen Window method because it allows us to estimate probabiliti
es of unordered sets, provided they have an addition operation and a kernel
 function exists to provide a distance metric.
\end_layout

\begin_layout Subsection
Single Channel Parzen Window
\end_layout

\begin_layout Standard
We will again begin by considering the single-channel case, then extend
 the resulting equations as necessary.
 The Parzen Window method requires the definition of a metric 
\begin_inset Formula $\|\cdot,\cdot\|$
\end_inset

 over the abstract space 
\begin_inset Formula $\Omega^{\mathcal{S}}$
\end_inset

.
 Such a metric can be defined using the Pearson divergence with probability
 measures 
\begin_inset Formula $\phi_{n}(\mathbf{x})\simeq Pr(X=\mathbf{x}\ |\ S_{n})$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align}
\|\phi_{n},\phi_{m}\|_{KL} & =\sum_{\mathbf{x}\in\{S_{n}\cup S_{m}\}}\left(\frac{\phi_{n}(\mathbf{x})}{\phi_{m}(\mathbf{x})}-1\right)^{2}\end{align}

\end_inset

 
\end_layout

\begin_layout Standard
The Pearson Divergence is not technically a metric as it is non-symmetric,
 however it is monotonically increasing and positive.
 Because we will always be comparing test data to training data (never training
 data to test data) this lack of symmetry is not a substantial problem.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
We shall see when we begin developing the equations for the Parzen-SVM that
 this lack of symmetriy will not prevent us from using standard quadratic
 optimization procedures; the kernel matrix remains symmetric.
\end_layout

\end_inset

 The Parzen Window estimation of 
\begin_inset Formula $\mathcal{P}^{\mathcal{S}}$
\end_inset

 is defined as:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\Pr(S\ |\ \mathcal{S})\simeq\sum_{i=1}^{\ell}\frac{1}{\ell}K_{\gamma}(S,S_{i})\label{eq:PrSParzenSingle}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where
\begin_inset Formula $K$
\end_inset

 is some kernel function, for instance the Radial Basis Function:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
K_{\gamma}(S_{i},S_{j})=\frac{1}{\gamma\sqrt{2\pi}}e^{-\frac{1}{\gamma}\|\phi_{i},\phi_{j}\|}\label{eq:KRBFparzen}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The same method can be used to estimate 
\begin_inset Formula $\phi_{n}(\mathbf{x})$
\end_inset

 for a given subset 
\begin_inset Formula $S_{n}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\phi_{n}(\mathbf{x}:\ S_{n})=\sum_{\mathbf{x}_{i}\in S_{n}}\frac{1}{|S_{n}|}k_{\gamma}(\mathbf{x},\mathbf{x}_{i})\label{eq:PhiSingle}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset Formula $|\cdot|$
\end_inset

 denotes cardinality.
 Substituting 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:PrSParzenSingle"

\end_inset

 into 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:PrS"

\end_inset

 our probability distribution estimate becomes:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\varphi_{P}(\mathbf{x},S_{Z}:\ \mathcal{S})=\sum_{i=1}^{\ell}\frac{1}{\ell}K_{\gamma}\left(\{S_{Z}\cup\mathbf{x}\},S_{i}\right)\label{eq:VarphiParzenSingle}\end{equation}

\end_inset

 
\end_layout

\begin_layout Subsection
Multiple Channel Parzen Window
\end_layout

\begin_layout Standard
Extending the Parzen Window approach requires the realization that 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:KLsingle"

\end_inset

 requires that 
\begin_inset Formula $S_{n}$
\end_inset

 and 
\begin_inset Formula $S_{m}$
\end_inset

 both be defined over the same abstract space 
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
 For instance if 
\begin_inset Formula $X_{n}\in\mathbb{R}^{2}$
\end_inset

 and 
\begin_inset Formula $X_{m}\in\mathbb{R}^{3}$
\end_inset

, it is impossible to calculate 
\begin_inset Formula $\phi_{n}\big((x_{m},t)\big)$
\end_inset

 because the quantituy 
\begin_inset Formula $\|x_{m}-x_{n}\|^{2}$
\end_inset

 from using 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:PhiSingle"

\end_inset

 is ambiguous.
\end_layout

\end_inset

.
 As mentioned earlier, in the Multiple Channel context each channel is treated
 as an orthonormal basis of 
\begin_inset Formula $\Omega^{\mathcal{S}}$
\end_inset

.
 An obvious approach to defining a measure over 
\begin_inset Formula $\Omega^{\mathcal{S}}$
\end_inset

 for mutliple channels is to use the Euclidean norm of the Kullbeck Liebler
 divergence of each channel considered independently:
\end_layout

\begin_layout Standard
\begin_inset Formula \[
S_{n}^{c}=S_{n}\cap X^{c}\]

\end_inset


\end_layout

\begin_layout Standard
This requires the following minor extension of 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:KRBFparzen"

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\mathcal{K}_{\gamma}(S_{i},S_{j})=\prod_{c}\frac{1}{\gamma\sqrt{2\pi}}e^{-\frac{1}{\gamma}\|S_{i}^{c},S_{j}^{c}\|_{KL}^{2}}\label{eq:KRBFParzenMultiple}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Notice that this means the e multiplier needs to be raised to c
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Parzen Windows with Linear Transforms
\end_layout

\begin_layout Standard
Thus far we have been dealing with a discrete set of windowing parameters,
 defined by the points being compared and some parameter 
\begin_inset Formula $\alpha$
\end_inset

.
 The approach as been to select windows over two sets of points, shift the
 two sets so the window is at 
\begin_inset Formula $t=0$
\end_inset

 and compare them to each other.
 We have established a method which windows both the testing data 
\begin_inset Formula $Y$
\end_inset

 and the training data 
\begin_inset Formula $Z$
\end_inset

 in order to handle localized patterns.
 We can simplify this process in two ways; by eliminating the windowing
 function over the testing data and by using a sliding window over the training
 data.
 In this approach we eliminate explicit windows and instead take the integral
 over all windows of the training data:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\bar{\varphi}_{sP}(\mathbf{x},Z:\ Y,\alpha) & =\sum_{i=1}^{\ell}\bar{K}_{\gamma}\left(\{Z\cup\mathbf{x}\},Y:\ t_{i},\alpha\right)\\
\bar{K}_{\gamma}\left(Z,Y:\ t,\alpha\right) & =\frac{1}{\gamma\sqrt{2\pi}}e^{-\frac{1}{\gamma}\|\phi_{Z},\phi_{Y}:\ t,\alpha\|}\\
\|\phi_{X},\phi_{Y}:\ t\| & =\sum_{\mathbf{x}_{i}\in X\cup Y}\int_{-\infty}^{\infty}\left(\frac{\phi\left((x_{i},t_{i}-\tau):\ X\right)}{\phi_{Y}\left((x_{i},t_{i}):\ Y\right)}-1\right)^{2}d\tau\\
\|\phi_{X},\phi_{Y}:\ \mathbf{A},\mathbf{b}\| & =\sum_{\vec{x}\in X\cup Y}\int\int_{-\infty}^{\infty}\left(\frac{\phi\left(\mathbf{A}\vec{x}+\mathbf{b}:\ X\right)}{\phi_{Y}\left(\vec{x}:\ Y\right)}-1\right)^{2}d\mathbf{A}d\mathbf{b}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
It can be shown that this reduces to the following:
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
\begin_inset Formula \begin{align*}
\int\left(\frac{\bar{\phi}(\mathbf{x}_{i}:\ X,\tau)}{\phi(\mathbf{x}_{i}:\ Y)}-1\right)^{2}d\tau & =\int\left(\frac{\sum_{\mathbf{x}_{i}\in X}\ k_{\gamma}\left((x,t+\tau),(x_{i},t_{i})\right)}{\phi(\mathbf{x}_{i}:\ Y)}-1\right)^{2}d\tau\\
 & =\int\frac{\left(\sum_{\mathbf{x}_{i}\in X}\ k_{\gamma}\left((x,t+\tau),(x_{i},t_{i})\right)\right)^{2}}{\phi(\mathbf{x}_{i}:\ Y)^{2}}d\tau-\int\frac{\sum_{\mathbf{x}_{i}\in X}\ 2k_{\gamma}\left((x,t+\tau),(x_{i},t_{i})\right)}{\phi(\mathbf{x}_{i}:\ Y)}d\tau\\
 & =\int\frac{\sum_{\mathbf{x}_{i}\in X}\sum_{\mathbf{x}_{j}\in X}\ k_{\gamma}\left((x,t+\tau),(x_{i},t_{i})\right)k_{\gamma}\left((x,t+\tau),(x_{j},t_{j})\right)}{\phi(\mathbf{x}_{i}:\ Y)^{2}}d\tau-\int\frac{\sum_{\mathbf{x}_{i}\in X}\ 2k_{\gamma}\left((x,t+\tau),(x_{i},t_{i})\right)}{\phi(\mathbf{x}_{i}:\ Y)}d\tau\\
 & =\sum_{\begin{array}{c}
\mathbf{x}_{i}\in X\\
\mathbf{x}_{j}\in X\end{array}}\int\frac{k_{\gamma}\left((x,t+\tau),(x_{i},t_{i})\right)k_{\gamma}\left((x,t+\tau),(x_{j},t_{j})\right)}{\phi(\mathbf{x}_{i}:\ Y)^{2}}d\tau-\sum_{\mathbf{x}_{i}\in X}\int\frac{2k_{\gamma}\left((x,t+\tau),(x_{i},t_{i})\right)}{\phi(\mathbf{x}_{i}:\ Y)}d\tau\\
 & =\sum_{\begin{array}{c}
\mathbf{x}_{i}\in X\\
\mathbf{x}_{j}\in X\end{array}}\int\frac{e^{-\frac{1}{\gamma}\left((t+\tau-t_{i})^{2}+(t+\tau-t_{j})^{2}+(x-x_{i})^{2}+(x-x_{j})^{2}\right)}}{\gamma^{2}2\pi\phi(\mathbf{x}_{i}:\ Y)^{2}}d\tau-\sum_{\mathbf{x}_{i}\in X}\int\frac{2e^{-\frac{1}{\gamma}\left((t+\tau-t_{i})^{2}+(x-x_{i})^{2}\right)}}{\gamma\sqrt{2\pi}\phi(\mathbf{x}_{i}:\ Y)}d\tau\\
 & =\sum_{\begin{array}{c}
\mathbf{x}_{i}\in X\\
\mathbf{x}_{j}\in X\end{array}}\lim_{n\rightarrow\infty}\frac{e^{-\frac{1}{2\gamma}\left((t-t_{i})^{2}-2(t-t_{i})(t-t_{j})+(t-t_{j})^{2}+2(x-x_{i})^{2}+2(x-x_{j})^{2}\right)}\sqrt{\frac{\pi}{2}}\text{erf}\left(\frac{\sqrt{\frac{1}{\gamma}}\left(2t-t_{i}-t_{j}+2n\right)}{\sqrt{2}}\right)}{2\sqrt{\frac{1}{\gamma}}\gamma^{2}2\pi\phi(\mathbf{x}_{i}:\ Y)^{2}}\\
 & \qquad\qquad\qquad-\sum_{\begin{array}{c}
\mathbf{x}_{i}\in X\\
\mathbf{x}_{j}\in X\end{array}}\lim_{n\rightarrow-\infty}\frac{e^{-\frac{1}{2\gamma}\left((t-t_{i})^{2}-2(t-t_{i})(t-t_{j})+(t-t_{j})^{2}+2(x-x_{i})^{2}+2(x-x_{j})^{2}\right)}\sqrt{\frac{\pi}{2}}\text{erf}\left(\frac{\sqrt{\frac{1}{\gamma}}\left(2t-t_{i}-t_{j}+2n\right)}{\sqrt{2}}\right)}{2\sqrt{\frac{1}{\gamma}}\gamma^{2}2\pi\phi(\mathbf{x}_{i}:\ Y)^{2}}\\
 & \qquad\qquad\qquad-\sum_{\mathbf{x}_{i}\in X}\lim_{n\rightarrow\infty}\frac{e^{-\frac{1}{\gamma}\left((x-x_{i})^{2}+(x-x_{j})^{2}\right)}\sqrt{\pi}\text{erf}\left(\sqrt{\frac{1}{\gamma}}\left((t-t_{i})+n\right)\right)}{\sqrt{\frac{1}{\gamma}}\gamma\sqrt{2\pi}\phi(\mathbf{x}_{i}:\ Y)}\\
 & \qquad\qquad\qquad+\sum_{\mathbf{x}_{i}\in X}\lim_{n\rightarrow-\infty}\frac{e^{-\frac{1}{\gamma}\left((x-x_{i})^{2}+(x-x_{j})^{2}\right)}\sqrt{\pi}\text{erf}\left(\sqrt{\frac{1}{\gamma}}\left((t-t_{i})+n\right)\right)}{\sqrt{\frac{1}{\gamma}}\gamma\sqrt{2\pi}\phi(\mathbf{x}_{i}:\ Y)}\\
 & =\sum_{\begin{array}{c}
\mathbf{x}_{i}\in X\\
\mathbf{x}_{j}\in X\end{array}}\frac{\sqrt{\frac{\pi}{2}}e^{-\frac{1}{2\gamma}\left((t-t_{i})^{2}-2(t-t_{i})(t-t_{j})+(t-t_{j})^{2}+2\sum_{c}(x^{c}-x_{i}^{c})^{2}+(x^{c}-x_{j}^{c})^{2}\right)}}{\sqrt{\frac{1}{\gamma}}\gamma^{2}2\pi\phi(\mathbf{x}_{i}:\ Y)^{2}}-\sum_{\mathbf{x}_{i}\in X}\frac{2\sqrt{\pi}e^{-\frac{1}{\gamma}\left((x-x_{i})^{2}+(x-x_{j})^{2}\right)}}{\sqrt{\frac{1}{\gamma}}\gamma\sqrt{2\pi}\phi(\mathbf{x}_{i}:\ Y)}\end{align*}

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Check that equation to make sure the subscripts haven't gotten mixed up.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\|\phi_{X},\phi_{Y}:\ t,\alpha\| & =\sum_{\begin{array}{c}
\mathbf{x}_{i}\in X\\
\mathbf{x}_{j}\in X\\
\mathbf{x}_{k}\in X\end{array}}\frac{\omega(t_{i}:\ t,\alpha)\sqrt{\frac{\pi}{2}}e^{-\frac{1}{2\gamma}\left((t_{i}-t_{j})^{2}-2(t_{i}-t_{j})(t_{i}-t_{k})+(t_{i}-t_{k})^{2}+2\sum_{c}(x_{i}^{c}-x_{j}^{c})^{2}+(x_{i}^{c}-x_{k}^{c})^{2}\right)}}{\sqrt{\frac{1}{\gamma}}\gamma^{2}2\pi\phi\left((x_{i},t_{i}):\ Y\right)^{2}}-\sum_{\begin{array}{c}
\mathbf{x}_{i}\in X\\
\mathbf{x}_{j}\in X\end{array}}\frac{\omega(t_{i}:\ t,\alpha)2\sqrt{\pi}e^{-\frac{1}{\gamma}\left((x_{i}-x_{j})^{2}+(x_{i}-x_{k})^{2}\right)}}{\sqrt{\frac{1}{\gamma}}\gamma\sqrt{2\pi}\phi\left((x_{i},t_{i}-t):\ Y\right)}\end{align*}

\end_inset


\end_layout

\begin_layout Section
Support Vector Estimation
\end_layout

\begin_layout Standard
The Parzen Window method is neither sparse nor computationally efficient,
 and as the number of observations grows, these deficiencies quickly become
 prohibitive.
 We now investigate the use of Support Vector Machines to generate estimates.
\end_layout

\begin_layout Subsection
Generalized Parzen-SVM 
\end_layout

\begin_layout Standard
Support Vector Machines are often used
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
ref?
\end_layout

\end_inset

 to estimate probability distributions by solving the related problem of
 estimating the cumulative distribution function of the random variable
 in question.
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
This isn't really the essence of SVM - put more verbage in the intro regarding
 what SVM's are, why they work, and why they're superior to other approaches
 (say, neural networks)
\end_layout

\end_inset

 This reduces the problem to one of estimating a non-linear mapping from
 observations to cumulative distribution values, which can be formulated
 as an optimization problem over a linear operator equation.
 Unfortunately, these methods depend on the ability to calculate an empirical
 distribution for each observation:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
F_{\ell}(x)=\frac{1}{\ell}\sum_{i}\theta(x-x_{i})\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\theta(x)$
\end_inset

 is the indicator function.
 To evaluate this function, the abstract space 
\begin_inset Formula $\Omega^{\mathcal{S}}$
\end_inset

 must be ordered.
 While we have described a distance metric over 
\begin_inset Formula $\Omega^{\mathcal{S}}$
\end_inset

, it is not clear what a meaningful ordering relation would be.
\end_layout

\begin_layout Standard
Rather than calculating the cumulative probability distribution of 
\begin_inset Formula $X$
\end_inset

, we begin with the assumption that the Parzen Window estimate of the probabilit
y distribution is acceptably accurate and attempt to minimize the difference
 between the Support Vector estimate and the Parzen Window estimate.
 The support vector approach requires that we define an optimization problem
 
\begin_inset Formula $W(\beta)$
\end_inset

 which will determine which observations (or as we shall see, windows) will
 be used in estimations and which can be discarded as redundant or irrelevant
 information.
 We therefore define a set of weights 
\begin_inset Formula $\beta$
\end_inset

, some of which will be non-zero at the conclusion of our optimization task.
 We define the optimization problem as minimizing the square loss between
 the Support Vector and Parzen Window estimates over some set of observations
 
\begin_inset Formula $X$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
W(\beta) & =\min_{\beta}\sum_{\mathbf{x}\in X}\ \sum_{i=1}^{\ell}\left(\varphi_{P}(\mathbf{x})-\varphi_{SV}(\mathbf{x}:\ \beta_{i})\right)^{2}+\lambda\Omega\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
explain regularizer
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Before discussing the nature of this optimization task, we must establish
 the Support Vector estimation function 
\begin_inset Formula $\varphi_{SV}$
\end_inset

.
 The support vector target functional is identical to the Parzen Window
 estimator, with the exception that rather than averaging over all the observati
ons in the training set (or integrating over all offsets), we use the weighted
 sum of each training observation:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\varphi_{SV}(\mathbf{x},Z:\ \alpha,\beta,Y) & =\sum_{i=1}^{\ell}\beta_{i}\bar{K}_{\gamma}\left(\{Z\cup\mathbf{x}\},Y:\ t_{i},\alpha\right)\end{align*}

\end_inset


\end_layout

\begin_layout Subsection
Windowing Function
\end_layout

\begin_layout Standard
Another approach to handling localized patterns in the training set is to
 use a windowing function 
\begin_inset Formula $\omega(\cdot)$
\end_inset

.
 Where the subset approach compares localized patterns by reducing the number
 of input points being compared, using a windowing functions allows us to
 localize our comparison by reducing the influence of points based on their
 distance from two reference points.
 This is a generalized case of subsets; the same result can be produce by
 using a step function for the windowing function.
 The strength of the windowing approach is that it allows us to eliminate
 the task of defining specific subsets and allows us to use a gradient of
 influence rather than an all-or-nothing approach.
 We define a windowing function as a kernel function which returns a weight
 value based on the distance from a point to a window defined by a location
 in the time axis 
\begin_inset Formula $t$
\end_inset

 and some set of parameters 
\begin_inset Formula $\alpha$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\omega(\mathbf{x}:\ t,\alpha) & \ge0\\
\int_{-\infty}^{\infty}\omega(\mathbf{x}:\ t,\alpha)dt & =1\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Implementing a windowing function requires to to reconsider the distance
 metric 
\begin_inset Formula $\|\cdot,\cdot\|$
\end_inset

 discussed previously.
 The established distance metric compares the divergence between two sets
 of data; we must now define a distance metric capable of comparing the
 divergence between two windows defined on a single set of data (as well
 as two windows defined on two discrete sets of data).
 If we use the Kullback-Liebler divergence as our divergence measure, we
 can use the following as our distance metric:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\|\phi_{X},\phi_{Y}:\ t,\alpha\| & =D_{KL}(\phi_{n},\phi_{m}:\ t,0,\alpha)+D_{KL}(\phi_{m},\phi_{n}:\ 0,t,\alpha)\\
D_{KL}(\phi_{i},\phi_{j}:\ t_{i},t_{j},\alpha) & =\sum_{\mathbf{x}_{k}\in X\cup Y}\omega(\mathbf{x}_{k}:\ t_{i},\alpha)\omega(\mathbf{x}_{k}:\ t_{j},\alpha)\ \phi\left((x_{k},t_{k}-t_{i}):\ X\right)\log\frac{\phi\left((x_{k},t_{k}-t_{i}):\ X\right)}{\phi\left((x_{k},t_{k}-t_{j}):\ Y\right)}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
The shifting hasn't been explained
\end_layout

\end_inset


\end_layout

\begin_layout Standard
This definition of the distance metric essentially weights the contribution
 of each point's entropy to the divergence based on its distance from either
 of the windows defined by 
\begin_inset Formula $(t_{i},\alpha)$
\end_inset

 and 
\begin_inset Formula $(t_{j},\alpha)$
\end_inset

.
 
\end_layout

\begin_layout Standard
We can now reformulate the Parzen Window equations to use the windowing
 function in place of explicit subsets.
 For cumputational reasons we check windows centered on each of the points
 in the sets being compared:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\bar{\varphi}_{P}(\mathbf{x},Z:\ Y) & =\sum_{i=1}^{\ell}\sum_{j=1}^{|Z|}\frac{1}{\ell|Z|}\bar{K}_{\gamma}\left(\{Z\cup\mathbf{x}\},Y\right)\\
\bar{K}_{\gamma}\left(Z,Y:\ t_{i},t_{j}\alpha\right) & =\frac{1}{\gamma\sqrt{2\pi}}e^{-\frac{1}{\gamma}\|\phi_{Z},\phi_{Y}\|}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We can now develop a method of evaluating the 'usefulness' of time window
 in a set of training data.
 To do this we begin by observing that the duration of the training set
 is likely larger than the duration of test sets, which means we will have
 to make estimates using less information that is available in the training
 data; whatever optimization problem we define will need to take this into
 account.
 The optimization equation compares the results of Parzen estimation to
 the results of SVM estimation using a portion of the training data as synthetic
 testing data.
 Fortunately, we have already developed an understanding of a windowing
 function which can be applied to this situation.
 For the optimization problem, we check the difference between the two estimates
 only at windows defined by the observations in the training set.
 The set of weights used in the Support Vector estimation must have a discrete
 number of elements; for simplicity we choose to assign a weight to each
 window defined by the time value of an observation in the training set
 and the constant parameter 
\begin_inset Formula $\alpha$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
W(\beta:\ Y,\alpha) & \longmapsto\min_{\beta}\sum_{\mathbf{x}\in Y}\ \sum_{i=1}^{\ell}\left(\varphi_{P}(\mathbf{x},Y:\ Y)-\varphi_{SV}(\mathbf{x},Y:\ \alpha,\beta_{i},Y)\right)^{2}+\lambda\Omega\\
 & =\sum_{\mathbf{x}\in Y}\ \sum_{i=1}^{\ell}\varphi_{P}(\mathbf{x},Y:\ Y)^{2}-2\varphi_{P}(\mathbf{x},Y:\ Y)\varphi_{SV}(\mathbf{x},Y:\ \alpha,\beta_{i},Y)+\varphi_{SV}(\mathbf{x},Y:\ \alpha,\beta_{i},Y)^{2}+\lambda\Omega\\
 & =\sum_{\mathbf{x}\in Y}\ \sum_{\begin{array}{c}
i=1\\
j=1\end{array}}^{\ell}\beta_{i}\beta_{j}\bar{K}_{\gamma}\left(\{Z\cup\mathbf{x}\},Y:\ t_{i},\alpha\right)\bar{K}_{\gamma}\left(\{Z\cup\mathbf{x}\},Y:\ t_{j},\alpha\right)-\sum_{i=1}^{\ell}\beta_{i}2\varphi_{P}(\mathbf{x},Y:\ Y)+\varphi_{P}(\mathbf{x},Y:\ Y)^{2}+\lambda\Omega\end{align*}

\end_inset


\end_layout

\begin_layout Standard
This is a quadratic optimization problem and can be solved with known methods.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Regularizer term?
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Parzen-SVM Decomposition 
\end_layout

\begin_layout Standard
Due to the simplicity of the constraints in the optimization problem, it
 is possible to use the decomposition method of Osuna to reduce the memory
 requirements of the Parzen-SVM algorithm.
\end_layout

\begin_layout Subsubsection
 Sub-Problem Definition 
\end_layout

\begin_layout Standard
The decomposition algorithm breaks 
\begin_inset Formula $X$
\end_inset

 into two working sets 
\begin_inset Formula $B,N$
\end_inset

, and attempts to optimize 
\begin_inset Formula $B$
\end_inset

 while keeping 
\begin_inset Formula $N$
\end_inset

 fixed.
 This results in the following iterative optimization problem where 
\begin_inset Formula $\boldsymbol{\beta}^{k}$
\end_inset

 denotes the result of the previous iteration:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
W(\boldsymbol{\beta}_{B}) & =\frac{1}{2}\begin{bmatrix}\boldsymbol{\beta}_{B}^{T} & (\boldsymbol{\beta}_{N}^{k})^{T}\end{bmatrix}\begin{bmatrix}P_{BB} & P_{BN}\\
P_{NB} & P_{NN}\end{bmatrix}\begin{bmatrix}\boldsymbol{\beta}_{B}\\
\boldsymbol{\beta}_{N}^{k}\end{bmatrix}-\begin{bmatrix}q_{B}^{T} & q_{N}^{T}\end{bmatrix}\begin{bmatrix}\boldsymbol{\beta}_{B}\\
\boldsymbol{\beta}_{N}^{k}\end{bmatrix}\\
 & =\frac{1}{2}\boldsymbol{\beta}_{B}^{T}P_{BB}\boldsymbol{\beta}_{B}-(-q_{B}+P_{BN}\boldsymbol{\beta}_{N}^{k})^{T}\boldsymbol{\beta}_{B}\\
 & =\frac{1}{2}\begin{bmatrix}\beta_{i} & \beta_{j}\end{bmatrix}\begin{bmatrix}P_{ii} & P_{ij}\\
P_{ij} & P_{jj}\end{bmatrix}\begin{bmatrix}\beta_{i}\\
\beta_{j}\end{bmatrix}-(-q_{B}+P_{BN}\boldsymbol{\beta}_{N}^{k})^{T}\begin{bmatrix}\beta_{i}\\
\beta_{j}\end{bmatrix}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\text{subject to}\quad0\le\beta_{i},\beta_{j},\quad\beta_{i}+\beta_{j}=1-\mathbf{1}^{T}\boldsymbol{\beta}_{N}^{k}\end{equation}

\end_inset


\end_layout

\begin_layout Subsubsection
 Working Set Selection 
\end_layout

\begin_layout Standard
Select
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align}
 & i\in\text{arg}\max_{t}\left\{ -\nabla f(\boldsymbol{\beta}^{k})_{t}\ |\quad t\in I(\boldsymbol{\beta}^{k})\right\} \\
 & j\in\text{arg}\min_{t}\left\{ -\frac{b_{it}^{2}}{a_{it}}\ |\quad t\in I(\boldsymbol{\beta}^{k}),\quad-\nabla f(\boldsymbol{\beta}^{k})_{t}<-\nabla f(\boldsymbol{\beta}^{k})_{i}\right\} \end{align}

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% This needs to be checked for the new optimization scenario
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align}
I(\boldsymbol{\beta}) & \equiv\{t\ |\quad\beta_{t}<1\quad\text{or}\quad\beta_{t}>0\}\\
 & a_{it}=P_{ii}+P_{tt}-2P_{it}\\
 & \bar{a}_{it}=\begin{cases}
a_{it} & \text{if}\ a_{it}>0\\
\delta & \text{otherwise}\end{cases}\\
 & b_{it}=-\nabla f(\boldsymbol{\beta}^{k})_{i}+\nabla f(\boldsymbol{\beta}^{k})_{t}\\
\nabla f(\boldsymbol{\beta})_{i} & \equiv P_{i}\boldsymbol{\beta}-q_{i}\end{align}

\end_inset


\end_layout

\begin_layout Subsubsection
 Stopping Condition 
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\max_{i\in I(\boldsymbol{\alpha}^{k})}-\nabla f(\boldsymbol{\alpha})_{i}+\min_{j\in I(\boldsymbol{\alpha}^{k})}\nabla f(\boldsymbol{\alpha})_{j}\le\epsilon\end{equation}

\end_inset


\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Subsection
Eunite Competition Data
\end_layout

\begin_layout Subsection
Santa Fe Data
\end_layout

\begin_layout Subsection
CATS Benchmark Data
\end_layout

\begin_layout Subsection
Results Summary
\end_layout

\begin_layout Section
Further Research
\end_layout

\begin_layout Subsection
Data Pre-Processing
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% logistic function using mean and sd to put most training points between
 .1 and .9
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% upper and lower bounds on 
\backslash
(
\backslash
int 
\backslash
Delta 
\backslash
) as non-stationary data detections mechanism
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% logistic function from delta using mean and sd in same way if data non-station
ary
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% iterative integration process until stationary data found ?
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Ensemble System
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% more analysis of recent data, some type of transfer from short to long-term
 'memory'
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% heirarchical system?
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% what parameters change?
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Weighted Influence
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% a weight term could be added to the regularizer to control the influence
 of specific sets.
  This could provide more information in 'important' situations, which could
 be useful in motivated learning.
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Standard
Eat it, bitches
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
nocite{Moreno03}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
bibliographystyle{plain}
\end_layout

\end_inset

 
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "Research/research.bib"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
