#LyX 1.6.2 created this file. For more info see http://www.lyx.org/
\lyxformat 345
\begin_document
\begin_header
\textclass article
\begin_preamble



\usepackage{amsfonts}
\end_preamble
\use_default_options false
\language english
\inputencoding auto
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\paperfontsize 10
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 0
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\author "" 
\author "" 
\end_header

\begin_body

\begin_layout Title
Parzen-SVM: An Approach to Timeseries Analysis Using Support Vector Machines
 based on Parzen Windows
\end_layout

\begin_layout Author
Ryan Michael
\begin_inset Newline newline
\end_inset

 
\family typewriter
kerinin@gmail.com
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Subsection
Existing Work
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Expectation-maximization_algorithm
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Stationary_process
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Ergodicity
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Mixing_(mathematics)
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Lyapunov_exponent
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Recurrence_quantification_analysis
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Recurrence_plot
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Autoregressive_fractionally_integrated_moving_averag
e
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Autocorrelation
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
url{http://en.wikipedia.org/wiki/Linear_discriminant_analysis}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Hidden Markov Model
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% cannot account for future states - only capable of prediction
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% weak, short-term memory
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Box-Jenkins
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% intended for simplistic processes with well-understood stationarity and
 periodicity
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% http://en.wikipedia.org/wiki/Box-Jenkins
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Spectral Analysis
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% assumes some type of frequency-domain decomposition.
  Frequency-domain signal representations do not do a very good job predicting
 time-domain values.
 
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Shrinking-
\begin_inset Formula $\epsilon$
\end_inset

 SVM Regression
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% theoretical foundation weak; only compensates for the relevance of recent
 data.
  See Markhov problem
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
General Overview
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Try to work out the scale invariance, the re-frame this entire paper in
 the context of multiple scale and shift invariant parameters (if that's
 possible of course)
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The goal is to create a method of statistical inference capable of processing
 timeseries data which is both multi-variate and exhibits different behaviors
 at different times.
 This type of data is common, and developing a robust method of analysis
 has applications in many domains.
 The general approach is to create a series of estimates using subsets of
 the observed data, and to then combine these estimates in an intelligent
 manner which captures the relevance of each estimate to the current prediction
 task.
 By using a set of 'typical' estimates, we are able to reduce the computational
 demands of the system, as each estimate is a condensed representation of
 the data from which it was derived.
 This approach also allows us to reduce data redundancy by only using distinct
 estimates.
\end_layout

\begin_layout Standard
The most basic operation used in this system is the estimation of probability
 densities.
 Based on a set of observations drawn from some random process, we generate
 an estimate of the underlying probability distribution.
 This estimate tells us the probability of each point in the input space
 being observed.
 Areas of the input space in which a dense set of observations are observed
 are given high probability, while areas of the input space with few observation
s are given low probability.
\end_layout

\begin_layout Standard
We assume that observations are pulled from multiple independent sources,
 all of which respond to some underlying phenomena.
 For instance one set of observations could be from a microphone and another
 from a light detector.
 We do not know how the inputs are related or to what extent their behavior
 changes over time.
 For example one input could be a steady sine wave and another could be
 based on the decay of a radioactive isotope.
 In the former case previous observations of the source are useful, in the
 latter they're not.
 Alternately two inputs could be light detectors in the same room or they
 could be on different continents; in the former their input would be highly
 similar, in the latter not as much.
\end_layout

\begin_layout Section
Problem Setting
\end_layout

\begin_layout Standard
We begin with a hidden random variable
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\mathbf{X}=(\boldsymbol{\Omega},\mathcal{\boldsymbol{F}},\mathcal{\boldsymbol{P}})\]

\end_inset


\end_layout

\begin_layout Standard
Our knowledge of 
\begin_inset Formula $\mathbf{X}$
\end_inset

 comes from a set of independant sources 
\begin_inset Formula $X^{n}$
\end_inset

 which we treat as random processes generated by 
\begin_inset Formula $\mathbf{X}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \[
X^{n}:\Omega\mapsto\Omega^{n}\in\mathbb{R}^{d}\times\mathbb{R}_{+}\]

\end_inset

 
\begin_inset Formula \[
X^{n}=(\Omega^{n},\mathcal{F}^{n},\mathcal{P}^{n})\]

\end_inset

 
\begin_inset Formula \[
\mathbf{X}=[X^{0},...,X^{c}]\]

\end_inset


\end_layout

\begin_layout Standard
We refer to each of these sources as a channel, and refer to each channel
 as the 
\begin_inset Formula $i^{\text{th}}$
\end_inset

 element of the set 
\begin_inset Formula $\mathbf{X}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \[
X^{n}=[\Omega^{n},\mathcal{F}^{n},\mathcal{P}^{n}]\]

\end_inset


\end_layout

\begin_layout Standard
For each channel we are given a set of 
\begin_inset Formula $\ell$
\end_inset

 observations of dimension 
\begin_inset Formula $d$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\mathbf{x}_{i}^{n}=(x_{i}^{n},t_{i}^{n})\]

\end_inset

 
\begin_inset Formula \[
X^{n}=\left[\mathbf{x}_{0}^{n},\ldots,\mathbf{x}_{\ell}^{n}\right],\ \mathbf{x}_{m}^{n}\in\mathbb{R}^{d}\times\mathbb{R}_{+}\]

\end_inset


\end_layout

\begin_layout Standard
We define a set of time values and durations:
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\mathcal{T}=[t_{0},\ldots,t_{\ell}]\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \[
\Theta=[\theta_{0},\ldots,\theta_{z}]\]

\end_inset


\end_layout

\begin_layout Standard
We assume that the probability measure 
\begin_inset Formula $\mathcal{P}^{n}$
\end_inset

 of 
\begin_inset Formula $X^{n}$
\end_inset

can be approximated in a given time window using a shifted and weighted
 set of distributions defined over some set of time intervals.
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\mathcal{P}(t,\theta)=\sum_{i}\delta_{i}\cdot f_{i}(t-t_{i})\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\delta_{n}$
\end_inset

 is the weight corresponding to 
\begin_inset Formula $f_{n}$
\end_inset

 and 
\begin_inset Formula $t_{i}$
\end_inset

 is the magnitude of the time shift.
 Finally, we assume that a similar mixture of distributions can be determined
 for each channel:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\mathcal{P}^{n}(t,\theta)=\sum_{i}\delta_{i}^{n}\cdot f_{i}^{n}(t-t_{i})\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
Single Channel Setting
\end_layout

\begin_layout Standard
We begin by considering the case where only one channel exists, so for now
 we will omit the superscript and refer to 
\begin_inset Formula $X^{n}=(\Omega^{n},\mathcal{F}^{n},\mathcal{P}^{n})$
\end_inset

 as 
\begin_inset Formula $X=(\Omega,\mathcal{F},\mathcal{P})$
\end_inset

.
 Given a set of training data 
\begin_inset Formula $Y$
\end_inset

, our task is to estimate a probability distribution based on a set of test
 data 
\begin_inset Formula $Z$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
Y & =\left[(y_{1},t_{1}),...,(y_{\ell},t_{\ell})\right]\longmapsto\left[\mathbf{y}_{1},...,\mathbf{y}_{\ell}\right]\\
Z & =\left[(z_{1},t_{1}),...,(z_{\eta},t_{\eta})\right]\longmapsto\left[\mathbf{z}_{1},...,\mathbf{z}_{\eta}\right]\end{align*}

\end_inset

For each observation in the training data we define a subset of 
\begin_inset Formula $Y$
\end_inset

 which contains all the observations which fall in a time window starting
 at the training observation and of duration greater than or equal to the
 duration of the test data and, shifted to the interval 
\begin_inset Formula $t\in[0,\theta)$
\end_inset

:
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
elaborate on what the time windows are and how they're denoted
\end_layout

\end_inset


\begin_inset Formula \begin{align}
\theta & \ge\max_{t}Z-\min_{t}Z\nonumber \\
S_{i} & =\left[(y_{j},t_{j}-t_{i})|\quad\mathbf{y}\in Y,t_{i}\le t_{j}<t_{j}+\theta\right]\label{eq:sSingle}\\
\mathcal{S} & =\left[S_{1},...,S_{\ell}\right]\nonumber \end{align}

\end_inset


\end_layout

\begin_layout Standard
We treat 
\begin_inset Formula $\mathcal{S}$
\end_inset

 as a random process:
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\mathcal{S}=[\Omega^{\mathcal{S}},\mathcal{F}^{\mathcal{S}},\mathcal{P}^{\mathcal{S}}]\]

\end_inset


\end_layout

\begin_layout Standard
and observe that 
\begin_inset Formula $Z$
\end_inset

 can be treated as an observation of 
\begin_inset Formula $\mathcal{S}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
S_{Z}=\left[\left(z_{i},z_{i}-\min_{t}Z\right)|\quad(x_{i},t_{i})\in Z\right]\label{eq:SySingle}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
We can now frame the task of estimating the probability distribution of
 
\begin_inset Formula $Z$
\end_inset

 as a task of estimating a probability density function 
\begin_inset Formula $\varphi$
\end_inset

 for the random process 
\begin_inset Formula $\mathcal{S}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\Pr(X=\mathbf{x}\ |\ Y\cup Z)\mapsto\ \Pr(\mathcal{S}=\{S_{Z}\cup\mathbf{x}\}|\quad Z)\ \simeq\ \varphi(\mathbf{x},S_{Z}:\ \mathcal{S})\label{eq:PrS}\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
Multiple Channel Setting
\end_layout

\begin_layout Standard
In order to extend this result to settings in which multiple channels exist,
 we return to 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:sSingle"

\end_inset

 and extend the scope to multiple channels:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
S_{i}=\left[(y_{j}^{n},t_{j}-t_{i})|\quad\mathbf{y}^{n}\in\mathbf{Y},t_{i}\le t_{j}<t_{j}+\theta\right]\label{eq:Smultiple}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
In this case, we treat each channel as an orthonormal basis of the abstract
 space 
\begin_inset Formula $\Omega^{\mathcal{S}}$
\end_inset

.
 Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:SySingle"

\end_inset

 is likewise extended in the same manner:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
S_{Z}=\left[\left(z_{i}^{n},z_{i}-\min_{t}Z\right)|\quad\mathbf{z}^{n}\in\mathbf{Z}\right]\label{eq:SyMultiple}\end{equation}

\end_inset


\end_layout

\begin_layout Section
Parzen Window Estimation
\end_layout

\begin_layout Standard
One method of evaluating 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:PrS"

\end_inset

 is by using the Parzen Window method.
 We choose the Parzen Window method because it allows us to estimate probabiliti
es of unordered sets, provided they have an addition operation and a kernel
 function exists to provide a distance metric.
\end_layout

\begin_layout Subsection
 Single Channel Parzen Window
\end_layout

\begin_layout Standard
We will again begin by considering the single-channel case, then extend
 the resulting equations as necessary.
 The Parzen Window method requires the definition of a metric 
\begin_inset Formula $\|\cdot,\cdot\|$
\end_inset

 over the abstract space 
\begin_inset Formula $\Omega^{\mathcal{S}}$
\end_inset

.
 Such a metric can be defined using the symmetric Kullbeck Liebler divergence
 with probability measures 
\begin_inset Formula $\phi_{n}(\mathbf{x})\simeq Pr(X=\mathbf{x}\ |\ S_{n})$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align}
D_{KL}(\phi_{n},\phi_{m}) & =\sum_{\mathbf{x}\in\{S_{n}\cup S_{m}\}}\phi_{n}(\mathbf{x})\log\frac{\phi_{n}(\mathbf{x})}{\phi_{m}(\mathbf{x})}\label{eq:KLsingle}\\
 & =-\sum_{\mathbf{x}\in\{S_{n}\cup S_{m}\}}\phi_{n}(\mathbf{x})\log\phi_{m}(\mathbf{x})+\sum_{\mathbf{x}\in\{S_{n}\cup S_{m}\}}\phi_{n}(\mathbf{x})\log\phi_{n}(\mathbf{x})\\
\|\phi_{n},\phi_{m}\|_{KL} & =D_{KL}(\phi_{n},\phi_{m})+D_{KL}(\phi_{m},\phi_{n})\end{align}

\end_inset

 
\end_layout

\begin_layout Standard
The Parzen Window estimation of 
\begin_inset Formula $\mathcal{P}^{\mathcal{S}}$
\end_inset

 is defined as:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\Pr(S\ |\ \mathcal{S})\simeq\sum_{i=1}^{\ell}\frac{1}{\ell}K_{\gamma}(S,S_{i})\label{eq:PrSParzenSingle}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where
\begin_inset Formula $K$
\end_inset

 is some kernel function, for instance the Radial Basis Function:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
K_{\gamma}(S_{i},S_{j})=\frac{1}{\gamma\sqrt{2\pi}}e^{-\frac{1}{\gamma}\|\phi_{i},\phi_{j}\|}\label{eq:KRBFparzen}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The same method can be used to estimate 
\begin_inset Formula $\phi_{n}(\mathbf{x})$
\end_inset

 for a given subset 
\begin_inset Formula $S_{n}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\phi_{n}(\mathbf{x}:\ S_{n})=\sum_{\mathbf{x}_{i}\in S_{n}}\frac{1}{|S_{n}|}k_{\gamma}(\mathbf{x},\mathbf{x}_{i})\label{eq:PhiSingle}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset Formula $|\cdot|$
\end_inset

 denotes cardinality.
 Substituting 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:PrSParzenSingle"

\end_inset

 into 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:PrS"

\end_inset

 our probability distribution estimate becomes:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\varphi_{P}(\mathbf{x},S_{Z}:\ \mathcal{S})=\sum_{i=1}^{\ell}\frac{1}{\ell}K_{\gamma}\left(\{S_{Z}\cup\mathbf{x}\},S_{i}\right)\label{eq:VarphiParzenSingle}\end{equation}

\end_inset

 
\end_layout

\begin_layout Subsection
Multiple Channel Parzen Window
\end_layout

\begin_layout Standard
Extending the Parzen Window approach requires the realization that 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:KLsingle"

\end_inset

 requires that 
\begin_inset Formula $S_{n}$
\end_inset

 and 
\begin_inset Formula $S_{m}$
\end_inset

 both be defined over the same abstract space 
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
 For instance if 
\begin_inset Formula $X_{n}\in\mathbb{R}^{2}$
\end_inset

 and 
\begin_inset Formula $X_{m}\in\mathbb{R}^{3}$
\end_inset

, it is impossible to calculate 
\begin_inset Formula $\phi_{n}\big((x_{m},t)\big)$
\end_inset

 because the quantituy 
\begin_inset Formula $\|x_{m}-x_{n}\|^{2}$
\end_inset

 from using 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:PhiSingle"

\end_inset

 is ambiguous.
\end_layout

\end_inset

.
 As mentioned earlier, in the Multiple Channel context each channel is treated
 as an orthonormal basis of 
\begin_inset Formula $\Omega^{\mathcal{S}}$
\end_inset

.
 An obvious approach to defining a measure over 
\begin_inset Formula $\Omega^{\mathcal{S}}$
\end_inset

 for mutliple channels is to use the Euclidean norm of the Kullbeck Liebler
 divergence of each channel considered independently:
\end_layout

\begin_layout Standard
\begin_inset Formula \[
S_{n}^{c}=S_{n}\cap X^{c}\]

\end_inset


\end_layout

\begin_layout Standard
This requires the following minor extension of 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:KRBFparzen"

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\mathcal{K}_{\gamma}(S_{i},S_{j})=\prod_{c}\frac{1}{\gamma\sqrt{2\pi}}e^{-\frac{1}{\gamma}\|S_{i}^{c},S_{j}^{c}\|_{KL}^{2}}\label{eq:KRBFParzenMultiple}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Notice that this means the e multiplier needs to be raised to c
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Parzen Window with Windowing Function
\end_layout

\begin_layout Standard
Another approach to handling localized patterns in the training set is to
 use a windowing function 
\begin_inset Formula $\omega(\cdot)$
\end_inset

.
 Where the subset approach compares localized patterns by reducing the number
 of input points being compared, using a windowing functions allows us to
 localize our comparison by reducing the influence of points based on their
 distance from two reference points.
 This is a generalized case of subsets; the same result can be produce by
 using a step function for the windowing function.
 The strength of the windowing approach is that it allows us to eliminate
 the task of defining specific subsets and allows us to use a gradient of
 influence rather than an all-or-nothing approach.
 We define a windowing function as a kernel function which returns a weight
 value based on the distance from a point to a window defined by a location
 in the time axis 
\begin_inset Formula $t$
\end_inset

 and some set of parameters 
\begin_inset Formula $\alpha$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\omega(\mathbf{x}:\ t,\alpha) & \ge0\\
\int_{-\infty}^{\infty}\omega(\mathbf{x}:\ t,\alpha)dt & =1\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Implementing a windowing function requires to to reconsider the distance
 metric 
\begin_inset Formula $\|\cdot,\cdot\|$
\end_inset

 discussed previously.
 The established distance metric compares the divergence between two sets
 of data; we must now define a distance metric capable of comparing the
 divergence between two windows defined on a single set of data (as well
 as two windows defined on two discrete sets of data).
 If we use the Kullback-Liebler divergence as our divergence measure, we
 can use the following as our distance metric:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\|\phi_{X},\phi_{Y}:\ t,\alpha\| & =D_{KL}(\phi_{n},\phi_{m}:\ t,0,\alpha)+D_{KL}(\phi_{m},\phi_{n}:\ 0,t,\alpha)\\
D_{KL}(\phi_{i},\phi_{j}:\ t_{i},t_{j},\alpha) & =\sum_{\mathbf{x}_{k}\in X\cup Y}\omega(\mathbf{x}_{k}:\ t_{i},\alpha)\omega(\mathbf{x}_{k}:\ t_{j},\alpha)\ \phi\left((x_{k},t_{k}-t_{i}):\ X\right)\log\frac{\phi\left((x_{k},t_{k}-t_{i}):\ X\right)}{\phi\left((x_{k},t_{k}-t_{j}):\ Y\right)}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
The shifting hasn't been explained
\end_layout

\end_inset


\end_layout

\begin_layout Standard
This definition of the distance metric essentially weights the contribution
 of each point's entropy to the divergence based on its distance from either
 of the windows defined by 
\begin_inset Formula $(t_{i},\alpha)$
\end_inset

 and 
\begin_inset Formula $(t_{j},\alpha)$
\end_inset

.
 
\end_layout

\begin_layout Standard
We can now reformulate the Parzen Window equations to use the windowing
 function in place of explicit subsets.
 For cumputational reasons we check windows centered on each of the points
 in the sets being compared:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\bar{\varphi}_{P}(\mathbf{x},Z:\ Y) & =\sum_{i=1}^{\ell}\sum_{j=1}^{|Z|}\frac{1}{\ell|Z|}\bar{K}_{\gamma}\left(\{Z\cup\mathbf{x}\},Y\right)\\
\bar{K}_{\gamma}\left(Z,Y:\ t_{i},t_{j}\alpha\right) & =\frac{1}{\gamma\sqrt{2\pi}}e^{-\frac{1}{\gamma}\|\phi_{Z},\phi_{Y}\|}\end{align*}

\end_inset


\end_layout

\begin_layout Subsection
Sliding Windowing Function
\end_layout

\begin_layout Standard
Thus far we have been dealing with a discrete set of windowing parameters,
 defined by the points being compared and some parameter 
\begin_inset Formula $\alpha$
\end_inset

.
 The approach as been to select windows over two sets of points, shift the
 two sets so the window is at 
\begin_inset Formula $t=0$
\end_inset

 and compare them to each other.
 We have established a method which windows both the testing data 
\begin_inset Formula $Y$
\end_inset

 and the training data 
\begin_inset Formula $Z$
\end_inset

 in order to handle localized patterns.
 We can simplify this process in two ways; by eliminating the windowing
 function over the testing data and by using a sliding window over the training
 data.
 In this approach we eliminate explicit windows and instead take the integral
 over all windows of the training data:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\bar{\varphi}_{sP}(\mathbf{x},Z:\ Y,\alpha) & =\sum_{i=1}^{\ell}\bar{K}_{\gamma}\left(\{Z\cup\mathbf{x}\},Y:\ t_{i},\alpha\right)\\
\bar{K}_{\gamma}\left(Z,Y:\ t,\alpha\right) & =\frac{1}{\gamma\sqrt{2\pi}}e^{-\frac{1}{\gamma}\|\phi_{Z},\phi_{Y}:\ t,\alpha\|}\\
\|\phi_{x},\phi_{Y}:\ t,\alpha\| & =\int_{-\infty}^{\infty}D_{KL}(\phi_{n},\phi_{m}:\ \tau,t,\alpha)+D_{KL}(\phi_{m},\phi_{n}:\ t,\tau,\alpha)d\tau\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We now turn out attention to the distance metric 
\begin_inset Formula $\|\cdot,\cdot\|$
\end_inset

.
 We mentioned earlier that the Kullback-Liebler distance cannot be integrated
 in the context of sliding Parzen Windows.
 We address this shortcoming by substituting the Pearson Divergence for
 the Kullbak-Liebler divergence.
 We will further simplify our calculations by omitting the symmetrizing
 redundancy.
 While this choice means that 
\begin_inset Formula $\varphi(\mathbf{x},Y:\ Z)\neq\varphi(\mathbf{x},Z:\ Y)$
\end_inset

, in normal practice this is not a substantial sacrifice.
 We can now restate the distance metric as:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\|\phi_{x},\phi_{Y}:\ t,\alpha\| & =\sum_{\mathbf{x}_{i}\in X\cup Y}\omega(t_{i}:\ t,\alpha)\int_{-\infty}^{\infty}\left(\frac{\phi\left((x_{i},t_{i}-\tau):\ X\right)}{\phi_{Y}\left((x_{i},t_{i}):\ Y\right)}-1\right)^{2}d\tau\end{align*}

\end_inset


\end_layout

\begin_layout Standard
It can be shown that this reduces to the following:
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
\begin_inset Formula \begin{align*}
\int\left(\frac{\bar{\phi}(\mathbf{x}_{i}:\ X,\tau)}{\phi(\mathbf{x}_{i}:\ Y)}-1\right)^{2}d\tau & =\int\left(\frac{\sum_{\mathbf{x}_{i}\in X}\ k_{\gamma}\left((x,t+\tau),(x_{i},t_{i})\right)}{\phi(\mathbf{x}_{i}:\ Y)}-1\right)^{2}d\tau\\
 & =\int\frac{\left(\sum_{\mathbf{x}_{i}\in X}\ k_{\gamma}\left((x,t+\tau),(x_{i},t_{i})\right)\right)^{2}}{\phi(\mathbf{x}_{i}:\ Y)^{2}}d\tau-\int\frac{\sum_{\mathbf{x}_{i}\in X}\ 2k_{\gamma}\left((x,t+\tau),(x_{i},t_{i})\right)}{\phi(\mathbf{x}_{i}:\ Y)}d\tau\\
 & =\int\frac{\sum_{\mathbf{x}_{i}\in X}\sum_{\mathbf{x}_{j}\in X}\ k_{\gamma}\left((x,t+\tau),(x_{i},t_{i})\right)k_{\gamma}\left((x,t+\tau),(x_{j},t_{j})\right)}{\phi(\mathbf{x}_{i}:\ Y)^{2}}d\tau-\int\frac{\sum_{\mathbf{x}_{i}\in X}\ 2k_{\gamma}\left((x,t+\tau),(x_{i},t_{i})\right)}{\phi(\mathbf{x}_{i}:\ Y)}d\tau\\
 & =\sum_{\begin{array}{c}
\mathbf{x}_{i}\in X\\
\mathbf{x}_{j}\in X\end{array}}\int\frac{k_{\gamma}\left((x,t+\tau),(x_{i},t_{i})\right)k_{\gamma}\left((x,t+\tau),(x_{j},t_{j})\right)}{\phi(\mathbf{x}_{i}:\ Y)^{2}}d\tau-\sum_{\mathbf{x}_{i}\in X}\int\frac{2k_{\gamma}\left((x,t+\tau),(x_{i},t_{i})\right)}{\phi(\mathbf{x}_{i}:\ Y)}d\tau\\
 & =\sum_{\begin{array}{c}
\mathbf{x}_{i}\in X\\
\mathbf{x}_{j}\in X\end{array}}\int\frac{e^{-\frac{1}{\gamma}\left((t+\tau-t_{i})^{2}+(t+\tau-t_{j})^{2}+(x-x_{i})^{2}+(x-x_{j})^{2}\right)}}{\gamma^{2}2\pi\phi(\mathbf{x}_{i}:\ Y)^{2}}d\tau-\sum_{\mathbf{x}_{i}\in X}\int\frac{2e^{-\frac{1}{\gamma}\left((t+\tau-t_{i})^{2}+(x-x_{i})^{2}\right)}}{\gamma\sqrt{2\pi}\phi(\mathbf{x}_{i}:\ Y)}d\tau\\
 & =\sum_{\begin{array}{c}
\mathbf{x}_{i}\in X\\
\mathbf{x}_{j}\in X\end{array}}\lim_{n\rightarrow\infty}\frac{e^{-\frac{1}{2\gamma}\left((t-t_{i})^{2}-2(t-t_{i})(t-t_{j})+(t-t_{j})^{2}+2(x-x_{i})^{2}+2(x-x_{j})^{2}\right)}\sqrt{\frac{\pi}{2}}\text{erf}\left(\frac{\sqrt{\frac{1}{\gamma}}\left(2t-t_{i}-t_{j}+2n\right)}{\sqrt{2}}\right)}{2\sqrt{\frac{1}{\gamma}}\gamma^{2}2\pi\phi(\mathbf{x}_{i}:\ Y)^{2}}\\
 & \qquad\qquad\qquad-\sum_{\begin{array}{c}
\mathbf{x}_{i}\in X\\
\mathbf{x}_{j}\in X\end{array}}\lim_{n\rightarrow-\infty}\frac{e^{-\frac{1}{2\gamma}\left((t-t_{i})^{2}-2(t-t_{i})(t-t_{j})+(t-t_{j})^{2}+2(x-x_{i})^{2}+2(x-x_{j})^{2}\right)}\sqrt{\frac{\pi}{2}}\text{erf}\left(\frac{\sqrt{\frac{1}{\gamma}}\left(2t-t_{i}-t_{j}+2n\right)}{\sqrt{2}}\right)}{2\sqrt{\frac{1}{\gamma}}\gamma^{2}2\pi\phi(\mathbf{x}_{i}:\ Y)^{2}}\\
 & \qquad\qquad\qquad-\sum_{\mathbf{x}_{i}\in X}\lim_{n\rightarrow\infty}\frac{e^{-\frac{1}{\gamma}\left((x-x_{i})^{2}+(x-x_{j})^{2}\right)}\sqrt{\pi}\text{erf}\left(\sqrt{\frac{1}{\gamma}}\left((t-t_{i})+n\right)\right)}{\sqrt{\frac{1}{\gamma}}\gamma\sqrt{2\pi}\phi(\mathbf{x}_{i}:\ Y)}\\
 & \qquad\qquad\qquad+\sum_{\mathbf{x}_{i}\in X}\lim_{n\rightarrow-\infty}\frac{e^{-\frac{1}{\gamma}\left((x-x_{i})^{2}+(x-x_{j})^{2}\right)}\sqrt{\pi}\text{erf}\left(\sqrt{\frac{1}{\gamma}}\left((t-t_{i})+n\right)\right)}{\sqrt{\frac{1}{\gamma}}\gamma\sqrt{2\pi}\phi(\mathbf{x}_{i}:\ Y)}\\
 & =\sum_{\begin{array}{c}
\mathbf{x}_{i}\in X\\
\mathbf{x}_{j}\in X\end{array}}\frac{\sqrt{\frac{\pi}{2}}e^{-\frac{1}{2\gamma}\left((t-t_{i})^{2}-2(t-t_{i})(t-t_{j})+(t-t_{j})^{2}+2\sum_{c}(x^{c}-x_{i}^{c})^{2}+(x^{c}-x_{j}^{c})^{2}\right)}}{\sqrt{\frac{1}{\gamma}}\gamma^{2}2\pi\phi(\mathbf{x}_{i}:\ Y)^{2}}-\sum_{\mathbf{x}_{i}\in X}\frac{2\sqrt{\pi}e^{-\frac{1}{\gamma}\left((x-x_{i})^{2}+(x-x_{j})^{2}\right)}}{\sqrt{\frac{1}{\gamma}}\gamma\sqrt{2\pi}\phi(\mathbf{x}_{i}:\ Y)}\end{align*}

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Check that equation to make sure the subscripts haven't gotten mixed up.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\|\phi_{x},\phi_{Y}:\ t,\alpha\| & =\sum_{\begin{array}{c}
\mathbf{x}_{i}\in X\\
\mathbf{x}_{j}\in X\\
\mathbf{x}_{k}\in X\end{array}}\frac{\omega(t_{i}:\ t,\alpha)\sqrt{\frac{\pi}{2}}e^{-\frac{1}{2\gamma}\left((t_{i}-t_{j})^{2}-2(t_{i}-t_{j})(t_{i}-t_{k})+(t_{i}-t_{k})^{2}+2\sum_{c}(x_{i}^{c}-x_{j}^{c})^{2}+(x_{i}^{c}-x_{k}^{c})^{2}\right)}}{\sqrt{\frac{1}{\gamma}}\gamma^{2}2\pi\phi\left((x_{i},t_{i}):\ Y\right)^{2}}-\sum_{\begin{array}{c}
\mathbf{x}_{i}\in X\\
\mathbf{x}_{j}\in X\end{array}}\frac{\omega(t_{i}:\ t,\alpha)2\sqrt{\pi}e^{-\frac{1}{\gamma}\left((x_{i}-x_{j})^{2}+(x_{i}-x_{k})^{2}\right)}}{\sqrt{\frac{1}{\gamma}}\gamma\sqrt{2\pi}\phi\left((x_{i},t_{i}-t):\ Y\right)}\end{align*}

\end_inset


\end_layout

\begin_layout Section
Support Vector Estimation
\end_layout

\begin_layout Standard
The Parzen Window method is neither sparse nor computationally efficient,
 and as the number of observations grows, these deficiencies quickly become
 prohibitive.
 We now investigate the use of Support Vector Machines to generate estimates.
\end_layout

\begin_layout Subsection
Generalized Parzen-SVM 
\end_layout

\begin_layout Standard
Support Vector Machines are often used
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
ref?
\end_layout

\end_inset

 to estimate probability distributions by solving the related problem of
 estimating the cumulative distribution function of the random variable
 in question.
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
This isn't really the essence of SVM - put more verbage in the intro regarding
 what SVM's are, why they work, and why they're superior to other approaches
 (say, neural networks)
\end_layout

\end_inset

 This reduces the problem to one of estimating a non-linear mapping from
 observations to cumulative distribution values, which can be formulated
 as an optimization problem over a linear operator equation.
 Unfortunately, these methods depend on the ability to calculate an empirical
 distribution for each observation:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
F_{\ell}(x)=\frac{1}{\ell}\sum_{i}\theta(x-x_{i})\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\theta(x)$
\end_inset

 is the indicator function.
 To evaluate this function, the abstract space 
\begin_inset Formula $\Omega^{\mathcal{S}}$
\end_inset

 must be ordered.
 While we have described a distance metric over 
\begin_inset Formula $\Omega^{\mathcal{S}}$
\end_inset

, it is not clear what a meaningful ordering relation would be.
\end_layout

\begin_layout Standard
Rather than calculating the cumulative probability distribution of 
\begin_inset Formula $X$
\end_inset

, we begin with the assumption that the Parzen Window estimate of the probabilit
y distribution is acceptably accurate and attempt to minimize the difference
 between the Support Vector estimate and the Parzen Window estimate.
 The support vector approach requires that we define an optimization problem
 
\begin_inset Formula $W(\beta)$
\end_inset

 which will determine which observations (or as we shall see, windows) will
 be used in estimations and which can be discarded as redundant or irrelevant
 information.
 We therefore define a set of weights 
\begin_inset Formula $\beta$
\end_inset

, some of which will be non-zero at the conclusion of our optimization task.
 We define the optimization problem as minimizing the square loss between
 the Support Vector and Parzen Window estimates over some set of observations
 
\begin_inset Formula $X$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
W(\beta) & =\min_{\beta}\sum_{\mathbf{x}\in X}\ \sum_{i=1}^{\ell}\left(\varphi_{P}(\mathbf{x})-\varphi_{SV}(\mathbf{x}:\ \beta_{i})\right)^{2}+\lambda\Omega\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
explain regularizer
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Before discussing the nature of this optimization task, we must establish
 the Support Vector estimation function 
\begin_inset Formula $\varphi_{SV}$
\end_inset

.
 The support vector target functional is identical to the Parzen Window
 estimator, with the exception that rather than averaging over all the observati
ons in the training set (or integrating over all offsets), we use the weighted
 sum of each training observation:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\varphi_{SV}(\mathbf{x},Z:\ \alpha,\beta,Y) & =\sum_{i=1}^{\ell}\beta_{i}\bar{K}_{\gamma}\left(\{Z\cup\mathbf{x}\},Y:\ t_{i},\alpha\right)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We can now develop a method of evaluating the 'usefulness' of time window
 in a set of training data.
 To do this we begin by observing that the duration of the training set
 is likely larger than the duration of test sets, which means we will have
 to make estimates using less information that is available in the training
 data; whatever optimization problem we define will need to take this into
 account.
 The optimization equation compares the results of Parzen estimation to
 the results of SVM estimation using a portion of the training data as synthetic
 testing data.
 Fortunately, we have already developed an understanding of a windowing
 function which can be applied to this situation.
 For the optimization problem, we check the difference between the two estimates
 only at windows defined by the observations in the training set.
 The set of weights used in the Support Vector estimation must have a discrete
 number of elements; for simplicity we choose to assign a weight to each
 window defined by the time value of an observation in the training set
 and the constant parameter 
\begin_inset Formula $\alpha$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
W(\beta:\ Y,\alpha) & \longmapsto\min_{\beta}\sum_{\mathbf{x}\in Y}\ \sum_{i=1}^{\ell}\left(\varphi_{P}(\mathbf{x},Y:\ Y)-\varphi_{SV}(\mathbf{x},Y:\ \alpha,\beta_{i},Y)\right)^{2}+\lambda\Omega\\
 & =\sum_{\mathbf{x}\in Y}\ \sum_{i=1}^{\ell}\varphi_{P}(\mathbf{x},Y:\ Y)^{2}-2\varphi_{P}(\mathbf{x},Y:\ Y)\varphi_{SV}(\mathbf{x},Y:\ \alpha,\beta_{i},Y)+\varphi_{SV}(\mathbf{x},Y:\ \alpha,\beta_{i},Y)^{2}+\lambda\Omega\\
 & =\sum_{\mathbf{x}\in Y}\ \sum_{\begin{array}{c}
i=1\\
j=1\end{array}}^{\ell}\beta_{i}\beta_{j}\bar{K}_{\gamma}\left(\{Z\cup\mathbf{x}\},Y:\ t_{i},\alpha\right)\bar{K}_{\gamma}\left(\{Z\cup\mathbf{x}\},Y:\ t_{j},\alpha\right)-\sum_{i=1}^{\ell}\beta_{i}2\varphi_{P}(\mathbf{x},Y:\ Y)+\varphi_{P}(\mathbf{x},Y:\ Y)^{2}+\lambda\Omega\end{align*}

\end_inset


\end_layout

\begin_layout Standard
This is a quadratic optimization problem and can be solved with known methods.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Regularizer term?
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Parzen-SVM Decomposition 
\end_layout

\begin_layout Standard
Due to the simplicity of the constraints in the optimization problem, it
 is possible to use the decomposition method of Osuna to reduce the memory
 requirements of the Parzen-SVM algorithm.
\end_layout

\begin_layout Subsubsection
 Sub-Problem Definition 
\end_layout

\begin_layout Standard
The decomposition algorithm breaks 
\begin_inset Formula $X$
\end_inset

 into two working sets 
\begin_inset Formula $B,N$
\end_inset

, and attempts to optimize 
\begin_inset Formula $B$
\end_inset

 while keeping 
\begin_inset Formula $N$
\end_inset

 fixed.
 This results in the following iterative optimization problem where 
\begin_inset Formula $\boldsymbol{\beta}^{k}$
\end_inset

 denotes the result of the previous iteration:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
W(\boldsymbol{\beta}_{B}) & =\frac{1}{2}\begin{bmatrix}\boldsymbol{\beta}_{B}^{T} & (\boldsymbol{\beta}_{N}^{k})^{T}\end{bmatrix}\begin{bmatrix}P_{BB} & P_{BN}\\
P_{NB} & P_{NN}\end{bmatrix}\begin{bmatrix}\boldsymbol{\beta}_{B}\\
\boldsymbol{\beta}_{N}^{k}\end{bmatrix}-\begin{bmatrix}q_{B}^{T} & q_{N}^{T}\end{bmatrix}\begin{bmatrix}\boldsymbol{\beta}_{B}\\
\boldsymbol{\beta}_{N}^{k}\end{bmatrix}\\
 & =\frac{1}{2}\boldsymbol{\beta}_{B}^{T}P_{BB}\boldsymbol{\beta}_{B}-(-q_{B}+P_{BN}\boldsymbol{\beta}_{N}^{k})^{T}\boldsymbol{\beta}_{B}\\
 & =\frac{1}{2}\begin{bmatrix}\beta_{i} & \beta_{j}\end{bmatrix}\begin{bmatrix}P_{ii} & P_{ij}\\
P_{ij} & P_{jj}\end{bmatrix}\begin{bmatrix}\beta_{i}\\
\beta_{j}\end{bmatrix}-(-q_{B}+P_{BN}\boldsymbol{\beta}_{N}^{k})^{T}\begin{bmatrix}\beta_{i}\\
\beta_{j}\end{bmatrix}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\text{subject to}\quad0\le\beta_{i},\beta_{j},\quad\beta_{i}+\beta_{j}=1-\mathbf{1}^{T}\boldsymbol{\beta}_{N}^{k}\end{equation}

\end_inset


\end_layout

\begin_layout Subsubsection
 Working Set Selection 
\end_layout

\begin_layout Standard
Select
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align}
 & i\in\text{arg}\max_{t}\left\{ -\nabla f(\boldsymbol{\beta}^{k})_{t}\ |\quad t\in I(\boldsymbol{\beta}^{k})\right\} \\
 & j\in\text{arg}\min_{t}\left\{ -\frac{b_{it}^{2}}{a_{it}}\ |\quad t\in I(\boldsymbol{\beta}^{k}),\quad-\nabla f(\boldsymbol{\beta}^{k})_{t}<-\nabla f(\boldsymbol{\beta}^{k})_{i}\right\} \end{align}

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% This needs to be checked for the new optimization scenario
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align}
I(\boldsymbol{\beta}) & \equiv\{t\ |\quad\beta_{t}<1\quad\text{or}\quad\beta_{t}>0\}\\
 & a_{it}=P_{ii}+P_{tt}-2P_{it}\\
 & \bar{a}_{it}=\begin{cases}
a_{it} & \text{if}\ a_{it}>0\\
\delta & \text{otherwise}\end{cases}\\
 & b_{it}=-\nabla f(\boldsymbol{\beta}^{k})_{i}+\nabla f(\boldsymbol{\beta}^{k})_{t}\\
\nabla f(\boldsymbol{\beta})_{i} & \equiv P_{i}\boldsymbol{\beta}-q_{i}\end{align}

\end_inset


\end_layout

\begin_layout Subsubsection
 Stopping Condition 
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\max_{i\in I(\boldsymbol{\alpha}^{k})}-\nabla f(\boldsymbol{\alpha})_{i}+\min_{j\in I(\boldsymbol{\alpha}^{k})}\nabla f(\boldsymbol{\alpha})_{j}\le\epsilon\end{equation}

\end_inset


\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Subsection
Eunite Competition Data
\end_layout

\begin_layout Subsection
Santa Fe Data
\end_layout

\begin_layout Subsection
CATS Benchmark Data
\end_layout

\begin_layout Subsection
Results Summary
\end_layout

\begin_layout Section
Further Research
\end_layout

\begin_layout Subsection
Data Pre-Processing
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% logistic function using mean and sd to put most training points between
 .1 and .9
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% upper and lower bounds on 
\backslash
(
\backslash
int 
\backslash
Delta 
\backslash
) as non-stationary data detections mechanism
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% logistic function from delta using mean and sd in same way if data non-station
ary
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% iterative integration process until stationary data found ?
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Ensemble System
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% more analysis of recent data, some type of transfer from short to long-term
 'memory'
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% heirarchical system?
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% what parameters change?
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Weighted Influence
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% a weight term could be added to the regularizer to control the influence
 of specific sets.
  This could provide more information in 'important' situations, which could
 be useful in motivated learning.
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Standard
Eat it, bitches
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
nocite{Moreno03}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
bibliographystyle{plain}
\end_layout

\end_inset

 
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "Research/research.bib"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
