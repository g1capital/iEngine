#LyX 1.6.2 created this file. For more info see http://www.lyx.org/
\lyxformat 345
\begin_document
\begin_header
\textclass article
\begin_preamble



\usepackage{amsfonts}
\end_preamble
\use_default_options false
\language english
\inputencoding auto
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\paperfontsize 10
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 0
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\leftmargin 1.25in
\topmargin 1in
\rightmargin 1.25in
\bottommargin 1.25in
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\author "" 
\author "" 
\end_header

\begin_body

\begin_layout Title
Motivated Decision-Making using Transformation Invariant Probability Estimates
\end_layout

\begin_layout Author
Ryan Michael
\begin_inset Newline newline
\end_inset

 
\family typewriter
kerinin@gmail.com
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Subsection
Existing Work
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Expectation-maximization_algorithm
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Stationary_process
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Ergodicity
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Mixing_(mathematics)
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Lyapunov_exponent
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Recurrence_quantification_analysis
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Recurrence_plot
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Autoregressive_fractionally_integrated_moving_averag
e
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Autocorrelation
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
url{http://en.wikipedia.org/wiki/Linear_discriminant_analysis}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Hidden Markov Model
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% cannot account for future states - only capable of prediction
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% weak, short-term memory
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Box-Jenkins
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% intended for simplistic processes with well-understood stationarity and
 periodicity
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% http://en.wikipedia.org/wiki/Box-Jenkins
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Spectral Analysis
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% assumes some type of frequency-domain decomposition.
  Frequency-domain signal representations do not do a very good job predicting
 time-domain values.
 
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Shrinking-
\begin_inset Formula $\epsilon$
\end_inset

 SVM Regression
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% theoretical foundation weak; only compensates for the relevance of recent
 data.
  See Markhov problem
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
General Overview
\end_layout

\begin_layout Standard
The goal is to create a method of statistical inference capable of processing
 multiplle sources of data which are both multi-variate and exhibits transformat
ion-invariant behaviors.
 This type of data is common, and developing a robust method of analysis
 has applications in many domains.
 
\end_layout

\begin_layout Standard
The most basic operation used in this system is the estimation of probability
 densities.
 Based on a set of observations drawn from some random process, we generate
 an estimate of the underlying probability distribution.
 This estimate tells us the probability of each point in the input space
 being observed.
 Areas of the input space in which a dense set of observations are observed
 are given high probability, while areas of the input space with few observation
s are given low probability.
\end_layout

\begin_layout Standard
We assume that observations are pulled from multiple independent sources,
 all of which respond to some underlying phenomena.
 For instance one set of observations could be from a microphone and another
 from a light detector.
 We do not know how the inputs are related internally, between inputs, or
 to what extent their behavior changes over time.
 For example one input could be a steady sine wave and another could be
 based on the decay of a radioactive isotope.
 In the former case previous observations of the source are useful, in the
 latter they're not.
 Alternately two inputs could be light detectors in the same room or they
 could be on different continents; in the former their input would be highly
 similar, in the latter not as much.
\end_layout

\begin_layout Section
Problem Setting
\end_layout

\begin_layout Standard
Our analysis take place in the context of a set of 
\begin_inset Formula $\ell$
\end_inset

 vector observations 
\begin_inset Formula $\vec{x}_{i}$
\end_inset

 of a hidden random variables 
\begin_inset Formula $X$
\end_inset

 with a real-valued event space of arbitrary dimension:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
X & =(\Omega,\mathcal{F},\mathcal{P})\\
\Omega & \in\mathbb{R}^{d}\\
X & =[\vec{x}_{1},...,\vec{x}_{\ell}]\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Our objective is to develop an estimate 
\begin_inset Formula $\varphi(x:\ X)$
\end_inset

 of the probability of a given point 
\begin_inset Formula $x$
\end_inset

 given a set of observations 
\begin_inset Formula $X$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\varphi(x:\ X) & \longmapsto\Pr(x|X)\pm\xi\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We assume that our estimate's error is upper-bounded by 
\begin_inset Formula $\xi$
\end_inset

.
 
\end_layout

\begin_layout Section
Transformation Invariant Parzen Windows
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
General discussion of Parzen Windows
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Parzen Windows for i.i.d.
 Data
\end_layout

\begin_layout Standard
Our task is to estimate the probability of a given point 
\begin_inset Formula $\vec{x}$
\end_inset

 in the abstract space 
\begin_inset Formula $\Omega$
\end_inset

 based on a set of observations 
\begin_inset Formula $X$
\end_inset

.
 One method of accomplishing this is by using the Parzen Window method.
 We choose the Parzen Window method because it allows us to estimate probabiliti
es of unordered sets, provided they have an addition operation and a kernel
 function exists to provide a distance metric.
 The basic operation of the Parzen Window method is to estimate the probability
 of a point based on the sum of the distance from that point to each point
 in a set of prior observations.
 The Parzen Window approach to PDF estimation is as follows; given a set
 of prior observations 
\begin_inset Formula $X$
\end_inset

, a kernel function 
\begin_inset Formula $K_{\gamma}(\cdot,\cdot)$
\end_inset

 with width parameter 
\begin_inset Formula $\gamma$
\end_inset

, the probability of a point 
\begin_inset Formula $x$
\end_inset

 is determined by :
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\varphi(\vec{x}:\ X) & =\sum_{i}^{\ell}\frac{1}{\ell}K_{\gamma}(\vec{x},\vec{x}_{i})\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Multiple kernel functions exist, in this paper we will use the Radial Basis
 Function (RBF) kernel:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
K_{\gamma}\left(\vec{x},\vec{y}\right) & =\prod_{\upsilon=1}^{d}\frac{1}{\gamma\sqrt{2\pi}}e^{-\frac{1}{\gamma}\|x^{\nu},y^{\nu}\|^{2}}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset Formula $\|\cdot,\cdot\|$
\end_inset

 is some metric, for instance the 
\begin_inset Formula $L^{2}$
\end_inset

 distance:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\|x,y\| & =\left(x-y\right)^{2}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
This approach is useful in the context of a set of observations drawn i.i.d.
 from a statid probability density function (PDF), however it is unable
 to address transformation invariance.
 Transformation invariance (TI) refers to datasets in which one or multiple
 PDF's occur in various transformed states within the dataset, for example
 a given sound (described by a known PDF) could be repeated multiple times
 in an audio recording, or a given image could appear in multiple locations
 in an image.
 While the Parzen Window method can generate an estimate in the presence
 of data generated by transformed PDF's, its inability to recognize multiple
 instances of a single PDF limits its power.
 
\end_layout

\begin_layout Subsection
Contextual Estimation
\end_layout

\begin_layout Standard
The crucial distinction between i.i.d data from a static PDF and data which
 exhibits transformational invariance is that when estimating the probability
 of a point, you must consider to context of the point as well as the location
 of the point in 
\begin_inset Formula $\Omega$
\end_inset

.
 In this paper we will establish the context of 
\begin_inset Formula $x$
\end_inset

 by defining a 'neighborhood' of points around 
\begin_inset Formula $x$
\end_inset

.
 We will do so through the use of a windowing kernel function 
\begin_inset Formula $\omega(\cdot)$
\end_inset

 which returns a multiplier to describe the distance between an arbitrary
 point and 
\begin_inset Formula $x$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\omega_{\alpha}(x_{i},x_{j}) & \Rightarrow[0,\infty)\\
\int_{-\infty}^{\infty}\omega_{\alpha}(x_{i},x_{j})dx_{i} & =1\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We can use any kernel function for 
\begin_inset Formula $\omega(\cdot)$
\end_inset

, however we assume that the windowing function has a peak at 
\begin_inset Formula $\Delta=0$
\end_inset

 and that it drops off as 
\begin_inset Formula $\Delta\rightarrow\pm\infty$
\end_inset

.
\end_layout

\begin_layout Subsection
Parzen Windows for Transformation-Invariant Data
\end_layout

\begin_layout Standard
Because we must establish a context for TI data, we must think of the set
 of observations 
\begin_inset Formula $X$
\end_inset

 as part of both the problem definition and the solution.
 This means that we can no longer simply calculate the kernel distance between
 
\begin_inset Formula $x$
\end_inset

 and each of the observations in 
\begin_inset Formula $X$
\end_inset

 independantly - we must compare the 
\emph on
context
\emph default
 of 
\begin_inset Formula $x$
\end_inset

 with 
\begin_inset Formula $X$
\end_inset

.
 This requires a kernel function capable of comparing two 
\emph on
sets of points
\emph default
.
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
\begin_inset Formula \begin{align*}
K_{\gamma}\left(X_{i},X_{j}\right) & =\prod_{\upsilon=1}^{d}\frac{1}{\gamma\sqrt{2\pi}}e^{-\frac{1}{\gamma}\|X_{i}^{\nu},X_{j}^{\nu}\|}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We can simplify this equation by observing that we are not actually dealing
 with two sets; we're dealing with the a set of observations 
\begin_inset Formula $X$
\end_inset

, a point whose probability we'd like to estimate 
\begin_inset Formula $x$
\end_inset

, and a neighborhood defined by 
\begin_inset Formula $x$
\end_inset

 and some parameter 
\begin_inset Formula $\alpha$
\end_inset

:
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
\begin_inset Formula \begin{align*}
K_{\gamma}\left(\vec{x}_{n},\vec{x}_{m},X\right) & =\prod_{\upsilon=1}^{d}\frac{1}{\gamma\sqrt{2\pi}}e^{-\frac{1}{\gamma}\|x_{n}^{\nu},x_{m}^{\nu},X\|}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Notice we've added 
\begin_inset Formula $\alpha$
\end_inset

 to the distance metric to allow each point in 
\begin_inset Formula $X$
\end_inset

 to contribute based on it's proximity to the neighborhood of 
\begin_inset Formula $x$
\end_inset

 rather than attempting to explicitly define the neighborhood as a discrete
 set.
 This raises the question not only of how each point's influence is modified
 but of how we define the distance between two sets of observations.
 There are multiple divergence measures available which can be used to define
 a 'distance' between two sets.
 We will later show the importance of the distance metric 
\begin_inset Formula $\|\cdot,\cdot\|$
\end_inset

 being integrable; for this reason we will use the Pearson Divergence as
 our distance metric when comparing sets.
 In order to acommodate TI comparisons, we add a linear transform 
\begin_inset Formula $(\mathbf{A},\mathbf{b})$
\end_inset

 to the windowed set.
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\|X^{\nu},Y^{\nu}\| & =\sum_{x^{\nu}\in\{X\cup Y\}}\left(\frac{\varphi(x^{\nu}:\ \mathbf{A}^{\nu}X^{\nu}+\mathbf{b}^{\nu})}{\varphi(x^{\nu}:\ Y^{\nu})}-1\right)^{2}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Notice that we are transforming the windowed set of data points 
\emph on
but
\emph default
 
\emph on
not
\emph default
 
\begin_inset Formula $x^{v}$
\end_inset

.
 What we're trying to do is match transformed points in the windowed set
 to existing points in the full set - this tells us the 'distance' between
 the windowed set and the full set.
 We also want to establish how adding the point 
\begin_inset Formula $x^{\nu}$
\end_inset

 to the windowed set affects the correlation between the two sets (this
 is how we determine the probability of 
\begin_inset Formula $x^{\nu}$
\end_inset

).
 We're only testing a discrete subset possible values of 
\begin_inset Formula $x^{\nu}$
\end_inset

 (we can't integrate over all possible points because resulting equation
 isn't integrable), and we want to make sure that when we calculate the
 difference measure we use a 'good' set of points .
 Since we're assuming the windowed data contains all the points in 
\begin_inset Formula $X$
\end_inset

 this shouldn't be a problem, and using the points in 
\begin_inset Formula $X$
\end_inset

 to test the distance between the windowed set and 
\begin_inset Formula $X$
\end_inset

 should give us an accurate measure of the correlation between the two.
\end_layout

\begin_layout Standard
Because the Pearson Divergence is a summation, we can control the influence
 of each point by scaling it using the windowing function:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\|x_{n}^{\nu},x_{m}^{\nu},X\| & =\sum_{i=1}^{\ell}\omega_{\alpha}(x_{n}^{\nu},x_{i}^{\nu})\left(\frac{\varphi(x_{i}^{\nu}:\ x_{m}^{\nu},\mathbf{A}X+\mathbf{b})}{\varphi(x_{i}^{\nu}:\ x_{m}^{\nu},X^{\nu})}-1\right)^{2}\\
\varphi(x^{\nu}:\ x_{n}^{\nu},X) & =\sum_{i=1}^{\ell}\frac{\omega_{\alpha}(x_{i}^{\nu},x_{n}^{\nu})}{\sum_{j=1}^{\ell}\omega_{\alpha}(x_{j}^{\nu},x_{n}^{\nu})}K_{\gamma}(x^{\nu},x_{i}^{\nu})\\
 & =\sum_{i=1}^{\ell}\ddot{\omega}_{\alpha}(x_{i}^{\nu},x_{n}^{\nu},X)\ K_{\gamma}(x^{\nu},x_{i}^{\nu})\end{align*}

\end_inset


\end_layout

\begin_layout Standard
In both of these equations we have used 
\begin_inset Formula $(\mathbf{A},\mathbf{b})$
\end_inset

 without commenting on their definition.
 These two matrix transformations are used to shift, scale, rotate, shear
 or mirror 
\begin_inset Formula $\{x\cup X\}$
\end_inset

 prior to comparing it to 
\begin_inset Formula $X$
\end_inset

.
 The first transformation matrix 
\begin_inset Formula $\mathbf{A}$
\end_inset

 is an 
\begin_inset Formula $(\bar{d}+d_{x})\times(\bar{d}+d_{x})$
\end_inset

 matrix.
 The linear operator 
\begin_inset Formula $\mathbf{A}X$
\end_inset

 allows us to scale, rotate, shear, and mirror 
\begin_inset Formula $X$
\end_inset

 depending on the matrix values of 
\begin_inset Formula $\mathbf{A}$
\end_inset

.
 The second transformation amtrix 
\begin_inset Formula $\mathbf{b}$
\end_inset

 is a 
\begin_inset Formula $(\bar{d}+d_{x})\times1$
\end_inset

 matrix; adding these terms together allows us to shift 
\begin_inset Formula $X$
\end_inset

 along any axis based on the values of 
\begin_inset Formula $\mathbf{b}$
\end_inset

.
 One approach to these transformations is to explicitly determine values
 for the two matrices and to determine the distance 
\begin_inset Formula $\|\cdot,\cdot\|$
\end_inset

 using these values.
 A more robust approach is to integrate over all possible values of 
\begin_inset Formula $(\mathbf{A},\mathbf{b}):$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\|x_{n}^{\nu},x_{m}^{\nu},X:\ \alpha\| & \sum_{x_{i}^{\nu}\in\{x\cup X\}}\omega_{\alpha}(x_{n}^{\nu},x_{i}^{\nu})\int\int_{-\infty}^{\infty}\left(\frac{\varphi(x_{i}^{\nu}:\ x_{m}^{\nu},\mathbf{A}X+\mathbf{b})}{\varphi(x_{i}^{\nu}:\ x_{m}^{\nu},X^{\nu})}-1\right)^{2}d\mathbf{A}d\mathbf{b}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
This approach allows us to compare the distance between any linear transform
 of 
\begin_inset Formula $\{x\cup X\}$
\end_inset

 and 
\begin_inset Formula $X$
\end_inset

 - a far more powerful and less computationally demanding approach.
 Doing so requires that we calculate the following integral (in which we
 assume 
\begin_inset Formula $x\in\mathbb{R}^{1}$
\end_inset

 and 
\begin_inset Formula $X\rightarrow[|X|,1]$
\end_inset

] ):
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\int\int_{-\infty}^{\infty}\left(\frac{\varphi(x_{i}^{\nu}:\ x_{m}^{\nu},\mathbf{A}X+\mathbf{b})}{\varphi(x_{i}^{\nu}:\ x_{m}^{\nu},X^{\nu})}-1\right)^{2}d\mathbf{A}d\mathbf{b}\]

\end_inset


\end_layout

\begin_layout Standard
Because 
\begin_inset Formula $(\mathbf{A},\mathbf{b})$
\end_inset

 is a pair of matrix transformation, we must integrate the previous equation
 element-wise:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\int\int_{-\infty}^{\infty}\left(\frac{\varphi(x^{\nu}:\ x_{n}^{\nu},\mathbf{A}X+\mathbf{b})}{\varphi(x^{\nu}:\ x_{n}^{\nu},X)}-1\right)^{2}d\mathbf{A}d\mathbf{b} & =\int...\int\left(\frac{\varphi\left(x^{\nu}:\ x_{n}^{\nu},\left[\mathbf{b}_{\nu}+\sum_{n=1}^{d}\mathbf{A}_{\nu,n}x_{i}^{n}|\ x_{i}\in X\right]\right)}{\varphi(x^{\nu}:\ x_{n}^{\nu},X)}-1\right)^{2}d\mathbf{A}_{\nu,0}...d\mathbf{A}_{\nu,d}d\mathbf{b}_{\nu}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
 & =\int...\int\left(\frac{\sum_{i=1}^{\ell}\ddot{\omega}_{\alpha}(x_{i}^{\nu},x_{n}^{\nu},X)\ K_{\gamma}\left(x^{\nu},\left(\mathbf{b}_{\nu}+\sum_{n=1}^{d}\sum_{i=1}^{\ell}\mathbf{A}_{\nu,n}x_{i}^{n}\right)\right)}{\sum_{i}^{\ell}\ddot{\omega}_{\alpha}(x_{i}^{\nu},x_{n}^{\nu},X)\ K_{\gamma}(x^{\nu},X)}-1\right)^{2}d\mathbf{A}_{\nu,0}...d\mathbf{A}_{\nu,d}d\mathbf{b}_{\nu}\\
 & =\int...\int\left(\frac{\sum_{i=1}^{\ell}\ddot{\omega}_{\alpha}(x_{i}^{\nu},x_{n}^{\nu},X)}{\sum_{i=1}^{\ell}\ddot{\omega}_{\alpha}(x_{i}^{\nu},x_{n}^{\nu},X)\ \exp\left(-\frac{1}{\gamma}\left(x^{\nu}-x_{i}^{\nu}\right)^{2}\right)}\ \exp\left(-\frac{1}{\gamma}\left(x^{\nu}-\left(\mathbf{b}_{\nu}+\sum_{n=1}^{d}\sum_{i=1}^{\ell}\mathbf{A}_{\nu,n}x_{i}^{n}\right)\right)^{2}\right)-1\right)^{2}d\mathbf{A}_{\nu,0}...d\mathbf{A}_{\nu,d}d\mathbf{b}_{\nu}\\
 & =\int...\int\left(\frac{\sum_{i=1}^{\ell}\ddot{\omega}_{\alpha}(x_{i}^{\nu},x_{n}^{\nu},X)}{\sum_{i=1}^{\ell}\ddot{\omega}_{\alpha}(x_{i}^{\nu},x_{n}^{\nu},X)\ \exp\left(-\frac{1}{\gamma}\left(x^{\nu}-x_{i}^{\nu}\right)^{2}\right)}\ \exp\left(-\frac{1}{\gamma}\left(\left(x^{\nu}\right)^{2}-2x^{\nu}\left(\mathbf{b}_{\nu}+\sum_{n=1}^{d}\sum_{i=1}^{\ell}\mathbf{A}_{\nu,n}x_{i}^{n}\right)+\left(\mathbf{b}_{\nu}+\sum_{n=1}^{d}\sum_{i=1}^{\ell}\mathbf{A}_{\nu,n}x_{i}^{n}\right)^{2}\right)\right)-1\right)^{2}d\mathbf{A}_{\nu,0}...d\mathbf{A}_{\nu,d}d\mathbf{b}_{\nu}\\
 & =\int...\int\left(\frac{\sum_{i=1}^{\ell}\ddot{\omega}_{\alpha}(x_{i}^{\nu},x_{n}^{\nu},X)}{\sum_{i=1}^{\ell}\ddot{\omega}_{\alpha}(x_{i}^{\nu},x_{n}^{\nu},X)\ \exp\left(-\frac{1}{\gamma}\left(x^{\nu}-x_{i}^{\nu}\right)^{2}\right)}\ \exp\left(-\frac{1}{\gamma}\left(\left(x^{\nu}\right)^{2}-2x^{\nu}\mathbf{b}_{\nu}+2x^{\nu}\sum_{n=1}^{d}\sum_{i=1}^{\ell}\mathbf{A}_{\nu,n}x_{i}^{n}+\mathbf{b}_{\nu}^{2}+\mathbf{b}_{\nu}\sum_{n=1}^{d}\sum_{i=1}^{\ell}\mathbf{A}_{\nu,n}x_{i}^{n}+\sum_{\begin{array}{c}
n=1\\
m=1\end{array}}^{d}\sum_{\begin{array}{c}
i=1\\
j=1\end{array}}^{\ell}\mathbf{A}_{\nu,n}x_{i}^{n}\mathbf{A}_{\nu,m}x_{j}^{m}\right)\right)-1\right)^{2}d\mathbf{A}_{\nu,0}...d\mathbf{A}_{\nu,d}d\mathbf{b}_{\nu}\\
 & =\int...\int\left(\frac{\sum_{i=1}^{\ell}\ddot{\omega}_{\alpha}(x_{i}^{\nu},x_{n}^{\nu},X)}{\sum_{i=1}^{\ell}\ddot{\omega}_{\alpha}(x_{i}^{\nu},x_{n}^{\nu},X)\ \exp\left(-\frac{1}{\gamma}\left(x^{\nu}-x_{i}^{\nu}\right)^{2}\right)}\ \exp\left(-\frac{1}{\gamma}\left(\sum_{\begin{array}{c}
n=1\\
m=1\end{array}}^{d}\sum_{\begin{array}{c}
i=1\\
j=1\end{array}}^{\ell}\mathbf{A}_{\nu,n}x_{i}^{n}\mathbf{A}_{\nu,m}x_{j}^{m}+\left(x^{\nu}\right)^{2}+\sum_{n=1}^{d}\sum_{i=1}^{\ell}(2x^{\nu}+\mathbf{b}_{\nu})\mathbf{A}_{\nu,n}x_{i}^{n}-2x^{\nu}\mathbf{b}_{\nu}+\mathbf{b}_{\nu}^{2}\right)\right)-1\right)^{2}d\mathbf{A}_{\nu,0}...d\mathbf{A}_{\nu,d}d\mathbf{b}_{\nu}\\
 & \simeq\int...\int\left(a\ \exp\left(-b\left(\mathbf{A}^{2}+\mathbf{A}c+\mathbf{A}d+f\right)\right)-1\right)^{2}\\
 & (\text{mathmatica})\\
 & =\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Finish this up
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Extend this to integrate over a subset of 
\begin_inset Formula $d$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Performance
\end_layout

\begin_layout Standard
Big-O
\end_layout

\begin_layout Subsection
Multiple Data Sources
\end_layout

\begin_layout Standard
Consider the case where data from multiple data sources contributes to our
 set of observations 
\begin_inset Formula $X$
\end_inset

, for instance the data drawn from a microphone and a video camera.
 Let us assume that both input sources are timestamped, but the sampling
 frequency is different for each source and that the vector data points
 have different dimensionality, we will refer to observations of the former
 as 
\begin_inset Formula $X=(\Omega^{X},\mathcal{F}^{X},\mathcal{P}^{X})$
\end_inset

 and the latter as 
\begin_inset Formula $Y=(\Omega^{Y},\mathcal{P}^{Y},\mathcal{F}^{Y})$
\end_inset

.
 In this situation we can treat the timestamp as a 'shared' dimension, but
 all other dimensions of the two vectors are independant.
 It is clear that if we intend to establish a PDF of the joint probability
 space 
\begin_inset Formula $(\Omega^{Y,X},\mathcal{P}^{Y,X},\mathcal{F}^{Y,X})$
\end_inset

, we must treat each dimension in 
\begin_inset Formula $\Omega^{X}$
\end_inset

 and 
\begin_inset Formula $\Omega^{Y}$
\end_inset

 as orthonormal to each other.
 The fact that both event spaces share a time dimension means that a TI
 analysis over the time dimension is likely to produce useful results.
 
\end_layout

\begin_layout Standard
The most straightforward way to handle this situation is to assume that
 vector observations constitute sparse matrices; for any given dimension
 of an observation 
\begin_inset Formula $\vec{x}$
\end_inset

, the value can either be a real number or null:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\vec{x}^{v} & \in[\mathbb{R},\textrm{Ø}]\end{align*}

\end_inset

 In this case we only include in our analysis operations between real-valued
 dimensions.
 We can easily accomodate the shared time dimension by including the two
 time dimensions in our TI analysis.
 This allows us to consider not only the situation where both inputs are
 timestamped with accurate clocks, but the situation where the two clocks
 are independantly inaccurate.
\end_layout

\begin_layout Subsection
Single Channel Summary
\end_layout

\begin_layout Standard
We have developed a PDF estimation technique which allows us to take advantage
 of TI data.
 The solution uses a novel distance metric to compare the similarity between
 two sets of points in the context of arbitrary linear transformations of
 one set.
 The solution proposed is restricted to observations with shared dimensionality,
 however it imposes no restrictions on which dimensions are TI.
 The types of transformations considered by the proposed solution are restricted
 to linear matrix transformations shifts.
 In the next section we will extend the solution to cases where multiple
 'channels' of data are present allowing us to handle TI between observations
 with non-shared dimensions.
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\varphi(\vec{x}:\ X) & =\prod_{\upsilon=1}^{d}\sum_{i=1}^{|X|}\frac{1}{|X|}\ K_{\gamma}(x^{\nu},x_{i}^{\nu},X^{\nu})\\
K_{\gamma}\left(x_{n}^{\nu},x_{m}^{\nu},X^{\nu}\right) & =\frac{1}{\gamma\sqrt{2\pi}}e^{-\frac{1}{\gamma}\|x_{n}^{\nu},x_{m}^{\nu},X^{\nu}\|}\\
a\\
\ddot{\omega}_{\alpha}(x_{i}^{\nu},x_{n}^{\nu},X) & =\frac{\omega_{\alpha}(x_{i}^{\nu},x_{n}^{\nu})}{\sum_{j=1}^{\ell}\omega_{\alpha}(x_{j}^{\nu},x_{n}^{\nu})}\end{align*}

\end_inset


\end_layout

\begin_layout Section
Support Vector Optimizations
\end_layout

\begin_layout Standard
The Parzen Window method is neither sparse nor computationally efficient,
 and as the number of observations grows, these deficiencies quickly become
 prohibitive.
 We now investigate the use of Support Vector Machines to generate estimates.
\end_layout

\begin_layout Subsection
Generalized Parzen-SVM 
\end_layout

\begin_layout Standard
Support Vector Machines are often used
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
ref?
\end_layout

\end_inset

 to estimate probability distributions by solving the related problem of
 estimating the cumulative distribution function of the random variable
 in question.
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
This isn't really the essence of SVM - put more verbage in the intro regarding
 what SVM's are, why they work, and why they're superior to other approaches
 (say, neural networks)
\end_layout

\end_inset

 This reduces the problem to one of estimating a non-linear mapping from
 observations to cumulative distribution values, which can be formulated
 as an optimization problem over a linear operator equation.
 Unfortunately, these methods depend on the ability to calculate an empirical
 distribution for each observation:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
F_{\ell}(x)=\frac{1}{\ell}\sum_{i}\theta(x-x_{i})\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\theta(x)$
\end_inset

 is the indicator function.
 To evaluate this function, the abstract space 
\begin_inset Formula $\Omega^{\mathcal{S}}$
\end_inset

 must be ordered.
 While we have described a distance metric over 
\begin_inset Formula $\Omega^{\mathcal{S}}$
\end_inset

, it is not clear what a meaningful ordering relation would be.
\end_layout

\begin_layout Standard
Rather than calculating the cumulative probability distribution of 
\begin_inset Formula $X$
\end_inset

, we begin with the assumption that the Parzen Window estimate of the probabilit
y distribution is acceptably accurate and attempt to minimize the difference
 between the Support Vector estimate and the Parzen Window estimate.
 In this spirit, we will use a modification of the Parzen Window estimator
 which substitutes a set of weights 
\begin_inset Formula $\beta$
\end_inset

 for the normalizing contant 
\begin_inset Formula $\frac{1}{|X|}$
\end_inset

 in the Parzen Window equation:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\varphi(\vec{x}:\ \beta,X) & =\prod_{\upsilon=1}^{d}\sum_{i=1}^{\ell}\beta_{i}K_{\gamma}(x^{\nu},x_{i}^{\nu},X^{\nu})\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The support vector approach requires that we define an optimization problem
 
\begin_inset Formula $W(\beta)$
\end_inset

 to determine the value of 
\begin_inset Formula $\beta$
\end_inset

.
 This optimization problem will determine which observations (or as we shall
 see, windows) will be used in estimations and which can be discarded as
 redundant or irrelevant information; the result of the optimization problem
 will be that a substantial number of multipliers 
\begin_inset Formula $\beta_{i}$
\end_inset

 will be 
\begin_inset Formula $0$
\end_inset

, allowing us to omit the windows defined by these points to be ignored
 in the prediction phase.
 We define the optimization problem as minimizing the square loss between
 the Support Vector and Parzen Window estimates over some set of observations
 
\begin_inset Formula $X$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
W(\beta:\ X) & \longmapsto\min_{\beta}\sum_{i=1}^{\ell}\left(\varphi(\vec{x}_{i}:\ X)-\varphi(\vec{x}_{i}:\ \beta,X)\right)^{2}+\beta\Omega(\lambda,X)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The optimization equation compares the results of Parzen estimation to the
 results of SVM estimation using a portion of the training data as synthetic
 testing data.
 For the optimization problem, we check the difference between the two estimates
 at windows defined by the observations 
\begin_inset Formula $X$
\end_inset

.
 The set of weights used in the Support Vector estimation must have a discrete
 number of elements; for simplicity we choose to assign a weight to each
 window defined by the time value of an observation in the training set
 and the constant parameter 
\begin_inset Formula $\alpha$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\sum_{i=1}^{\ell}\left(\varphi(x_{i}:\ X)-\varphi(x_{i}:\ \beta,X)\right)^{2} & =\sum_{i=1}^{\ell}\left(\varphi(x_{i}:\ X)\right)^{2}-2\left(\varphi(x_{i}:\ X)\varphi(x_{i}:\ \beta,X)\right)+\left(\varphi(x_{i}:\ \beta,X)\right)^{2}\\
 & =\sum_{i=1}^{\ell}\left(\left(\sum_{j=1}^{\ell}\frac{1}{\ell}K_{\gamma}(x_{i},x_{j},X)\right)^{2}-2\left(\sum_{j=1}^{\ell}\frac{1}{\ell}K_{\gamma}(x_{i},x_{j},X)\right)\left(\sum_{j=1}^{\ell}\beta_{j}K_{\gamma}(x_{i},x_{j},X)\right)+\left(\sum_{j=1}^{\ell}\beta_{j}K_{\gamma}(x_{i},x_{j},X)\right)^{2}\right)\\
 & =\sum_{i=1}^{\ell}\left(\sum_{\begin{array}{c}
j=1\\
k=1\end{array}}^{\ell}\frac{1}{\ell^{2}}K_{\gamma}(x_{i},x_{j},X)K_{\gamma}(x_{i},x_{k},X)-2\sum_{\begin{array}{c}
j=1\\
k=1\end{array}}^{\ell}\frac{1}{\ell}K_{\gamma}(x_{i},x_{j},X)\beta_{j}K_{\gamma}(x_{i},x_{k},X)+\sum_{\begin{array}{c}
j=1\\
k=1\end{array}}^{\ell}\beta_{j}\beta_{k}K_{\gamma}(x_{i},x_{j},X)K_{\gamma}(x_{i},x_{k},X)\right)\\
 & =\sum_{\begin{array}{c}
i=1\\
j=1\\
k=1\end{array}}^{\ell}\beta_{j}\beta_{k}K_{\gamma}(x_{i},x_{j},X)K_{\gamma}(x_{i},x_{k},X)-\sum_{\begin{array}{c}
i=1\\
j=1\\
k=1\end{array}}^{\ell}\frac{2\beta_{j}}{\ell}K_{\gamma}(x_{i},x_{j},X)K_{\gamma}(x_{i},x_{k},X)+\sum_{\begin{array}{c}
i=1\\
j=1\\
k=1\end{array}}^{\ell}\frac{1}{\ell^{2}}K_{\gamma}(x_{i},x_{j},X)K_{\gamma}(x_{i},x_{k},X)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Because this is an minimization problem we can eliminate the last term (changing
 
\begin_inset Formula $\beta$
\end_inset

 won't affect its value).
 Substituting our optimization problem becomes:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
W(\beta:\ X) & \longmapsto\min_{\beta}\sum_{\begin{array}{c}
i=1\\
j=1\\
k=1\end{array}}^{\ell}\beta_{j}\beta_{k}K_{\gamma}(x_{i},x_{j},X)K_{\gamma}(x_{i},x_{k},X)-\sum_{\begin{array}{c}
i=1\\
j=1\\
k=1\end{array}}^{\ell}\frac{2\beta_{j}}{\ell}K_{\gamma}(x_{i},x_{j},X)K_{\gamma}(x_{i},x_{k},X)+\lambda\Omega(\beta,X)\\
\text{subject to} & \beta_{i}\ge0,\ \sum\beta=1\end{align*}

\end_inset


\end_layout

\begin_layout Subsection
Quadratic Optimization Problem
\end_layout

\begin_layout Subsection
Support Vector Decomposition 
\end_layout

\begin_layout Standard
Due to the simplicity of the constraints in the optimization problem, it
 is possible to use the decomposition method of Osuna to reduce the memory
 requirements of the Parzen-SVM algorithm.
\end_layout

\begin_layout Subsubsection
 Sub-Problem Definition 
\end_layout

\begin_layout Standard
The decomposition algorithm breaks 
\begin_inset Formula $X$
\end_inset

 into two working sets 
\begin_inset Formula $B,N$
\end_inset

, and attempts to optimize 
\begin_inset Formula $B$
\end_inset

 while keeping 
\begin_inset Formula $N$
\end_inset

 fixed.
 This results in the following iterative optimization problem where 
\begin_inset Formula $\boldsymbol{\beta}^{k}$
\end_inset

 denotes the result of the previous iteration:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
W(\boldsymbol{\beta}_{B}) & =\frac{1}{2}\begin{bmatrix}\boldsymbol{\beta}_{B}^{T} & (\boldsymbol{\beta}_{N}^{k})^{T}\end{bmatrix}\begin{bmatrix}P_{BB} & P_{BN}\\
P_{NB} & P_{NN}\end{bmatrix}\begin{bmatrix}\boldsymbol{\beta}_{B}\\
\boldsymbol{\beta}_{N}^{k}\end{bmatrix}-\begin{bmatrix}q_{B}^{T} & q_{N}^{T}\end{bmatrix}\begin{bmatrix}\boldsymbol{\beta}_{B}\\
\boldsymbol{\beta}_{N}^{k}\end{bmatrix}\\
 & =\frac{1}{2}\boldsymbol{\beta}_{B}^{T}P_{BB}\boldsymbol{\beta}_{B}-(-q_{B}+P_{BN}\boldsymbol{\beta}_{N}^{k})^{T}\boldsymbol{\beta}_{B}\\
 & =\frac{1}{2}\begin{bmatrix}\beta_{i} & \beta_{j}\end{bmatrix}\begin{bmatrix}P_{ii} & P_{ij}\\
P_{ij} & P_{jj}\end{bmatrix}\begin{bmatrix}\beta_{i}\\
\beta_{j}\end{bmatrix}-(-q_{B}+P_{BN}\boldsymbol{\beta}_{N}^{k})^{T}\begin{bmatrix}\beta_{i}\\
\beta_{j}\end{bmatrix}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\text{subject to}\quad0\le\beta_{i},\beta_{j},\quad\beta_{i}+\beta_{j}=1-\mathbf{1}^{T}\boldsymbol{\beta}_{N}^{k}\end{equation}

\end_inset


\end_layout

\begin_layout Subsubsection
 Working Set Selection 
\end_layout

\begin_layout Standard
Select
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align}
 & i\in\text{arg}\max_{t}\left\{ -\nabla f(\boldsymbol{\beta}^{k})_{t}\ |\quad t\in I(\boldsymbol{\beta}^{k})\right\} \\
 & j\in\text{arg}\min_{t}\left\{ -\frac{b_{it}^{2}}{a_{it}}\ |\quad t\in I(\boldsymbol{\beta}^{k}),\quad-\nabla f(\boldsymbol{\beta}^{k})_{t}<-\nabla f(\boldsymbol{\beta}^{k})_{i}\right\} \end{align}

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% This needs to be checked for the new optimization scenario
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align}
I(\boldsymbol{\beta}) & \equiv\{t\ |\quad\beta_{t}<1\quad\text{or}\quad\beta_{t}>0\}\\
 & a_{it}=P_{ii}+P_{tt}-2P_{it}\\
 & \bar{a}_{it}=\begin{cases}
a_{it} & \text{if}\ a_{it}>0\\
\delta & \text{otherwise}\end{cases}\\
 & b_{it}=-\nabla f(\boldsymbol{\beta}^{k})_{i}+\nabla f(\boldsymbol{\beta}^{k})_{t}\\
\nabla f(\boldsymbol{\beta})_{i} & \equiv P_{i}\boldsymbol{\beta}-q_{i}\end{align}

\end_inset


\end_layout

\begin_layout Subsubsection
 Stopping Condition 
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\max_{i\in I(\boldsymbol{\alpha}^{k})}-\nabla f(\boldsymbol{\alpha})_{i}+\min_{j\in I(\boldsymbol{\alpha}^{k})}\nabla f(\boldsymbol{\alpha})_{j}\le\epsilon\end{equation}

\end_inset


\end_layout

\begin_layout Section
Transformation Invariance
\end_layout

\begin_layout Standard
Some datasets do not lend themselves to TI analysis, these datasets exhibit
 a single PDF with no similarity between the distribution at different locations
 along any axis.
 In order to understand when a TI analysis is useful we must develop a rigorous
 understanding of the problems TI solves and a method to quantify that problem
 for a given dataset.
\end_layout

\begin_layout Subsection
Contextual Information
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
The contextual information function needs to have two properties; the sum
 of the entropy of two sets must be smaller than the entropy of the two
 sets together, and the sum of the entropy of two dimensions of a dataset
 must be less than the entropy of the two dimensions together.
 The latter is a basic property of entropy, and it makes sense for it to
 hold true (it also informs the search algorithm).
 The former allows us to calculate the entropy of subsets of 
\begin_inset Formula $X$
\end_inset

, combine them with previous entropy calculations, and identify any dimension
 sets which are above the threshold.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Transformation invariance assumes that the context of an observation carries
 information relevant to the PDF of 
\begin_inset Formula $X$
\end_inset

 within that context.
 As such, we can say that the higher the expected information carried by
 a context, the more useful a TI analysis will be.
 This raises the question of how we determine the amount of information
 a given context contains 
\emph on
about the PDF within that context
\emph default
.
 In order for a context to contain information about itself, it must be
 similar to other contexts - we must have examples of similar behaviors
 from which we can generalize.
 So on the one hand, the more information we can extract from different
 contexts, the more likely we are to benefit from a TI analysis.
 On the other hand, if there are a relatively small number of divergent
 contexts, we can assume that the PDF of 
\begin_inset Formula $X$
\end_inset

 is not strongly contextualized, and therefore a TI analysis (based on contextua
l information) won't be particularly beneficial.
 These are the two factors which influence the relevance of a TI analysis;
 the variety of contexts and the overlap between contexts.
 In order to evaluate the relevance of TI analysis, we must develop orthogonal
 descriptions of these two factors.
 
\end_layout

\begin_layout Standard
The variety of contexts within a dataset can be described as the expected
 divergence between adjacent contexts:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
H_{\varepsilon}^{V}(X) & =\sum_{\begin{array}{c}
i=1\\
j=1\end{array}}^{\ell}\frac{1}{\ell^{2}}\omega_{\varepsilon}(x_{i},x_{j})\omega_{\alpha}(x_{i},x_{j})\|x_{i},x_{j},X\|\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The overlap between contexts can be described as the expected divergence
 between all possible contexts.
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
H_{\varepsilon}^{O}(X) & =\sum_{\begin{array}{c}
i=1\\
j=1\end{array}}^{\ell}\frac{1}{\ell^{2}}\omega_{\alpha}(x_{i},x_{j})\|x_{i},x_{j},X\|\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We would like the TI value to increase as either of these factors increase.
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
H_{\varepsilon}(X) & =H_{\varepsilon}^{V}(X)+H_{\varepsilon}^{O}(X)\\
 & =\sum_{\begin{array}{c}
i=1\\
j=1\end{array}}^{\ell}\frac{1}{\ell^{2}}\omega_{\varepsilon}(x_{i},x_{j})\omega_{\alpha}(x_{i},x_{j})\|x_{i},x_{j},X\|+\frac{1}{\ell^{2}}\omega_{\alpha}(x_{i},x_{j})\|x_{i},x_{j},X\|\\
 & =\sum_{\begin{array}{c}
i=1\\
j=1\end{array}}^{\ell}\frac{1}{\ell^{2}}\omega_{\alpha}(x_{i},x_{j})\|x_{i},x_{j},X\|\left(\omega_{\varepsilon}(x_{i},x_{j})+1\right)\end{align*}

\end_inset


\end_layout

\begin_layout Subsection
Multi-dimensional Contextual Information
\end_layout

\begin_layout Standard
This can be extended to multi-dimensional data by observing that the window
 used previously is defined along a single dimension; we can just as easily
 consider a multi-dimensional window.
 In this case, we simply need to compare the expected divergence between
 adjacent windows in multiple dimensions and the expected divergence between
 all possible windows in multiple dimensions (here we use the 
\begin_inset Formula $L^{2}$
\end_inset

 metric):
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
H_{\varepsilon}^{V}(X) & =\sum_{\begin{array}{c}
i=1\\
j=1\end{array}}^{\ell}\sqrt{\sum_{\nu=1}^{d}\left(\frac{1}{\ell^{2}}\omega_{\varepsilon}(x_{i}^{\nu},x_{j}^{\nu})\omega_{\alpha}(x_{i}^{\nu},x_{j}^{\nu})\|x_{i}^{\nu},x_{j}^{\nu},X\|\right)^{2}}\\
H_{\varepsilon}^{O}(X) & =\sum_{\begin{array}{c}
i=1\\
j=1\end{array}}^{\ell}\sqrt{\sum_{\nu=1}^{d}\left(\frac{1}{\ell^{2}}\omega_{\alpha}(x_{i}^{\nu},x_{j}^{\nu})\|x_{i}^{\nu},x_{j}^{\nu},X\|\right)^{2}}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We can refine these to apply only to a subset of 
\begin_inset Formula $d$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
H_{\varepsilon}^{V}(X:\ D) & =\sum_{\begin{array}{c}
i=1\\
j=1\end{array}}^{\ell}\sqrt{\sum_{\nu\in D}\left(\frac{1}{\ell^{2}}\omega_{\varepsilon}(x_{i}^{\nu},x_{j}^{\nu})\omega_{\alpha}(x_{i}^{\nu},x_{j}^{\nu})\|x_{i}^{\nu},x_{j}^{\nu},X,D\|\right)^{2}}\\
H_{\varepsilon}^{O}(X:\ D) & =\sum_{\begin{array}{c}
i=1\\
j=1\end{array}}^{\ell}\sqrt{\sum_{\nu\in D}\left(\frac{1}{\ell^{2}}\omega_{\alpha}(x_{i}^{\nu},x_{j}^{\nu})\|x_{i}^{\nu},x_{j}^{\nu},X,D\|\right)^{2}}\\
D & =[d_{n},...,d_{m}],\ 1\le d_{i}<d\end{align*}

\end_inset


\end_layout

\begin_layout Subsection
Selecting TI Dimensions
\end_layout

\begin_layout Standard
We now turn our attention to the task of selecting which dimensions and
 which combinations of dimensions warrant a TI analysis.
 We will search for sets of dimensions whose contextual information is greater
 than some constant 
\begin_inset Formula $h$
\end_inset

.
 We know that the cardinality of the set of all possible subsets of 
\begin_inset Formula $D$
\end_inset

 is 
\begin_inset Formula $2^{|D|}$
\end_inset

, which quickly becomes intractable as the dimensionality of 
\begin_inset Formula $\Omega$
\end_inset

 increases.
 We need to develop some heuristics for reducing the search space when investiga
ting contextual information.
\end_layout

\begin_layout Standard
Let us begin by considering information entropy in the abstract.
 Given a set of 2D vector observations 
\begin_inset Formula $X$
\end_inset

, we can state that the entropy of the vector set 
\begin_inset Formula $H(X)$
\end_inset

 is always great than or equal to the sum of the entropy of either dimension
 
\begin_inset Formula $H(X^{i})$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
H(X) & \ge H(X^{1})+H(X^{2})\end{align*}

\end_inset


\end_layout

\begin_layout Standard
This means that in our search for dimension sets which warrant TI analysis,
 for any set of dimensions 
\begin_inset Formula $D$
\end_inset

, if the set 
\begin_inset Formula $X^{D}$
\end_inset

 does not warrant TI analysis, than no subset 
\begin_inset Formula $D'\subset D$
\end_inset

 warrants analysis either:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
H_{\varepsilon}(X,D)<h & \longmapsto H_{\varepsilon}(X,D')\ \forall D'\subset D\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Our search algorithm should therefore start with high-dimensional partitions
 of 
\begin_inset Formula $D$
\end_inset

 and then investigate lower dimensional partitions - this allows us to ignore
 any subsets of a partition whose contextual information is less than the
 constant 
\begin_inset Formula $h$
\end_inset

.
 
\end_layout

\begin_layout Standard
A second consideration relates to the number of observations used in the
 calculation of 
\begin_inset Formula $H(X)$
\end_inset

.
 Let us consider two disjoint subsets 
\begin_inset Formula $Y$
\end_inset

 and 
\begin_inset Formula $Z$
\end_inset

 of our full set of observations 
\begin_inset Formula $X$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
Y & \subset X\\
Z & \subset X\\
Y\cap Z & =\emptyset\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Reviewing the equation for 
\begin_inset Formula $H(X)$
\end_inset

, it is clear that in the absence of the normalizer 
\begin_inset Formula $\frac{1}{\ell^{2}}$
\end_inset

, the contextual sum of the contextual information for out two subsets would
 necessarily be less than both the contextual entropy of the union of these
 sets and the contextual information of 
\begin_inset Formula $X$
\end_inset

.
 We therefore establish a function which determines the minimum entropy
 of two such subsets:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
H_{\min}(Y,Z:\ D) & =\frac{|Y|^{2}H_{\varepsilon}(Y:\ D)+|Z|^{2}H_{\varepsilon}(Z:\ D)}{\left(|Y|+|Z|\right)^{2}}\\
H_{\min}(Y,Z:\ D) & \le H_{\varepsilon}(Y\cup Z:\ D)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Note that using this equation means that we can't really eliminate search
 branches unless we've analyzed all their data points.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
This equation allows us to restrict entropy calculations to subsets of the
 data; if 
\begin_inset Formula $H_{\min}(Y,Z:\ D)>h$
\end_inset

, we know that 
\begin_inset Formula $H_{\varepsilon}(X:\ D)>h$
\end_inset

.
 Taken together these two observations allow us to reduce the computational
 demands of the search algorithm by reducing the number of observations
 used in a given contextual information calculation and to discard partitions
 of dimensional sets whose entropy is below our threshold 
\begin_inset Formula $h$
\end_inset

.
 This leaves us with the task of developing search heuristics which optimize
 the computational demands of generating contextual information for dimensional
 sets and the system's accuracy.
 We use the contextual information measure 
\begin_inset Formula $H(X)$
\end_inset

 as the measure of the expected increase in prediction accuracy; the more
 information contained in a dimensional set the more we stand to gain from
 analyzing it.
\end_layout

\begin_layout Standard
We now develop a cost function which quantifies the computational cost of
 evaluating the contextual information of a given dimensional set.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Cost function?
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Algorithm needs to choose between partitioning a dimensional set and evaluating
 more data points for a data set.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
This means that our search will be iterative; we will select a partition
 
\begin_inset Formula $D'$
\end_inset

 of 
\begin_inset Formula $D$
\end_inset

 and determine the contextual information for each partition 
\begin_inset Formula $D''$
\end_inset

 with cardinality 
\begin_inset Formula $|D'|-1$
\end_inset

.
 We have three basic options for selecting which set 
\begin_inset Formula $D'$
\end_inset

 will be partitioned; we can execute a depth-first search, a breadth-first
 search, or a maximal 
\begin_inset Formula $H_{\varepsilon}$
\end_inset

 search.
 A breadth-first search will eliminate subsets faster, however if our objective
 is to find the best set of low-dimensional partitions for which to apply
 TI analysis this approach will likely take longer, as it must traverse
 each potential high-dimensional partition first.
 A depth-first search will produce low-dimensional partitions fastest, and
 if we choose the partition 
\begin_inset Formula $D''$
\end_inset

 with the highest contextual information at each step, we are likely to
 quickly find low-dimensional partitions with high contextual information
 relative to other partitions along their local search tree, but we will
 have no guarantees that these partitions have high contextual information
 relative to all partitions of the same cardinality.
 The performance of the third approach will be highly dependant on the nature
 of the data being analyzed; in cases where all partitions of 
\begin_inset Formula $D$
\end_inset

 have similar contextual information it will traverse the search tree in
 a basically breadth-first manner, in cases where a few dimensions of 
\begin_inset Formula $D$
\end_inset

 contain the majority of the contextual information it will traverse the
 search tree in a basically depth-first manner.
\end_layout

\begin_layout Section
Ensemble System
\end_layout

\begin_layout Standard
There are limits to the predictive ability of the current architecture.
 Most significantly, the context which influences a prediction is limited
 by the nature of 
\begin_inset Formula $\omega(\cdot,\cdot)$
\end_inset

.
 A more robust architecture would allow the context of a prediction to be
 more flexible, for example responding to two discrete windows while ignoring
 data outside those windows.
 A simple example of this is delayed causality; recognizing the relationship
 between an action and a delayed result requires that the system be capable
 of ignoring the intervening data.
 One solution to this limitation is to include multiple windows in the context
 of an estimate.
 In this case we would compare data drawn from multiple windows to data
 in a similar set of windows in 
\begin_inset Formula $X$
\end_inset

.
 
\end_layout

\begin_layout Standard
Another perspective in this limitation is that the estimates we generate
 (as well as the contexts from which we make the estimates) is limited by
 the nature of 
\begin_inset Formula $\omega(\cdot,\cdot)$
\end_inset

; it would be helpful for our estimates to make predictions about the probabilit
y of values in multiple windows of the abstract space 
\begin_inset Formula $\Omega^{X}$
\end_inset

.
 Taken together, we can see that a superior architecture would be capable
 of making estimates in the form of multiple windows using on observations
 
\emph on
based
\emph default
 on multiple windows.
 This can be accomplished by treating windows over 
\begin_inset Formula $X$
\end_inset

 as independant observations of the random variable 
\begin_inset Formula $Y$
\end_inset

.
 For any such window defined by a point 
\begin_inset Formula $D_{n}$
\end_inset

, we define the value of the point 
\begin_inset Formula $y_{m}^{n}$
\end_inset

 in the abstract space 
\begin_inset Formula $\Omega^{Y}$
\end_inset

as the divergence between 
\begin_inset Formula $X$
\end_inset

 in the window defined by 
\begin_inset Formula $D_{n}$
\end_inset

and the window defined by 
\begin_inset Formula $D_{m}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
y_{m}^{n} & =g(x_{m}:\ X)\\
g(x_{m}:\ X) & =\|x_{n},x_{m},X,D\|\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The dimensionality of 
\begin_inset Formula $\Omega^{Y}$
\end_inset

 is determined by the number of windows over 
\begin_inset Formula $X$
\end_inset

 which are used as observations of 
\begin_inset Formula $Y$
\end_inset

.
 We can define the value of each dimension of 
\begin_inset Formula $Y$
\end_inset

 at any point 
\begin_inset Formula $D_{i}$
\end_inset

 by determining the divergence between that dimension's corresponding window
 over 
\begin_inset Formula $X$
\end_inset

 and the window over 
\begin_inset Formula $X$
\end_inset

 defined by 
\begin_inset Formula $D_{i}$
\end_inset

.
 Because observations of 
\begin_inset Formula $Y$
\end_inset

 are derived from 
\begin_inset Formula $X$
\end_inset

, we must establish some method of choosing values of 
\begin_inset Formula $D$
\end_inset

 for which to derive observations; the nature of this method will depend
 on the nature of the data being analyzed and the computational resources
 available.
 We leave this for future investigation.
\end_layout

\begin_layout Standard
The generation of estimates of 
\begin_inset Formula $\mathcal{P}^{Y}$
\end_inset

 in a given window 
\begin_inset Formula $D$
\end_inset

 can be accomplished using the techniques already set forth, and in this
 sense the revised architecture is recursive.
 The estimator for each abstract space is referred to as a 'layer'; depending
 on the way estimates are used, the system can be seein as either a hierarchical
 system or a feed-forward system.
 We define a layer as such:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\varphi^{i+1}(g(x:\ X^{i}):\ X^{i+1}) & \longmapsto\Pr(g(x:\ X^{i})|X^{i+1})\pm\xi\\
X^{i+1} & =[g(x_{1}:\ X^{i}),...,g(x_{\ell^{i+1}}:\ X^{i})]\\
g(x:\ X^{i}) & =\left[\|x_{1},x,X^{i},D\|,...,\|x_{d^{i+1}},x,X^{i},D\|\right]\\
g(x:\ X^{i+1})^{j+1} & =g(g(x:\ X^{i})^{j}:\ X^{i+1})\\
g(x:\ X^{i})^{0} & =\|x_{n},x_{m},X,D\|\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We must therefore develop a method of translating estimates in the abstract
 space 
\begin_inset Formula $\Omega^{Y}$
\end_inset

 into estimates in the abstract space 
\begin_inset Formula $\Omega^{X}$
\end_inset

.
 One method is to use a feed-forward system, transforming a prediction 
\begin_inset Formula $y_{i}$
\end_inset

 into a PDF in 
\begin_inset Formula $\Omega^{X}$
\end_inset

.
 Because each dimension of 
\begin_inset Formula $y_{i}$
\end_inset

 is defined as a PDF in 
\begin_inset Formula $\Omega^{X}$
\end_inset

 and combining these PDF's is a trivial task.
 There are two weakness in this approach; it requries that the all the informati
on in a window 
\begin_inset Formula $D_{n}$
\end_inset

 over 
\begin_inset Formula $X$
\end_inset

 be used in generating values of 
\begin_inset Formula $y^{n}$
\end_inset

 and our predictions are limited by the sampling rate of 
\begin_inset Formula $Y$
\end_inset

.
 A more robust approach would allow the full use of the information contained
 in 
\begin_inset Formula $X$
\end_inset

 regardless of the amount of information transferred from 
\begin_inset Formula $X$
\end_inset

 to 
\begin_inset Formula $Y$
\end_inset

 (determined by the sampling rate and method of generating observations).
 A better approach is to use a hierarchical system in which we use some
 combination of predictions drawn from each layer of the architecture:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\varphi(x:\ X^{1},...,X^{\mu}) & =f\left(\varphi^{1}(x:\ X^{1}),...,\varphi^{\mu}(g(x:\ X^{\mu-1})^{\mu-1}:\ X^{\mu}):\ \lambda\right)\\
\int_{-\infty}^{\infty}f(x)dx & =1\\
0\le f(x)\le1 & \forall x\end{align*}

\end_inset


\end_layout

\begin_layout Standard
One way of doing this would be to simply take the product of each estimate
 weighted by some parameter to control the influence of different layers:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\varphi(x:\ X^{1},...,X^{\mu}) & =\prod_{n=1}^{\mu}\lambda^{n}\varphi^{n}(x:\ X^{n})\end{align*}

\end_inset


\end_layout

\begin_layout Standard
This system is not strictly hierarchical; we can determine the influence
 of each layer individually (potentially favoring lower layers over higher
 ones), however we refer to it as such because successive layers have an
 increased scope of data from which to make predictions, and each of these
 is eventually combined in making estimates.
 
\end_layout

\begin_layout Subsection
Example System Architecture
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Control the dimensionality of this by restricting scope to adjacent windows?
\end_layout

\begin_layout Plain Layout
if we do SV points, those points will need to have D as dimensions - they'll
 need to know the context on which a given SV pattern occurred.
\end_layout

\begin_layout Plain Layout
Treat the SV points as sparse datasets using a divergence threshold.
\end_layout

\begin_layout Plain Layout
Here's what I'm thinking.
 We start from the perspective of a single TI dimension.
 We generate estimates in the dimension alone and isolate some SV's.
 We then begin to compare that dimension to others to see if shared TI is
 worthwhile.
 Using the other dimensions which match, we construct a new estimator using
 data points based on the shared dimensions and sparse SV's from the two
 dimensions.
 We can then build up to a third layer by comparing the SV's from the second
 layer to SV's in other dimensions, again generating a set of shared TI
 dimensions.
\end_layout

\begin_layout Plain Layout
This needs refinement - we may run into the ambiguity of the channels again.
\end_layout

\end_inset


\end_layout

\begin_layout Section
Motivated Decision-Making
\end_layout

\begin_layout Standard
Up to this point, our discussion has focused on data analysis.
 We now turn our attention to volitive systems; systems which have the ability
 influence the the probability distribution 
\begin_inset Formula $\mathcal{P}$
\end_inset

 of the observation space 
\begin_inset Formula $\Omega$
\end_inset

, and in which certain types of local probability distributions are preferred
 over others.
 
\end_layout

\begin_layout Subsection
Setting of the Motivation Problem
\end_layout

\begin_layout Standard
We assume that such a system influences 
\begin_inset Formula $\mathcal{P}$
\end_inset

 through actions which are discrete and quantifiable.
 Each action is therefore described as a vector 
\begin_inset Formula $\vec{a}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\vec{a}_{i} & =\left[a_{i}^{1},...,a_{i}^{\pi}\right]\end{align*}

\end_inset


\end_layout

\begin_layout Standard
In order to correlate actions with changes in 
\begin_inset Formula $\mathcal{P}$
\end_inset

, we treat each action as an observation of 
\begin_inset Formula $X$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
X & =(\Omega,\mathcal{F},\mathcal{P})\\
\Omega & \in\mathbb{R}^{d+\pi}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
In this case, our observations are of dimension 
\begin_inset Formula $d+\pi$
\end_inset

, the first 
\begin_inset Formula $d$
\end_inset

 dimensions describe the system's 'environment' and the last 
\begin_inset Formula $\pi$
\end_inset

 describe the system's 'actions'.
 In order to choose between possible actions, the system must prefer certain
 states of the input space 
\begin_inset Formula $\Omega$
\end_inset

 over others.
 We describe these preferred states as the system's using a function 
\begin_inset Formula $m_{\theta}(x:\ X)$
\end_inset

 we'll refer to as the motivator with control parameter 
\begin_inset Formula $\theta$
\end_inset

.
 The for a given dimension of 
\begin_inset Formula $\Omega$
\end_inset

, the motivator function returns a positive real value describing the system's
 preference for the state 
\begin_inset Formula $x^{i}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
m_{\theta}(x^{i}:\ X) & \in\mathbb{R}^{+}\\
m_{\theta}(x^{i}:\ X)>1 & \longmapsto\text{preferred state}\\
m_{\theta}(x^{i}:\ X)<1 & \longmapsto\text{discouraged state}\\
m_{\theta}(x^{i}:\ X)=1 & \longmapsto\text{no preference}\end{align*}

\end_inset


\end_layout

\begin_layout Subsection
Estimating Outcomes in the Incentive Space
\end_layout

\begin_layout Standard
Motivated decision making therefore is a two-stage process; the system must
 first create an estimate of the relationship between actions and observations
 of 
\begin_inset Formula $X$
\end_inset

 in the context of those actions and then estimate action values which maximize
 the probability of preferred states given the action's context.
 The first stage can be accomplished using the TI analysis already developed.
 The second stage requries that we apply the motivator to the estimation
 task in a specific context.
 In this case our estimates are no longer probability estimates; instead
 they describe the probability that a given action will correspond to preferred
 states in the context of the action.
 We refer to these estimates as as pints in the incentive space 
\begin_inset Formula $I$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\bar{\varphi}(x:\ X) & \longmapsto\Pr(x|I)\\
\bar{\varphi}(x:\ X) & =\prod_{\upsilon=1}^{d}\sum_{i=1}^{|X|}m_{\theta}(x_{i}^{\nu}:\ X)\beta_{i}\ \bar{K}_{\gamma}(x^{\nu},x_{i}^{\nu},X^{\nu})\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Still not sure if the first one makes any sense
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In this sense, the motivator function behaves like a weighting mechanism,
 increasing the incentive value of high-probability actions which correspond
 to preferred states and decreasing the incentive value of high-probability
 actions which correspond with discouraged states.
 Estimating values in the incentive space allows us to select actions whose
 influence is well understood to produce the desired states of 
\begin_inset Formula $X$
\end_inset

 in a given context.
 Choosing actions is reduced to determining the supremum of the action dimension
s of the incentive space in a given context.
\end_layout

\begin_layout Subsection
Incentivized Path Search
\end_layout

\begin_layout Standard
Because estimates can now influence the underlying data, generating estimates
 is no longer a simple matter of perception; the choice of a specific action
 will influence the probability space, making the next estimation task different
 from the last.
 Furthermore, in the context of complex actions made up of multiple vectors,
 the choice of a single action may modify the incentive space independant
 of any influence from the motivator function.
 Maximizing the expected incentive value in a deterministic and computationally
 feasible manner, in the context of these facts, would be of great utility
 and is left as an exercise for the reader.
\end_layout

\begin_layout Standard
We will instead focus on iterative approaches.
 We can create a set of actions 
\begin_inset Formula $A$
\end_inset

 through an iterative process.
 We first define a selector function 
\begin_inset Formula $s(X:\ D)$
\end_inset

 which selects a value of 
\begin_inset Formula $a_{i}$
\end_inset

 from the incentive space defined by 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $D$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
s(X:\ D) & \longmapsto a_{i}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
At each step 
\begin_inset Formula $i+1$
\end_inset

 in the iteration we select a context 
\begin_inset Formula $D$
\end_inset

 and select an action 
\begin_inset Formula $a_{i}=s(X_{i}:\ D)$
\end_inset

 from action dimensions for 
\begin_inset Formula $X_{i}$
\end_inset

 in the given context.
 We then append 
\begin_inset Formula $a_{i}$
\end_inset

 to 
\begin_inset Formula $X_{i}$
\end_inset

 and repeat the process, this time with the set 
\begin_inset Formula $X_{i+1}=X_{i}\cup a_{i}$
\end_inset

.
 
\end_layout

\begin_layout Standard
If 
\begin_inset Formula $s(X:\ D)$
\end_inset

 simply selects the supremum of the action dimensions of 
\begin_inset Formula $X$
\end_inset

, the system is susceptible to local minima.
 Various techniques exist to address this situation; we will simply assume
 that 
\begin_inset Formula $s(X:\ D)$
\end_inset

 is indeterminant and involves some element of randomness.
 This means that at each iteration, multiple values of 
\begin_inset Formula $a_{i}$
\end_inset

are possible.
 We can therefore think of our iterative approach as a tree search, with
 multiple search paths originating from each iteration.
 This leaves us with not simply a set of action paths 
\begin_inset Formula $A$
\end_inset

, but a set of paths, each one tracing a different route from trunk to tip,
 and each one producing different estimates of the expected incentives in
 different contexts.
 This presents yet another problem of choice; which branch of actions should
 actually be taken?
\end_layout

\begin_layout Standard
To address this problem, we define an ethics function 
\begin_inset Formula $e(A_{i})$
\end_inset

.
 The ethics function establishes different weights for different incentive
 values in different contexts:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
e(A_{i}) & \in\mathbb{R}^{+}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We refer to this function as 'ethical' because it makes trade-offs between
 the probability of satisfying the systems motivation and the context to
 which that probability refers; is it better to buy a new car today or save
 my money to buy a house in a year? 
\end_layout

\begin_layout Standard
We can now generate a set of actions which is likely to produce known results
 which will satisfy the system's motivational parameters by selecting the
 supremum of the results of our path search:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
A & =\sup_{A}e(A_{1})\end{align*}

\end_inset


\end_layout

\begin_layout Subsection
Curiosity
\end_layout

\begin_layout Standard
Without the ability to take action, a system's accuracy and efficiency for
 a given dataset is limited by the algorithm it uses and the computational
 resources available to it.
 Systems with the ability to influence the observations from which they
 make estimates, on the other hand, can take advantage of this ability to
 increase both their accuracy and performance.
 Before we can discuss the mechanisims by which this is possible, we must
 first develop a more rigorous understanding of these two objectives; accuracy
 and efficiency.
\end_layout

\begin_layout Standard
Any probabilistic system's estimation accuracy is limited by the amount
 of uncertainty inherent in the data it processes and the nature of the
 algorithm used to generate estimates.
 The rate of convergence in such a system's estimations and the 'true' probabili
ty distribution has an upper bound determined by its VC dimension; within
 these bounds a system's estimation accuracy improves as the number of observati
ons increases.
 In the context of TI data, it is possible to think of each neighborhood
 in a TI dimension as an independant estimation problem, thus an observation
 of 
\begin_inset Formula $X$
\end_inset

 will not increase the system's accuracy in a given neighborhood if the
 observation 
\begin_inset Formula $x$
\end_inset

 is not in that neighborhood.
 The task of improving a volitive system's accuracy in a given context therefore
 can be framed as the task of obtaining observations of that context.
 
\end_layout

\begin_layout Standard
The efficiency of the system we've developed is primarily controlled by
 the number of Support Vectors.
 For a given dataset and predictive accuracy, as the number of SV's decreases,
 the efficiency of the system increases.
 The efficiency of non-volitive systems is limited by the results of the
 optimization problem for 
\begin_inset Formula $\beta$
\end_inset

.
 In the context of a volitive system, decreasing the number of SV's can
 be accomplished by obtaining observations higher-order structure.
 Higher-order structure refers to the ability of a multi-layered system
 to break multiple high-entropy observations down into shared combinations
 of lower-entropy observations in such a way that the entropy of combined
 observations is lower than the entropy of the original observations.
 The result of such a process is the ability to eliminate portions of the
 original dataset as redundant, reducing the number of required SV's to
 match the accuracy of the system prior to observing the higher-order structure.
\end_layout

\begin_layout Standard
Working within the context of the existing architecture, our task is to
 develop a metric which can be used as a motivational parameter to drive
 these two behaviors.
 If we assume that the sampling rate of the system is independant of the
 action choices which are made, both behaviors can be reduced to minimizing
 the ratio of observations to support vectors.
 Increasing the system's accuracy can be viewed as determining a suitable
 support vector (or vectors) to represent behavior of 
\begin_inset Formula $X$
\end_inset

 in a given context (allowing the redundant observations to be discarded)
 and promoting the discovery of higher-order structure inherently increases
 this ratio by the same mechanism.
 We can therefore create a curious system by establishing the performance
 metric 
\begin_inset Formula $p(\beta,X)$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
p_{|X|}(\beta,X) & =\frac{|X|}{|\beta|-|\beta_{0}|}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\beta_{0}=[\beta_{i}|\ \beta_{i}=0]$
\end_inset

.
 Motivating the system towards curiosity simply entails establishing a motivator
 which prefers positive values of the second-derivative of 
\begin_inset Formula $p(\beta,X)$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
m(x_{i}^{p}:\ X) & =e^{\lambda\frac{d^{2}}{di^{2}}p_{i}(\beta,X)}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Using the second derivative motivates the system to prefer increases in
 the average change in 
\begin_inset Formula $p(\beta,X)$
\end_inset

.
 The value of 
\begin_inset Formula $p(\beta,X)$
\end_inset

 is likely non-stationary, so establishing a motivational parameter for
 set values isn't helpful.
 The first-derivative of 
\begin_inset Formula $p(\beta,X)$
\end_inset

 establishes the expected change in the system's comprehension, however
 this rate will vary with different datasets and would need to be determined
 for each dataset.
 The second-derivative however describes the rate at which the system's
 comprehension is increasing or decreasing, and is more likely to be tied
 to the system's actions than to the underlying data.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
This needs some housekeeping - explain values of p at each observation and
 work the work 'curiosity' and 'comprehension' into the earlier paragraphs
\end_layout

\end_inset


\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Subsection
Eunite Competition Data
\end_layout

\begin_layout Subsection
Santa Fe Data
\end_layout

\begin_layout Subsection
CATS Benchmark Data
\end_layout

\begin_layout Subsection
Results Summary
\end_layout

\begin_layout Section
Further Research
\end_layout

\begin_layout Subsection
Data Pre-Processing
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% logistic function using mean and sd to put most training points between
 .1 and .9
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% upper and lower bounds on 
\backslash
(
\backslash
int 
\backslash
Delta 
\backslash
) as non-stationary data detections mechanism
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% logistic function from delta using mean and sd in same way if data non-station
ary
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% iterative integration process until stationary data found ?
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Performance
\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Standard
Eat it, bitches
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
nocite{Moreno03}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
bibliographystyle{plain}
\end_layout

\end_inset

 
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "Research/research.bib"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
