#LyX 1.6.2 created this file. For more info see http://www.lyx.org/
\lyxformat 345
\begin_document
\begin_header
\textclass article
\begin_preamble



\usepackage{amsfonts}
\end_preamble
\use_default_options false
\language english
\inputencoding auto
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\paperfontsize 10
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 0
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\author "" 
\author "" 
\end_header

\begin_body

\begin_layout Title
Parzen-SVM: An Approach to Timeseries Analysis Using Support Vector Machines
 based on Parzen Windows
\end_layout

\begin_layout Author
Ryan Michael
\begin_inset Newline newline
\end_inset

 
\family typewriter
kerinin@gmail.com
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Subsection
Existing Work
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Expectation-maximization_algorithm
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Stationary_process
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Ergodicity
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Mixing_(mathematics)
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Lyapunov_exponent
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Recurrence_quantification_analysis
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Recurrence_plot
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Autoregressive_fractionally_integrated_moving_averag
e
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Autocorrelation
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
url{http://en.wikipedia.org/wiki/Linear_discriminant_analysis}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Hidden Markov Model
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% cannot account for future states - only capable of prediction
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% weak, short-term memory
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Box-Jenkins
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% intended for simplistic processes with well-understood stationarity and
 periodicity
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% http://en.wikipedia.org/wiki/Box-Jenkins
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Spectral Analysis
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% assumes some type of frequency-domain decomposition.
  Frequency-domain signal representations do not do a very good job predicting
 time-domain values.
 
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Shrinking-
\begin_inset Formula $\epsilon$
\end_inset

 SVM Regression
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% theoretical foundation weak; only compensates for the relevance of recent
 data.
  See Markhov problem
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
General Overview
\end_layout

\begin_layout Standard
The goal is to create a method of statistical inference capable of processing
 timeseries data which is both multi-variate and exhibits different behaviors
 at different times.
 This type of data is common, and developing a robust method of analysis
 has applications in many domains.
 The general approach is to create a series of estimates using subsets of
 the observed data, and to then combine these estimates in an intelligent
 manner which captures the relevance of each estimate to the current prediction
 task.
 By using a set of 'typical' estimates, we are able to reduce the computational
 demands of the system, as each estimate is a condensed representation of
 the data from which it was derived.
 This approach also allows us to reduce data redundancy by only using distinct
 estimates.
\end_layout

\begin_layout Standard
The most basic operation used in this system is the estimation of probability
 densities.
 Based on a set of observations drawn from some random process, we generate
 an estimate of the underlying probability distribution.
 This estimate tells us the probability of each point in the input space
 being observed.
 Areas of the input space in which a dense set of observations are observed
 are given high probability, while areas of the input space with few observation
s are given low probability.
\end_layout

\begin_layout Standard
We assume that observations are pulled from multiple independent sources,
 all of which respond to some underlying phenomena.
 For instance one set of observations could be from a microphone and another
 from a light detector.
 We do not know how the inputs are related or to what extent their behavior
 changes over time.
 For example one input could be a steady sine wave and another could be
 based on the decay of a radioactive isotope.
 In the former case previous observations of the source are useful, in the
 latter they're not.
 Alternately two inputs could be light detectors in the same room or they
 could be on different continents; in the former their input would be highly
 similar, in the latter not as much.
\end_layout

\begin_layout Section
Problem Setting
\end_layout

\begin_layout Standard
We begin with a hidden random variable
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\mathbf{X}=(\boldsymbol{\Omega},\mathcal{\boldsymbol{F}},\mathcal{\boldsymbol{P}})\]

\end_inset


\end_layout

\begin_layout Standard
Our knowledge of 
\begin_inset Formula $\mathbf{X}$
\end_inset

 comes from a set of independant sources 
\begin_inset Formula $X^{n}$
\end_inset

 which we treat as random processes generated by 
\begin_inset Formula $\mathbf{X}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \[
X^{n}:\Omega\mapsto\Omega^{n}\in\mathbb{R}^{d}\times\mathbb{R}_{+}\]

\end_inset

 
\begin_inset Formula \[
X^{n}=(\Omega^{n},\mathcal{F}^{n},\mathcal{P}^{n})\]

\end_inset

 
\begin_inset Formula \[
\mathbf{X}=[X^{0},...,X^{c}]\]

\end_inset


\end_layout

\begin_layout Standard
We refer to each of these sources as a channel, and refer to each channel
 as the 
\begin_inset Formula $i^{\text{th}}$
\end_inset

 element of the set 
\begin_inset Formula $\mathbf{X}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \[
X^{n}=[\Omega^{n},\mathcal{F}^{n},\mathcal{P}^{n}]\]

\end_inset


\end_layout

\begin_layout Standard
For each channel we are given a set of 
\begin_inset Formula $\ell$
\end_inset

 observations of dimension 
\begin_inset Formula $d$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\mathbf{x}_{i}^{n}=(x_{i}^{n},t_{i}^{n})\]

\end_inset

 
\begin_inset Formula \[
X^{n}=\left[\mathbf{x}_{0}^{n},\ldots,\mathbf{x}_{\ell}^{n}\right],\ \mathbf{x}_{m}^{n}\in\mathbb{R}^{d}\times\mathbb{R}_{+}\]

\end_inset


\end_layout

\begin_layout Standard
We define a set of time values and durations:
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\mathcal{T}=[t_{0},\ldots,t_{\ell}]\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \[
\Theta=[\theta_{0},\ldots,\theta_{z}]\]

\end_inset


\end_layout

\begin_layout Standard
We assume that the probability measure 
\begin_inset Formula $\mathcal{P}^{n}$
\end_inset

 of 
\begin_inset Formula $X^{n}$
\end_inset

can be approximated in a given time window using a shifted and weighted
 set of distributions defined over some set of time intervals.
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\mathcal{P}(t,\theta)=\sum_{i}\delta_{i}\cdot f_{i}(t-t_{i})\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\delta_{n}$
\end_inset

 is the weight corresponding to 
\begin_inset Formula $f_{n}$
\end_inset

 and 
\begin_inset Formula $t_{i}$
\end_inset

 is the magnitude of the time shift.
 Finally, we assume that a similar mixture of distributions can be determined
 for each channel:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\mathcal{P}^{n}(t,\theta)=\sum_{i}\delta_{i}^{n}\cdot f_{i}^{n}(t-t_{i})\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
Single Channel Setting
\end_layout

\begin_layout Standard
We begin by considering the case where only one channel exists, so for now
 we will omit the superscript and refer to 
\begin_inset Formula $X^{n}=(\Omega^{n},\mathcal{F}^{n},\mathcal{P}^{n})$
\end_inset

 as 
\begin_inset Formula $X=(\Omega,\mathcal{F},\mathcal{P})$
\end_inset

.
 Given a set of training data 
\begin_inset Formula $Y$
\end_inset

, our task is to estimate a probability distribution based on a set of test
 data 
\begin_inset Formula $Z$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
Y & =\left[(y_{1},t_{1}),...,(y_{\ell},t_{\ell})\right]\longmapsto\left[\mathbf{y}_{1},...,\mathbf{y}_{\ell}\right]\\
Z & =\left[(z_{1},t_{1}),...,(z_{\eta},t_{\eta})\right]\longmapsto\left[\mathbf{z}_{1},...,\mathbf{z}_{\eta}\right]\end{align*}

\end_inset

For each observation in the training data we define a subset of 
\begin_inset Formula $Y$
\end_inset

 which contains all the observations which fall in a time window starting
 at the training observation and of duration greater than or equal to the
 duration of the test data and, shifted to the interval 
\begin_inset Formula $t\in[0,\theta)$
\end_inset

:
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
elaborate on what the time windows are and how they're denoted
\end_layout

\end_inset


\begin_inset Formula \begin{align}
\theta & \ge\max_{t}Z-\min_{t}Z\nonumber \\
S_{i} & =\left[(y_{j},t_{j}-t_{i})|\quad\mathbf{y}\in Y,t_{i}\le t_{j}<t_{j}+\theta\right]\label{eq:sSingle}\\
\mathcal{S} & =\left[S_{1},...,S_{\ell}\right]\nonumber \end{align}

\end_inset


\end_layout

\begin_layout Standard
We treat 
\begin_inset Formula $\mathcal{S}$
\end_inset

 as a random process:
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\mathcal{S}=[\Omega^{\mathcal{S}},\mathcal{F}^{\mathcal{S}},\mathcal{P}^{\mathcal{S}}]\]

\end_inset


\end_layout

\begin_layout Standard
and observe that 
\begin_inset Formula $Z$
\end_inset

 can be treated as an observation of 
\begin_inset Formula $\mathcal{S}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
S_{Z}=\left[\left(z_{i},z_{i}-\min_{t}Z\right)|\quad(x_{i},t_{i})\in Z\right]\label{eq:SySingle}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
We can now frame the task of estimating the probability distribution of
 
\begin_inset Formula $Z$
\end_inset

 as a task of estimating a probability density function 
\begin_inset Formula $\varphi$
\end_inset

 for the random process 
\begin_inset Formula $\mathcal{S}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\Pr(X=\mathbf{x}\ |\ Y\cup Z)\mapsto\ \Pr(\mathcal{S}=\{S_{Z}\cup\mathbf{x}\}|\quad Z)\ \simeq\ \varphi(\mathbf{x},S_{Z}:\ \mathcal{S})\label{eq:PrS}\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
Multiple Channel Setting
\end_layout

\begin_layout Standard
In order to extend this result to settings in which multiple channels exist,
 we return to 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:sSingle"

\end_inset

 and extend the scope to multiple channels:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
S_{i}=\left[(y_{j}^{n},t_{j}-t_{i})|\quad\mathbf{y}^{n}\in\mathbf{Y},t_{i}\le t_{j}<t_{j}+\theta\right]\label{eq:Smultiple}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
In this case, we treat each channel as an orthonormal basis of the abstract
 space 
\begin_inset Formula $\Omega^{\mathcal{S}}$
\end_inset

.
 Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:SySingle"

\end_inset

 is likewise extended in the same manner:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
S_{Z}=\left[\left(z_{i}^{n},z_{i}-\min_{t}Z\right)|\quad\mathbf{z}^{n}\in\mathbf{Z}\right]\label{eq:SyMultiple}\end{equation}

\end_inset


\end_layout

\begin_layout Section
Parzen Window Estimation
\end_layout

\begin_layout Standard
One method of evaluating 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:PrS"

\end_inset

 is by using the Parzen Window method.
 We choose the Parzen Window method because it allows us to estimate probabiliti
es of unordered sets, provided they have an addition operation and a kernel
 function exists to provide a distance metric.
\end_layout

\begin_layout Subsection
 Single Channel Parzen Window
\end_layout

\begin_layout Standard
We will again begin by considering the single-channel case, then extend
 the resulting equations as necessary.
 The Parzen Window method requires the definition of a metric 
\begin_inset Formula $\|\cdot\|$
\end_inset

 over the abstract space 
\begin_inset Formula $\Omega^{\mathcal{S}}$
\end_inset

.
 Such a metric can be defined using the symmetric Kullbeck Liebler divergence
 with probability measures 
\begin_inset Formula $\phi_{n}(\mathbf{x})\simeq Pr(X=\mathbf{x}\ |\ S_{n})$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align}
D_{KL}(S_{n}\|S_{m}) & =\sum_{\mathbf{x}\in\{S_{n}\cup S_{m}\}}\phi_{n}(\mathbf{x})\log\frac{\phi_{n}(\mathbf{x})}{\phi_{m}(\mathbf{x})}\label{eq:KLsingle}\\
 & =-\sum_{\mathbf{x}\in\{S_{n}\cup S_{m}\}}\phi_{n}(\mathbf{x})\log\phi_{m}(\mathbf{x})+\sum_{\mathbf{x}\in\{S_{n}\cup S_{m}\}}\phi_{n}(\mathbf{x})\log\phi_{n}(\mathbf{x})\\
\|S_{n}-S_{m}\|_{KL} & =D_{KL}(S_{n}\|S_{m})+D_{KL}(S_{m}\|S_{n})\end{align}

\end_inset

 
\end_layout

\begin_layout Standard
The Parzen Window estimation of 
\begin_inset Formula $\mathcal{P}^{\mathcal{S}}$
\end_inset

 is defined as:
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
I don't think the Pr equation is correct - the arguments should probably
 be 
\begin_inset Formula $S_{\hat{x}}$
\end_inset

and 
\begin_inset Formula $S_{i}$
\end_inset

 or something similar
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\Pr(S\ |\ \mathcal{S})\simeq\sum_{i=1}^{\ell}\frac{1}{\ell}K_{\gamma}(S,S_{i})\label{eq:PrSParzenSingle}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where
\begin_inset Formula $K$
\end_inset

 is some kernel function, for instance the Radial Basis Function:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\mathcal{K}_{\gamma}(S_{i},S_{j})=\frac{1}{\gamma\sqrt{2\pi}}e^{-\frac{1}{\gamma}\|S_{i}-S_{j}\|_{KL}^{2}}\label{eq:KRBFparzen}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The same method can be used to estimate 
\begin_inset Formula $\phi_{n}(\mathbf{x})$
\end_inset

 for a given subset 
\begin_inset Formula $S_{n}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\phi_{n}(\mathbf{x}:\ S_{n})=\sum_{\mathbf{x}_{i}\in S_{n}}\frac{1}{|S_{n}|}k_{\gamma}(\mathbf{x},\mathbf{x}_{i})\label{eq:PhiSingle}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset Formula $|\cdot|$
\end_inset

 denotes cardinality.
 Substituting 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:PrSParzenSingle"

\end_inset

 into 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:PrS"

\end_inset

 our probability distribution estimate becomes:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\varphi_{P}(\mathbf{x},S_{Z}:\ \mathcal{S})=\sum_{i=1}^{\ell}\frac{1}{\ell}K_{\gamma}\left(\{S_{Z}\cup\mathbf{x}\},S_{i}\right)\label{eq:VarphiParzenSingle}\end{equation}

\end_inset

 
\end_layout

\begin_layout Subsection
Multiple Channel Parzen Window
\end_layout

\begin_layout Standard
Extending the Parzen Window approach requires the realization that 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:KLsingle"

\end_inset

 requires that 
\begin_inset Formula $S_{n}$
\end_inset

 and 
\begin_inset Formula $S_{m}$
\end_inset

 both be defined over the same abstract space 
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
 For instance if 
\begin_inset Formula $X_{n}\in\mathbb{R}^{2}$
\end_inset

 and 
\begin_inset Formula $X_{m}\in\mathbb{R}^{3}$
\end_inset

, it is impossible to calculate 
\begin_inset Formula $\phi_{n}\big((x_{m},t)\big)$
\end_inset

 because the quantituy 
\begin_inset Formula $\|x_{m}-x_{n}\|^{2}$
\end_inset

 from using 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:PhiSingle"

\end_inset

 is ambiguous.
\end_layout

\end_inset

.
 As mentioned earlier, in the Multiple Channel context each channel is treated
 as an orthonormal basis of 
\begin_inset Formula $\Omega^{\mathcal{S}}$
\end_inset

.
 An obvious approach to defining a measure over 
\begin_inset Formula $\Omega^{\mathcal{S}}$
\end_inset

 for mutliple channels is to use the Euclidean norm of the Kullbeck Liebler
 divergence of each channel considered independently:
\end_layout

\begin_layout Standard
\begin_inset Formula \[
S_{n}^{c}=S_{n}\cap X^{c}\]

\end_inset


\end_layout

\begin_layout Standard
This requires the following minor extension of 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:KRBFparzen"

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\mathcal{K}_{\gamma}(S_{i},S_{j})=\prod_{c}\frac{1}{\gamma\sqrt{2\pi}}e^{-\frac{1}{\gamma}\|S_{i}^{c}-S_{j}^{c}\|_{KL}^{2}}\label{eq:KRBFParzenMultiple}\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
Parzen Window with Windowing Function
\end_layout

\begin_layout Standard
Another way to approach the estimation problem is to consider the implications
 of using duration value larger than the duration of the training data.
 In this case defining multiple subsets has little value as the subsets
 no longer capture a localized behavior.
 This does not, however, prevent us from using the Parzen Window method.
 Rather than segmenting the training data and comparing it to the test data,
 we can simply 'slide' the training data to a point on the time dimension
 where the two sets overlap and compare the shifted test data to the training
 data.
 If we assign a variable to the magnitude of the shift, we can integrate
 over that variable to produce the effect of having defined infinite subsets.
\end_layout

\begin_layout Standard
We can re-frame the situation by substituting a windowing function 
\begin_inset Formula $\omega(\cdot)$
\end_inset

 for explicit subsets.
 Rather than using subsets whose membership is defined by a time and duration
 parameter, we can define a window on the input data defined by the windowing
 function:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\boldsymbol{\alpha} & =\left[\alpha_{1},\alpha_{2},...,\alpha_{\ell}\right]\\
t_{i} & \in\alpha_{i}\\
\bar{k}_{\gamma}(\mathbf{x}_{i},\mathbf{x}_{j}:\ \alpha_{k}) & =k_{\gamma}\left(\mathbf{x}_{i},(x_{j},t_{j}-t_{k})\right)\\
\bar{\phi}(\mathbf{x},X:\ \alpha) & =\sum_{\mathbf{x}_{i}\in X}\omega(\mathbf{x}_{i}:\ \alpha)\frac{1}{\sum_{\mathbf{x}\in X}\omega(\mathbf{x}:\ \alpha)}\bar{k}_{\gamma}(\mathbf{x},\mathbf{x}_{i}:\ \alpha)\\
\bar{D}_{KL}(\mathbf{x},\alpha_{i},\alpha_{j}:\ X,Y) & =\sum_{\mathbf{x}_{k}\in X\cup\mathbf{x}}\bar{\phi}(\mathbf{x}_{k},X:\ \alpha_{i})\log\frac{\bar{\phi}(\mathbf{x}_{k},X:\ \alpha_{i})}{\bar{\phi}(\mathbf{x}_{k},Y:\ \alpha_{j})}\\
\|\mathbf{x},\alpha_{i},\alpha_{j}:\ X,Y\|_{KL} & =\bar{D}_{KL}(\mathbf{x},\alpha_{i},\alpha_{j}:\ X,Y)+\bar{D}_{KL}(\mathbf{x},\alpha_{j},\alpha_{i}:\ X,Y)\\
\bar{K}_{\gamma}(\mathbf{x},\alpha_{i},\alpha_{j}:\ X,Y) & =\frac{1}{\gamma\sqrt{2\pi}}e^{-\frac{1}{\gamma}\|\mathbf{x},\alpha_{i},\alpha_{j}:\ X,Y\|_{KL}^{2}}\\
\bar{\varphi}_{P}(\mathbf{x}:\ \boldsymbol{\alpha},X,Y) & =\sum_{\begin{array}{c}
i\in\boldsymbol{\alpha}\\
j\in\boldsymbol{\alpha}\end{array}}\frac{1}{|\boldsymbol{\alpha}|^{2}}\bar{K}_{\gamma}(\mathbf{x},\alpha_{i},\alpha_{j}:\ X,Y)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Notice that these equations reduce to the previous set of equations if we
 set the windowing function to a step function defined by a time and duration
 parameter:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\alpha_{i} & =(t_{i},\theta)\\
\omega(\mathbf{x},X:\ \alpha_{i}) & =\begin{cases}
1: & t_{i}\le t<t_{i}+\theta\\
0: & \text{otherwise}\end{cases}\end{align*}

\end_inset


\end_layout

\begin_layout Subsection
Sliding Windowing Function
\end_layout

\begin_layout Standard
Thus far we have been dealing with a discrete set of windowing parameters
 and making comparisons between sets of parameters.
 Another approach is to treat the time offsets between windows as a parameter
 
\begin_inset Formula $\tau$
\end_inset

 and to investigate the divergence between the two windows at different
 values of 
\begin_inset Formula $\tau$
\end_inset

.
 In this approach, rather than evaluating the divergence at the single value
 of 
\begin_inset Formula $\tau=0$
\end_inset

, we evaluate the cumulative value of the divergence over the range 
\begin_inset Formula $\tau\in(-\infty,\infty)$
\end_inset

, which is to say we evaluate the following integral:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\|\alpha_{i},\alpha_{j}:\ X,\hat{X}\|_{S} & =\int_{-\infty}^{\infty}\|\alpha_{i},\alpha_{j}:\ X,\hat{X}\|d\tau\\
\bar{k}_{\gamma}(\mathbf{x}_{i},\mathbf{x}_{j}:\ \tau) & =k_{\gamma}\left(\mathbf{x}_{i},(x_{j},t_{j}-\tau)\right)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Right, so now just finish that integration and you're good to go...
 :)
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
(old) Sliding Parzen Windows
\end_layout

\begin_layout Standard
Thus far we have been dealing with discrete subsets with specific starting
 points and durations.
 We have compared these against test sets by shifting them along the time
 axis so their starting points align.
 Another approach is to treat the time difference between a training set
 and our test set as a variable 
\begin_inset Formula $\tau$
\end_inset

 and to investigate the divergence between the two sets at different values
 of 
\begin_inset Formula $\tau$
\end_inset

.
 In this approach, rather than evaluating the divergence at the single value
 of 
\begin_inset Formula $\tau=0$
\end_inset

, we evaluate the cumulative value of the divergence over the range 
\begin_inset Formula $\tau\in(-\infty,\infty)$
\end_inset

, which is to say we evaluate the following integral:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\|S_{\mathbf{x}}-S_{n}\|_{S} & =\int_{-\infty}^{\infty}\|S_{\mathbf{x}}-S_{n}\|d\tau\\
\phi_{n}^{\tau}(\mathbf{x}) & =\sum_{\mathbf{x}_{i}\in S_{n}}\frac{1}{|S_{n}|}K_{\gamma}(\mathbf{x},(x_{i},t_{i}+\tau))\end{align*}

\end_inset


\end_layout

\begin_layout Standard
This integration requires a combination of divergence measure and kernel
 function which can be integrated.
 We use the Pearson divergence :
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Check this integration
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\int\|S_{\mathbf{x}}-S_{n}\|_{P}\ d\tau & =\int\sum_{\mathbf{\bar{x}}\in S_{\mathbf{x}}}\left(\frac{\phi_{n}^{\tau}(\mathbf{\bar{x}})}{\phi_{\mathbf{x}}(\mathbf{\bar{x}})}-1\right)^{2}d\tau\\
 & =\sum_{\mathbf{\bar{x}}\in S_{\mathbf{x}}}\int\frac{\phi_{n}^{\tau}(\mathbf{\bar{x}})}{\phi_{\mathbf{x}}(\mathbf{\bar{x}})}^{2}d\tau-2\int\frac{\phi_{n}^{\tau}(\mathbf{\bar{x}})}{\phi_{\mathbf{x}}(\mathbf{\bar{x}})}d\tau+\int1d\tau\\
 & =\sum_{\mathbf{\bar{x}}\in S_{\mathbf{x}}}\int\sum_{\begin{array}{c}
\mathbf{x}_{i}\in S_{n}\\
\mathbf{x}_{j}\in S_{n}\end{array}}\frac{1}{|S_{n}|^{2}\phi_{\mathbf{x}}(\mathbf{\bar{x}})^{2}}K_{\gamma}(\mathbf{\bar{x}},(x_{i},t_{i}+\tau))K_{\gamma}(\mathbf{\bar{x}},(x_{j},t_{j}+\tau))d\tau-2\int\sum_{\mathbf{x}_{i}\in S_{n}^{\tau}}\frac{1}{|S_{n}|\phi_{\mathbf{x}}(\mathbf{\bar{x}})}K_{\gamma}(\mathbf{\bar{x}},(x_{i},t_{i}+\tau))d\tau+\int1d\tau\\
 & =\sum_{\mathbf{\bar{x}}\in S_{\mathbf{x}}}\sum_{\begin{array}{c}
\mathbf{x}_{i}\in S_{n}\\
\mathbf{x}_{j}\in S_{n}\end{array}}\frac{1}{|S_{n}|^{2}\phi_{\mathbf{x}}(\mathbf{\bar{x}})^{2}}\ \int K_{\gamma}(\mathbf{\bar{x}},(x_{i},t_{i}+\tau))K_{\gamma}(\mathbf{\bar{x}},(x_{j},t_{j}+\tau))d\tau-2\int\sum_{\mathbf{x}_{i}\in S_{n}^{\tau}}\frac{1}{|S_{n}|\phi_{\mathbf{x}}(\mathbf{\bar{x}})}\ \int K_{\gamma}(\mathbf{\bar{x}},(x_{i},t_{i}+\tau))d\tau+\int1d\tau\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Because we know the integral of the kernel function over the real line is
 1, we can eliminate that term over the range 
\begin_inset Formula $(-\infty,\infty)$
\end_inset

, and because our kernel function uses the tensor product of each dimension
 of 
\begin_inset Formula $\mathbf{x}$
\end_inset

, we can separate the kernel function into a portion dependant on 
\begin_inset Formula $\tau$
\end_inset

 and an independant portion.
 Finally, because this divergence is intended as a comparison, we eliminate
 the constant terms: 
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
WTF? Read up, try to work this through on paper
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\int\sum_{\mathbf{x}\in S_{\mathbf{x}}}\|S_{\mathbf{x}}-S_{n}\|_{P}\ d\tau & =\sum_{\bar{\mathbf{x}}\in S_{\mathbf{x}}}\sum_{\begin{array}{c}
\mathbf{x}_{i}\in S_{n}\\
\mathbf{x}_{j}\in S_{n}\end{array}}\frac{1}{|S_{n}|^{2}\phi_{\mathbf{x}}(\mathbf{\bar{x}})^{2}}K_{\gamma}(x,x_{i})K_{\gamma}(x,x_{j})\ \int K_{\gamma}(t,t_{i}+\tau)K_{\gamma}(t,t_{j}+\tau)d\tau-\frac{2}{\phi_{\mathbf{x}}(\bar{\mathbf{x}})}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The final term can be eliminated as the probability 
\begin_inset Formula $\phi_{\mathbf{x}}$
\end_inset

 over the range 
\begin_inset Formula $S_{\mathbf{x}}$
\end_inset

 must be 1, reducing the final term to a constant.
 Substituting the Gaussian kernel, our divergence measure become:
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
I assume that there was some Wolfram magic here...
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\int\sum_{\mathbf{x}\in S_{\mathbf{x}}}\|S_{\mathbf{x}}-S_{n}\|_{P}\ d\tau & =\sum_{\mathbf{\bar{x}}\in S_{\mathbf{x}}}\sum_{\begin{array}{c}
\mathbf{x}_{i}\in S_{n}\\
\mathbf{x}_{j}\in S_{n}\end{array}}\frac{1}{|S_{n}|^{2}\gamma^{2}2\pi\phi_{\mathbf{x}}(\mathbf{\bar{x}})^{2}}e^{-\frac{1}{\gamma}\left(\bar{x}-x_{i}\right)^{2}}e^{-\frac{1}{\gamma}\left(\bar{x}-x_{j}\right)^{2}}\ \int_{-\infty}^{\infty}e^{-\frac{1}{\gamma}\left(\bar{t}-t_{i}-\tau\right)^{2}}e^{-\frac{1}{\gamma}\left(\bar{t}-t_{j}-\tau\right)^{2}}d\tau\\
 & =\sum_{\mathbf{\bar{x}}\in S_{\mathbf{x}}}\sum_{\begin{array}{c}
\mathbf{x}_{i}\in S_{n}\\
\mathbf{x}_{j}\in S_{n}\end{array}}\frac{1}{|S_{n}|^{2}\gamma^{2}2\pi\phi_{\mathbf{x}}(\mathbf{\bar{x}})^{2}}e^{-\frac{1}{\gamma}\left(\bar{x}-x_{i}\right)^{2}-\frac{1}{\gamma}\left(\bar{x}-x_{j}\right)^{2}}\ \int_{-\infty}^{\infty}e^{-\frac{1}{\gamma}\left(\bar{t}-t_{i}-\tau\right)^{2}-\frac{1}{\gamma}\left(\bar{t}-t_{j}-\tau\right)^{2}}d\tau\\
 & =\sum_{\mathbf{\bar{x}}\in S_{\mathbf{x}}}\sum_{\begin{array}{c}
\mathbf{x}_{i}\in S_{n}\\
\mathbf{x}_{j}\in S_{n}\end{array}}\frac{1}{|S_{n}|^{2}\gamma^{2}2\pi\phi_{\mathbf{x}}(\mathbf{\bar{x}})^{2}}e^{-\frac{1}{\gamma}\left(\bar{x}-x_{i}\right)^{2}-\frac{1}{\gamma}\left(\bar{x}-x_{j}\right)^{2}}-\lim_{\tau\rightarrow\infty}\frac{1}{2}e^{-\frac{1}{2\gamma}(t_{j}-t_{i})^{2}}\sqrt{\frac{\gamma\pi}{2}}\text{erf}\left(\frac{(2\bar{t}-t_{i}-t_{j}-2\tau)}{\sqrt{2\gamma}}\right)\\
 & \qquad\qquad\qquad+\lim_{\tau\rightarrow-\infty}\frac{1}{2}e^{-\frac{1}{2\gamma}(t_{j}-t_{i})^{2}}\sqrt{\frac{\gamma\pi}{2}}\text{erf}\left(\frac{(2\bar{t}-t_{i}-t_{j}-2\tau)}{\sqrt{2\gamma}}\right)\\
 & =\sum_{\mathbf{\bar{x}}\in S_{\mathbf{x}}}\sum_{\begin{array}{c}
\mathbf{x}_{i}\in S_{n}\\
\mathbf{x}_{j}\in S_{n}\end{array}}\frac{1}{|S_{n}|^{2}\gamma^{2}2\pi\phi_{\mathbf{x}}(\mathbf{\bar{x}})^{2}}e^{-\frac{1}{\gamma}\left(\bar{x}-x_{i}\right)^{2}-\frac{1}{\gamma}\left(\bar{x}-x_{j}\right)^{2}}+\frac{1}{2}\sqrt{\frac{\pi}{2\gamma}}e^{-\frac{1}{2\gamma}(t_{j}-t_{i})^{2}}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Our new divergence measure 
\begin_inset Formula $\|S_{\mathbf{x}}-S_{n}\|_{S}$
\end_inset

 can now be substituted into 
\begin_inset Formula $\varphi(\mathbf{x})$
\end_inset

.
 We can make a fundamental change to the Parzen density estimation approach
 by observing that we can set 
\begin_inset Formula $S_{n}=X$
\end_inset

 without any loss of predictive ability.
 The continuous sliding window approach eliminates the need for specific
 windows, allowing us to evaluate the probability of a test set 
\begin_inset Formula $\hat{X}$
\end_inset

 based on a training set 
\begin_inset Formula $X$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\varphi_{P}(\hat{X}) & =\mathcal{K}_{\gamma}(\hat{X},X)\\
K_{\gamma}(\hat{X},X) & =\frac{1}{\gamma\sqrt{2\pi}}e^{-\frac{1}{\gamma}\|\hat{X}-X\|_{KL}^{2}}\\
\|\hat{X}-X\|_{S} & =\sum_{\mathbf{\bar{x}}\in\hat{X}}\sum_{\begin{array}{c}
\mathbf{x}_{i}\in X\\
\mathbf{x}_{j}\in X\end{array}}\frac{1}{|X|^{2}\gamma^{2}2\pi\phi_{\hat{X}}(\mathbf{\bar{x}})^{2}}e^{-\frac{1}{\gamma}\left(\bar{x}-x_{i}\right)^{2}-\frac{1}{\gamma}\left(\bar{x}-x_{j}\right)^{2}}+\frac{1}{2}\sqrt{\frac{\pi}{2\gamma}}e^{-\frac{1}{2\gamma}(t_{j}-t_{i})^{2}}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Aug 31: This section seems to be worked out.
 The last bit of math is over my head right now, but I seem to remember
 working it out correctly and spending a lot of time verifying the equations.
 This is certainly worth re-hashing (especially since familiarity will probably
 be required for the remaining parts) but I suspect it's correct and complete.
\end_layout

\end_inset


\end_layout

\begin_layout Section
Support Vector Estimation
\end_layout

\begin_layout Standard
The Parzen Window method is neither sparse nor computationally efficient,
 and as the number of observations grows, these deficiencies quickly become
 prohibitive.
 We now investigate the use of Support Vector Machines to generate estimates.
\end_layout

\begin_layout Subsection
Random Process Estimation 
\end_layout

\begin_layout Standard
Support Vector Machines are often used
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
ref?
\end_layout

\end_inset

 to estimate probability distributions by solving the related problem of
 estimating the cumulative distribution function of the random variable
 in question.
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
This isn't really the essence of SVM - put more verbage in the intro regarding
 what SVM's are, why they work, and why they're superior to other approaches
 (say, neural networks)
\end_layout

\end_inset

 This reduces the problem to one of estimating a non-linear mapping from
 observations to cumulative distribution values, which can be formulated
 as an optimization problem over a linear operator equation.
 Unfortunately, these methods depend on the ability to calculate an empirical
 distribution for each observation:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
F_{\ell}(x)=\frac{1}{\ell}\sum_{i}\theta(x-x_{i})\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\theta(x)$
\end_inset

 is the indicator function.
 To evaluate this function, the abstract space 
\begin_inset Formula $\Omega^{\mathcal{S}}$
\end_inset

 must be ordered.
 While we have described a distance metric over 
\begin_inset Formula $\Omega^{\mathcal{S}}$
\end_inset

, it is not clear what a meaningful ordering relation would be.
\end_layout

\begin_layout Standard
Rather than calculating the cumulative probability distribution of 
\begin_inset Formula $X$
\end_inset

, we begin with the assumption that the Parzen Window estimate of the probabilit
y distribution is acceptably accurate and attempt to minimize the difference
 between the Support Vector estimate and the Parzen Window estimate.
 The support vector approach requires that we define an optimization problem
 
\begin_inset Formula $W(\beta)$
\end_inset

 which will determine which observations will be used in estimations and
 which can be discarded as redundant or irrelevant information by defining
 a set of weights 
\begin_inset Formula $\beta$
\end_inset

, some of which will be non-zero.
 We define the optimization problem as minimizing the square loss between
 the Support Vector and Parzen Window estimates:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
W(\beta) & =\sum_{i=1}^{\ell}\left(\varphi_{P}(\mathbf{x})-\varphi_{SV}(\mathbf{x}:\ \beta)\right)^{2}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We must therefore develop a method of evaluating the 'usefulness' of each
 point in a set of training data.
 To do this we begin by observing that the duration of the training set
 is likely larger than the duration of test sets, which means we will have
 to make estimates using less information that is available in the training
 data; whatever optimization problem we define will need to take this into
 account.
 The optimization equation compares the results of Parzen estimation to
 the results of SVM estimation using a portion of the training data as synthetic
 testing data.
 This can be accomplished using subsets of the training data, but a more
 flexible approach is to introduce a windowing kernel function.
 This windowing function operates by adjusting the influence of the distance
 between two points on the value of 
\begin_inset Formula $\phi(\mathbf{x})$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\bar{\phi}(\mathbf{x}:\ X,\tau,\alpha) & =\sum_{\mathbf{x}_{i}\in X}\omega(\mathbf{x}_{i}:\ \tau,\alpha)\frac{1}{\sum_{\mathbf{x}\in X}\omega(\mathbf{x}:\ \tau,\alpha)}\bar{k}_{\gamma}(\mathbf{x},\mathbf{x}_{i}:\ \tau)\\
\bar{k}_{\gamma}(\mathbf{x}_{i},\mathbf{x}_{j}:\ \tau) & =k_{\gamma}\left(\mathbf{x}_{i},(x_{j},t_{j}-\tau)\right)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
This approach is much more powerful than using subsets, as it allows a gradient
 of magnitudes in the influence of each point, rather than forcing a binary
 decision of included/not included.
 In order to use this approach, we must modify both the distance metric
 and the kernel function to handle the additional parameters 
\begin_inset Formula $\tau$
\end_inset

 (which specifies the location of the windowing function in the time dimension)
 and 
\begin_inset Formula $\alpha$
\end_inset

 (which specifies parameters of the windowing function such as duration).
 We reformulate the Parzen Window equations thus:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\|X,Y:\ \tau,\alpha\|_{KL} & =\sum_{\mathbf{x}_{k}\in X\cup\mathbf{x}}\bar{\phi}(\mathbf{x}_{k}:\ X,\tau,\alpha)\log\frac{\bar{\phi}(\mathbf{x}_{k}:\ X,\tau,\alpha)}{\phi(\mathbf{x}_{k}:\ Y)}+\sum_{\mathbf{x}_{k}\in X\cup\mathbf{x}}\phi(\mathbf{x}_{k}:\ Y)\log\frac{\phi(\mathbf{x}_{k}:\ Y)}{\bar{\phi}(\mathbf{x}_{k}:\ X,\tau,\alpha)}\\
\bar{K}_{\gamma}\left(X,Y:\ \tau,\alpha\right) & =\frac{1}{\gamma\sqrt{2\pi}}e^{-\frac{1}{\gamma}\|X,Y:\ \tau,\alpha\|_{KL}^{2}}\\
\bar{\varphi}_{P}(\mathbf{x},Z:\ \alpha,\tau,Y) & =\int\sum_{i=1}^{\ell}\frac{1}{\ell}\bar{K}_{\gamma}\left(\{Y\cup\mathbf{x}\},Z:\ \tau,\alpha\right)d\tau\end{align*}

\end_inset


\end_layout

\begin_layout Standard
This formulation essentially replaces explicit subsets with equivalent functions
 based on the windowing function and adds our windowing parameters to the
 function.
 We can now elaborate on the 'target functional' of the Support Vector approach;
 the function which will be used to generate estimates once the optimization
 problem 
\begin_inset Formula $W(\beta)$
\end_inset

 has been solved.
 The target functional replaces the normalizing term 
\begin_inset Formula $\frac{1}{\ell}$
\end_inset

 in the Parzen Window function with 
\begin_inset Formula $\beta$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align}
\bar{\varphi}_{SV}(\mathbf{x},Z:\ \alpha,\tau,\beta,Y) & =\int\sum_{i=1}^{\ell}\beta_{i}\bar{K}_{\gamma}\left(\{Y\cup\mathbf{x}\},Z:\ \alpha,\tau\right)d\tau\label{eq:SVResultSingle}\end{align}

\end_inset


\end_layout

\begin_layout Standard
We can now refine the optimiation problem 
\begin_inset Formula $W(\beta)$
\end_inset

 to take into account the new definitions of 
\begin_inset Formula $\bar{\varphi}_{P}$
\end_inset

 and 
\begin_inset Formula $\bar{\varphi}_{SV}$
\end_inset

.
 We add a regularizer term 
\begin_inset Formula $\Omega$
\end_inset

 to controll the sparseness of the result by penalizing similar windows:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
W(\beta:\ \alpha) & =\int\sum_{\mathbf{x}\in X}\left(\bar{\varphi}_{P}(\mathbf{x},Y:\ \alpha,\tau,Y)-\bar{\varphi}_{SV}(\mathbf{x},Y:\ \alpha,\tau,\beta,Y)\right)^{2}d\tau+\lambda\Omega(\boldsymbol{\alpha},\beta,Y)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\text{subject to}\quad\sum_{i}\beta_{i}=1,\quad\beta_{i}\ge0,\ i=1,\ldots,|\mathcal{S}|\label{eq:SVProcessSingle}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
We choose the following for our regularizer (a modification of 
\begin_inset Formula $\bar{K}_{\gamma}$
\end_inset

 to allow windowing of both sets):
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\Omega(\boldsymbol{\alpha},\beta,Y)=\sum_{i=1}^{\ell}\beta_{i}\int\left(\sum_{\mathbf{x}_{k}\in Y}\bar{\phi}(\mathbf{x}_{k}:\ Y,\tau,\alpha)\log\frac{\bar{\phi}(\mathbf{x}_{k}:\ Y,\tau,\alpha)}{\bar{\phi}(\mathbf{x}_{k}:\ Y,t_{i},\alpha)}+\sum_{\mathbf{x}_{k}\in X\cup\mathbf{x}}\bar{\phi}(\mathbf{x}_{k}:\ Y,t_{i},\alpha)\log\frac{\bar{\phi}(\mathbf{x}_{k}:\ Y,t_{i},\alpha)}{\bar{\phi}(\mathbf{x}_{k}:\ Y,\tau,\alpha)}\right)^{-1}d\tau\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:SVProcessSingle"

\end_inset

 is a quadratic optimiztion problem defined as:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
W(\beta) & =\frac{1}{2}\beta^{T}P\beta+q^{T}\beta\\
P_{i,j} & =\sum_{\alpha\in\boldsymbol{\alpha}}\sum_{\mathbf{x}\in X}K_{\gamma}(\mathbf{x},\alpha,\alpha_{i}:\ X,X)K_{\gamma}(\mathbf{x},\alpha,\alpha_{j}:\ X,X)\\
q_{i} & =\sum_{\alpha_{i}\in\boldsymbol{\alpha}}\sum_{\mathbf{x}\in X}\left(\lambda K_{\gamma}(\mathbf{x},\alpha,\alpha_{i})^{-1}-\frac{1}{|\boldsymbol{\alpha}|^{2}}\sum_{\alpha_{j}\in\boldsymbol{\alpha}}K_{\gamma}(\mathbf{x},\alpha,\alpha_{i}:\ X,X)K_{\gamma}(\mathbf{x},\alpha,\alpha_{j}:\ X,X)\right)\end{align*}

\end_inset

 
\begin_inset Formula \begin{align}
P & =\langle\mathbf{K}^{T}\cdot\mathbf{K}\rangle\label{eq:SVPSingle}\\
q & =\lambda\langle\mathbf{K}^{T}\cdot\mathbf{1}_{(|\boldsymbol{\alpha}|,1)}\rangle^{-1}-\frac{1}{|\boldsymbol{\alpha}|}\langle\mathbf{K}^{T}\cdot\mathbf{K}\cdot\mathbf{1}_{(|\boldsymbol{\alpha}|,1)}\rangle\end{align}

\end_inset


\end_layout

\begin_layout Subsection
Sliding Windows 
\end_layout

\begin_layout Standard
A final refinement to the SVM solution comes from observing that the set
 of windows need not be defined by the input data; we can factor out the
 offset and restrict 
\begin_inset Formula $\alpha$
\end_inset

 to defining parameters relevant to the input data such as window duration.
 This can be accomplished by substituting an integral for the summation
 over the window set:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\varphi_{SV}(\mathbf{x}:\ \alpha,\beta,X,Y) & =\sum_{k\in Y}\beta_{k}\bar{K}_{\gamma}(\mathbf{x}:\ \alpha,X,Y)\\
\bar{\phi}(\mathbf{x},X:\ \tau,\alpha) & =\int_{i}\int_{j}\sum_{\mathbf{x}_{i}\in X}\omega(\mathbf{x}_{i}:\ \tau,\alpha)\frac{1}{\sum_{\mathbf{x}\in X}\omega(\mathbf{x}:\ \tau,\alpha)}\bar{k}_{\gamma}(\mathbf{x},\mathbf{x}_{i}:\ \tau)d\tau d\tau\\
\bar{k}_{\gamma}(\mathbf{x}_{i},\mathbf{x}_{j}:\ \tau) & =k_{\gamma}\left(\mathbf{x}_{i},(x_{j},t_{j}-\tau)\right)\\
W(\beta) & =\int\sum_{\mathbf{x}\in X}\left(\bar{\varphi}_{P}(\mathbf{x}:\ \alpha,X,X)-\bar{\varphi}_{SV}(\mathbf{x}:\ \alpha,\beta,X,X)\right)^{2}d\tau+\lambda\Omega(\boldsymbol{\alpha},\beta)\\
\Omega(\boldsymbol{\alpha},\beta) & =\int_{i}\int_{j}\sum_{\mathbf{x}\in X}\bar{K}_{\gamma}(\mathbf{x}:\ \alpha,X,X)^{-1}d\tau d\tau\end{align*}

\end_inset


\end_layout

\begin_layout Subsection
(old) Sliding Windows
\end_layout

\begin_layout Standard
We must begin by modifying the conceptual model of the SVM solution.
 Our task is to determine which of the training set 
\begin_inset Formula $X$
\end_inset

 should be used as SV's.
 We cannot treat each observation as independant, however, because we are
 searching for timeseries patterns; each observation of 
\begin_inset Formula $X$
\end_inset

 must be considered in the context of adjacent observations.
 
\end_layout

\begin_layout Standard
We begin by proposing two functions;
\begin_inset Formula $\omega(\cdot)$
\end_inset

 which assigns weights to inputs based on their time proximity to a given
 observation and 
\begin_inset Formula $\Delta(\cdot)$
\end_inset

 which shifts the inputs to 0.
 The function 
\begin_inset Formula $\omega(\cdot)$
\end_inset

 satisfies the properties of a kernel function: it is non-negative and its
 integral is 1.
 This is a different approach than subsets, as we are not excluding any
 observations, merely reducing their impact on the optimization algorithm.
 By using this function, we can treat each observation as both a point (which
 may or may not be a SV) and a set of points (which describe a timeseries
 pattern).
 Our task can now be seen as assigning weights to each point based on the
 relevancy of the set determined by that point in reproducing the estimates
 of a parzen estimator.
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\Delta(X:t) & =\left[\left(x_{i},t_{i}-t\right)|\ \left(x_{i},t_{i}\right)\in X\right]\end{align*}

\end_inset


\end_layout

\begin_layout Standard
In order to make this comparison, we must first establish how each observation
 and its associated weight are used to make predictions.
 Recall that the SV solution to the estimation problem is
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\varphi_{SV}(\mathbf{x},S:\beta)=\sum_{i\in\mathcal{S}}\beta_{i}K_{\gamma}(S_{\mathbf{x}},S_{i})\]

\end_inset


\end_layout

\begin_layout Standard
We can now substitute our weighting function for explicit sets, producing
 the following prediction equation:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\varphi_{sSV}(\mathbf{x}:\beta,X,\hat{X}) & =\int\sum_{i\in X}\beta_{i}K_{\gamma}\left(X,\Delta(\hat{X}:\tau)\right)d\tau\\
 & =\sum_{i\in X}\beta_{i}\ \int K_{\gamma}\left(X,\Delta(\hat{X}:\tau)\right)d\tau\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Extending the sliding Parzen approach, our objective is to determine which
 points 
\begin_inset Formula $\mathbf{x}\in X$
\end_inset

 should be selected as SV's.
 Let us now develop our SV optimization problem by seeking the least squared
 error between the Parzen and SV divergences:
\begin_inset Note Note
status open

\begin_layout Plain Layout
This can't be right - the weight function reduces to one and goes away.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
W(\beta) & =\int\sum_{\mathbf{x}\in\hat{X}}\omega(t-\tau)\left(\varphi_{sP}(\mathbf{x}:\hat{X})-\varphi_{sSV}(\mathbf{x}:\beta,\hat{X},\hat{X})\right)^{2}d\tau+\lambda\Omega(\beta,\hat{X})\end{align*}

\end_inset


\end_layout

\begin_layout Standard

\end_layout

\begin_layout Standard
Where 
\begin_inset Formula $\omega(\cdot)$
\end_inset

 is some kernel function controlling sequence length.
\end_layout

\begin_layout Subsection
Parzen-SVM Decomposition 
\end_layout

\begin_layout Standard
Due to the simplicity of the constraints in the optimization problem, it
 is possible to use the decomposition method of Osuna to reduce the memory
 requirements of the Parzen-SVM algorithm.
\end_layout

\begin_layout Subsubsection
 Sub-Problem Definition 
\end_layout

\begin_layout Standard
The decomposition algorithm breaks 
\begin_inset Formula $X$
\end_inset

 into two working sets 
\begin_inset Formula $B,N$
\end_inset

, and attempts to optimize 
\begin_inset Formula $B$
\end_inset

 while keeping 
\begin_inset Formula $N$
\end_inset

 fixed.
 This results in the following iterative optimization problem where 
\begin_inset Formula $\boldsymbol{\beta}^{k}$
\end_inset

 denotes the result of the previous iteration:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
W(\boldsymbol{\beta}_{B}) & =\frac{1}{2}\begin{bmatrix}\boldsymbol{\beta}_{B}^{T} & (\boldsymbol{\beta}_{N}^{k})^{T}\end{bmatrix}\begin{bmatrix}P_{BB} & P_{BN}\\
P_{NB} & P_{NN}\end{bmatrix}\begin{bmatrix}\boldsymbol{\beta}_{B}\\
\boldsymbol{\beta}_{N}^{k}\end{bmatrix}-\begin{bmatrix}q_{B}^{T} & q_{N}^{T}\end{bmatrix}\begin{bmatrix}\boldsymbol{\beta}_{B}\\
\boldsymbol{\beta}_{N}^{k}\end{bmatrix}\\
 & =\frac{1}{2}\boldsymbol{\beta}_{B}^{T}P_{BB}\boldsymbol{\beta}_{B}-(-q_{B}+P_{BN}\boldsymbol{\beta}_{N}^{k})^{T}\boldsymbol{\beta}_{B}\\
 & =\frac{1}{2}\begin{bmatrix}\beta_{i} & \beta_{j}\end{bmatrix}\begin{bmatrix}P_{ii} & P_{ij}\\
P_{ij} & P_{jj}\end{bmatrix}\begin{bmatrix}\beta_{i}\\
\beta_{j}\end{bmatrix}-(-q_{B}+P_{BN}\boldsymbol{\beta}_{N}^{k})^{T}\begin{bmatrix}\beta_{i}\\
\beta_{j}\end{bmatrix}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\text{subject to}\quad0\le\beta_{i},\beta_{j},\quad\beta_{i}+\beta_{j}=1-\mathbf{1}^{T}\boldsymbol{\beta}_{N}^{k}\end{equation}

\end_inset


\end_layout

\begin_layout Subsubsection
 Working Set Selection 
\end_layout

\begin_layout Standard
Select
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align}
 & i\in\text{arg}\max_{t}\left\{ -\nabla f(\boldsymbol{\beta}^{k})_{t}\ |\quad t\in I(\boldsymbol{\beta}^{k})\right\} \\
 & j\in\text{arg}\min_{t}\left\{ -\frac{b_{it}^{2}}{a_{it}}\ |\quad t\in I(\boldsymbol{\beta}^{k}),\quad-\nabla f(\boldsymbol{\beta}^{k})_{t}<-\nabla f(\boldsymbol{\beta}^{k})_{i}\right\} \end{align}

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% This needs to be checked for the new optimization scenario
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align}
I(\boldsymbol{\beta}) & \equiv\{t\ |\quad\beta_{t}<1\quad\text{or}\quad\beta_{t}>0\}\\
 & a_{it}=P_{ii}+P_{tt}-2P_{it}\\
 & \bar{a}_{it}=\begin{cases}
a_{it} & \text{if}\ a_{it}>0\\
\delta & \text{otherwise}\end{cases}\\
 & b_{it}=-\nabla f(\boldsymbol{\beta}^{k})_{i}+\nabla f(\boldsymbol{\beta}^{k})_{t}\\
\nabla f(\boldsymbol{\beta})_{i} & \equiv P_{i}\boldsymbol{\beta}-q_{i}\end{align}

\end_inset


\end_layout

\begin_layout Subsubsection
 Stopping Condition 
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\max_{i\in I(\boldsymbol{\alpha}^{k})}-\nabla f(\boldsymbol{\alpha})_{i}+\min_{j\in I(\boldsymbol{\alpha}^{k})}\nabla f(\boldsymbol{\alpha})_{j}\le\epsilon\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
 Random Process Kernel Definition 
\end_layout

\begin_layout Standard
When developing the Parzen Window algorithm, we used kernel function 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:KLsingle"

\end_inset

 which is based on the Kullbeck Liebler divergence of a probability estimate
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:PhiSingle"

\end_inset

 defined at the sets being evaluated.
 Our motivation in developing a Support Vector approach is to generate sparse
 representations of 
\begin_inset Formula $\mathcal{P}^{\mathcal{S}}$
\end_inset

, in part to reduce the computational demands of evaluating 
\begin_inset Formula $\varphi(\mathbf{x},S)$
\end_inset

 for test data sets.
 Unfortunately, kernel function 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:KLsingle"

\end_inset

 requires probability estimates of both sets being compared - it would be
 helpful to develop a kernel function capable of evaluating a distance between
 a test set 
\begin_inset Formula $S_{\hat{X}}$
\end_inset

 and a training set 
\begin_inset Formula $S_{i}$
\end_inset

 without first calculating an estimate of the probability distribution of
 
\begin_inset Formula $S_{\hat{X}}$
\end_inset

.
\end_layout

\begin_layout Standard
Note that 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:SVProcessSingle"

\end_inset

 is formulated in such a way that the first kernel argument is always 
\begin_inset Formula $S_{\mathbf{x}}$
\end_inset

, which allows us to define a kernel which takes a set of points as its
 first argument and a probability distribution as its second.
 The definition of 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:SVPSingle"

\end_inset

 also allows us to define non-symmetric kernels without sacrificing convexity
 or monotonicity in the optimization objective function.
 We can therefore use any measure of the divergence between a set of points
 and a probability distribution.
 We consider the Renyi entropy with 
\begin_inset Formula $\alpha=1$
\end_inset

 and select the Shannon Entropy as our kernel metric, as it is equivalent
 the the Kullback Lieblier divergence.
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\|X-\phi\|_{H}=\sum_{x\in X}\phi(x)\log\phi(x)\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Recalling that 
\begin_inset Formula $S_{\mathbf{x}}=\{S\cup\mathbf{x}\}$
\end_inset

, we observe that:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\|S_{\mathbf{x}}-S_{i}\|_{H}=\|S-S_{i}\|_{H}+\|\mathbf{x}-S_{i}\|_{H}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
We restate the kernel function as:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
K_{\gamma}(X,S_{i})=\frac{1}{\gamma\sqrt{2\pi}}e^{-\frac{1}{\gamma}\|X-S_{i}\|_{H}^{2}}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
In situations where probabilities are being calculated for a uniform set
 of points in 
\begin_inset Formula $\Omega^{\mathcal{S}}$
\end_inset

, this can considerably reduce the computational time needed to evaluate
 
\begin_inset Formula $\mathbf{K}$
\end_inset

, as 
\begin_inset Formula $\|\mathbf{x}-S_{i}\|_{H}$
\end_inset

 can be computed once and then added to each 
\begin_inset Formula $\|S-S_{i}\|_{H},S\in\mathcal{S}$
\end_inset

.
\end_layout

\begin_layout Subsection
 Random Vector Estimation 
\end_layout

\begin_layout Standard
Evaluating 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:SVProcessSingle"

\end_inset

 over a training set 
\begin_inset Formula $X$
\end_inset

 produces a set 
\begin_inset Formula $\mathcal{S}_{SV}\in\mathcal{S}$
\end_inset

 referred to as Support Vectors.
 While 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:SVProcessSingle"

\end_inset

 produces a sparse representation of 
\begin_inset Formula $\mathcal{P}^{\mathcal{S}}$
\end_inset

 by eliminating some 
\begin_inset Formula $S$
\end_inset

, further sparseness can be achieved by refining the definition of 
\begin_inset Formula $\phi(\mathbf{x})$
\end_inset

 to allow the elimination of some 
\begin_inset Formula $\mathbf{x}$
\end_inset

 in each 
\begin_inset Formula $S\in\mathcal{S}_{SV}$
\end_inset

.
 In the context of computing the kernel matrices used in optimizing 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:SVProcessSingle"

\end_inset

 the Parzen Window definition of 
\begin_inset Formula $\phi(\mathbf{x})$
\end_inset

 is computationally acceptable, as it results in a good approximation with
 a minimal amount of computation 
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
 The Parzen Window estimate at a point requires distance computations for
 each observation and a summation.
 By contrast, the SV estimate requires the same number of distance computations
 as well as the solving of a quadratic optimization problem whose complexity
 increases exponentially with the number of observations.
 
\end_layout

\end_inset

.
 In the context of evaluating 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:SVResultSingle"

\end_inset

 over a testing set 
\begin_inset Formula $\hat{X}$
\end_inset

, the Parzen Window algorithm for 
\begin_inset Formula $\phi(\mathbf{x})$
\end_inset

 is sub-optimal due to the fact that it requires the full set of observations
 for each 
\begin_inset Formula $S\in\mathcal{S}_{SV}$
\end_inset

.
 In this context, a sparse algorithm for 
\begin_inset Formula $\phi(\mathbf{x})$
\end_inset

 would reduce both the data required to store 
\begin_inset Formula $\mathcal{S}_{SV}$
\end_inset

 and the computational resources required to evaluate 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:DKLemp"

\end_inset

.
\end_layout

\begin_layout Standard
We return now to the empirical cumulative distribution function method of
 Support Vector density estimation and search for a solution in the following
 form, which is able to use multiple non-symmetric kernel functions simultaneous
ly:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\phi(x)=\sum_{i=1}^{\ell}\left(\beta_{i}^{1}\mathcal{K}_{1}(x_{i},x)+...+\beta_{i}^{\kappa}\mathcal{K}_{\kappa}(x_{i},x)\right)\label{eq:SVDensity}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
We select values of 
\begin_inset Formula $\beta$
\end_inset

 which minimize the square loss of the empirical cumulative probability
 distribution using some regularizer:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align}
 & \min\left(\sum_{i=1}^{\ell}\left(y_{i}-\sum_{j=1}^{\ell}\sum_{n=1}^{\kappa}\beta_{j}^{n}k_{n}(x_{i},x_{j})\right)^{2}+\lambda\sum_{i=1}^{\ell}\sum_{n=1}^{\kappa}\frac{1}{\gamma_{n}}\beta_{i}^{n}\right)\\
 & \text{subject to}\quad\sum_{i=1}^{\ell}\sum_{n=1}^{\kappa}\beta_{i}^{n}=1,\quad\beta_{i}\ge0\end{align}

\end_inset


\end_layout

\begin_layout Standard
given a kernel function 
\begin_inset Formula $k(x,x')$
\end_inset

 from the sigmoid family to approximate the cumulative probability distribution
 and it's derivative which we refer to as the cross-kernel 
\begin_inset Formula $\mathcal{K}(x,x')$
\end_inset

 which we use to construct an estimate of the probability distribution:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align}
 & k(x,x')=\frac{1}{1+e^{-\gamma(x-x')}}\\
 & \mathcal{K}(x,x')=-\frac{\gamma}{2+e^{\gamma(x-x')}+e^{-\gamma(x-x')}}\end{align}

\end_inset


\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Subsection
Eunite Competition Data
\end_layout

\begin_layout Subsection
Santa Fe Data
\end_layout

\begin_layout Subsection
CATS Benchmark Data
\end_layout

\begin_layout Subsection
Results Summary
\end_layout

\begin_layout Section
Further Research
\end_layout

\begin_layout Subsection
Data Pre-Processing
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% logistic function using mean and sd to put most training points between
 .1 and .9
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% upper and lower bounds on 
\backslash
(
\backslash
int 
\backslash
Delta 
\backslash
) as non-stationary data detections mechanism
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% logistic function from delta using mean and sd in same way if data non-station
ary
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% iterative integration process until stationary data found ?
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Ensemble System
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% more analysis of recent data, some type of transfer from short to long-term
 'memory'
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% heirarchical system?
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% what parameters change?
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Weighted Influence
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% a weight term could be added to the regularizer to control the influence
 of specific sets.
  This could provide more information in 'important' situations, which could
 be useful in motivated learning.
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Standard
Eat it, bitches
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
nocite{Moreno03}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
bibliographystyle{plain}
\end_layout

\end_inset

 
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "Research/research.bib"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
