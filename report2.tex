\documentclass[10pt]{article}

\usepackage{amsmath}
\usepackage{amsfonts}

\usepackage{hyperref}
\usepackage[all]{hypcap}

\setlength{\parskip}{.25cm plus4mm minus3mm}
\setlength{\parindent}{0in}
\setlength{\voffset}{-.5in}
\setlength{\hoffset}{-.5in}
\setlength{\textwidth}{6.0in}


% Pick a title and make sure the terminology is consistent (architecture / system / algoirthm / etc)

% Look into Hidden Markov to see if applicable, if not see if there are other approaches to timeseries statistics
% If there's no existing theory to explain the mixture model, you'll need to spend more time on it

% Do we need to think about inhibition at all?

% Window selection for estimates is *important*, and deserves its own section and algorithm. 


\begin{document}
\title{Robust Timeseries Analysis in the Context of Multiple Asynchronous Input Channels}
%\subtitle{Analysis of Ergodic Processes with Multiple Stationary Behaviors using Data from Multiple Asynchronous Input Channels}
\author{Ryan Michael\\ \texttt{kerinin@gmail.com}}
%\date{???}
\maketitle

\begin{abstract}
Spectral Analysis is a common form of analysing timeseries data.  We propose a method of spectral analysis based on sets probability distributions generated from subsets of the observed data.  By decomposing the observations into a set of observed behaviors, the spectral analysis can be used more accurately to predict the timseries' behavior.

\end{abstract}

\section{Introduction}
%\tableofcontents

\subsection{Existing Work}
 %http://en.wikipedia.org/wiki/Expectation-maximization_algorithm
%http://en.wikipedia.org/wiki/Stationary_process
%http://en.wikipedia.org/wiki/Ergodicity
%http://en.wikipedia.org/wiki/Mixing_(mathematics)
%http://en.wikipedia.org/wiki/Lyapunov_exponent
%http://en.wikipedia.org/wiki/Recurrence_quantification_analysis
%http://en.wikipedia.org/wiki/Recurrence_plot
%http://en.wikipedia.org/wiki/Autoregressive_fractionally_integrated_moving_average
%http://en.wikipedia.org/wiki/Autocorrelation

\url{http://en.wikipedia.org/wiki/Linear_discriminant_analysis}


% \subsubsection{Hidden Markov Model}
% cannot account for future states - only capable of prediction
% weak, short-term memory

% \subsubsection{Box-Jenkins}
% intended for simplistic processes with well-understood stationarity and periodicity
% http://en.wikipedia.org/wiki/Box-Jenkins

% \subsubsection{Spectral Analysis}
% assumes some type of frequency-domain decomposition.  Frequency-domain signal representations do not do a very good job predicting time-domain values. 

% \subsubsection{Shrinking-\epsilon SVM Regression}
% theoretical foundation weak; only compensates for the relevance of recent data.  See Markhov problem


\subsection{General Overview}
The goal is to create a method of statistical inference capable of processing timeseries data which is both multi-variate and exhibits different behaviors at different times.  This type of data is common, and developing a robust method of analysis has applications in many domains.  The general approach is to create a series of estimates using subsets of the observed data, and to then combine these estimates in an intelligent manner which captures the relevance of each estimate to the current prediction task.  By using a set of 'typical' estimates, we are able to reduce the computational demands of the system, as each estimate is a condensed representation of the data from which it was derived.  This approach also allows us to reduce data redundancy by only using distinct estimates.

The most basic operation used in this system is the estimation of probability densities.  Based on a set of observations drawn from some random process, we generate an estimate of the underlying probability distribution.  This estimate tells us the probability of each point in the input space being observed.  Areas of the input space in which a dense set of observations are observed are given high probability, while areas of the input space with few observations are given low probability.  This basic operation, in combination with some basic laws of statistics allow us to build the full inference system.

We assume that observations are pulled from multiple independent sources, all of which respond to some underlying phenomena.  For instance one set of observations could be from a microphone and another from a light detector.  We do not know how the inputs are related or to what extent their behavior changes over time.  For example one input could be a steady sine wave and another could be based on the decay of a radioactive isotope.  In the former case previous observations of the source are useful, in the latter they're not.  Alternately two inputs could be light detectors in the same room or they could be on different continents; in the former their input would be highly similar, in the latter not as much.

Our general strategy has three phases; generating estimates, correlating estimates, and applying estimates to a given prediction task.

\subsection{Generating Estimates}
Estimates are generated by picking a time window at random and only dealing with observations which take place in that window.  Using these observations we create an estimate of the probability distribution underlying the observations (this distribution would include time as a variable).  This process is repeated until we feel we have a reasonable sample of the system as a whole, at which time we can start to correlate the estimates.

\subsection{Correlating Estimates}
The goal of correlating estimates is to determine the relationships between estimates at different times and from different sources.  Estimates are correlated by treating them as variables whose value for a given time window is determined by the extent to which the observed data corresponds to the estimate.  For each time window there exist a set of estimates which have some value describing their accuracy at predicting the observed value.  We can treat the accuracy measurement of each estimate as a multi-dimensional point, each dimension determined by an estimate's accuracy.  Using a set of these points taken at different times, we create a probability distribution estimate.  The domain of this probability distribution has the same dimensionality as the number of estimates we have generated.  This probability density allows us to predict the probability of an estimate in of one source based on the probability of an estimate in of another source because frequent combinations of accuracy values will have higher probability than other combinations.  This 'correlation density' also tells us which estimates are most commonly observed - information we can use in conjunction with estimate similarity to determine which estimates to use and which to discard.

\subsection{Making Pedictions}
Once the estimates have been correlated, we are able to generate predictions.  For simplicity, we'll assume that the first two phases occur on as set of 'training' data which is representative of the underlying data, while prediction takes place continuously using new sets of observations which occur in some time window.  We make predictions for a given source by combining the existing estimates we have for that source based on their accuracy at predicting the given observations.  Because we have determined the correlation between estimate accuracy of different sources, we can use other sources to refine our confidence in each estimate of the given source; the influence of estimates of the given source which do not correspond to a likely 'point' in the correlation density are suppressed while the influence of estimates which correspond to likely points in the correlation density are enhanced.

\section{Formal Description}

% This gets more complex the more I think about it.  First, we can describe the entropy of a set as the information contained in that set.  We want to select for sets which have high information content.  Second, we can describe the variation between sets as a distance using the KL divergence.  Parzen window defines a point's probability as a sum of the kernelized distance to all known points.  In other words, the cumulative KL distance between a subset and the others constitutes the Parzen estimate of it's probability, using a linear kernel.  In this sense, minimizing the KL matrix of the SV's will maximize the probability of similar sets being observed.  Conversely, maximizing the KL matrix of the SV's will increase the net entropy of the SV set.  The trade-off between these two outcomes is a parameter which must be determined.

% Another way to think about this is that we want to pick SV's such that each observed set lies as close as possible to an SV, which is to say we locate SV's such that they give us a good a possible a cross-section of the data.  In this case, we don't need to trade divergence for probability, we only need to decide how accurate we want to be.  In this version, we assume that we want a small set of highly similar SV's to influence the estimate of a point, rather than assuming that SV's are essentially wavelets to be combined until they approach the solution.  Given the nature of the reconstruction algorithm, I think this is a better way of thinking about this.  

% We want to minimize the distance from each observed point to an SV, which probably means minimizing the enclosing hypershere in feature space.  This also gives us a way to think of 'outliers' - they're simply points which are unlike all the others, and thus cannot be easily represented by similarity to their neighbors.  These points are ignored because we do not have enough information to accurately model their behavior, although it might be interesting to select both BSV's and SV's for estimation.

% One thing to consider is the fact that we're using KL divergence to select estimates, then entropy to combine them.  It might be better to use the same measure of correlation in both cases, and I suspect the KL divergence could easily be used in place of the entropy.

\subsection{Problem Setting}
We begin with a hidden random variable 

\[ X = (\Omega,\mathcal{F},\mathcal{P}) \]

Our knowledge of \( X \) comes from a set of independant sources which we treat as random processes generated by \( X \):

\[ X^n : \Omega \mapsto \Omega^n \in \mathbb{R}^d \times \mathbb{R}_+  \]
\[ X^n = (\Omega^n,\mathcal{F}^n,\mathcal{P}^n) \]
\[ \mathbf{X} = [X^0,...,X^c]   \]

We refer to each of these sources as a channel, and refer to each channel as the \( i^\text{th} \) element of the set \( \mathbf{X} \):

\[ C^n = [\Omega^n,\mathcal{F}^n,\mathcal{P}^n] \]

For each channel we are given a set of \( \ell \) observations of dimension \( d \):

\[ \mathbf{x}_i^n = (x_i^n, t_i^n ) \]
\[ X^n = \left[ \mathbf{x}_0^n,\hdots,\mathbf{x}_\ell^n \right] \in \mathbb{R}^d \times \mathbb{R}_+ \]

Given a set of time durations \( \theta \in \Theta \), we define a set of time widows:

\[  \mathcal{T}^n =  
\begin{bmatrix} 
(t_0^n,\theta_0) & \hdots & (t_0^n,\theta_z) \\
\vdots & \ddots & \vdots \\
(t_\ell^n, \theta_0) & \hdots & (t_\ell^n, \theta_z) \\
\end{bmatrix} 
\]

We assume that \( \mathcal{P} \) can be approximated in a given time window using a scaled and shifted set of weighted distributions defined over some set of time intervals.

\begin{equation} \mathcal{P}(t,\theta) = \sum_i \delta_i \cdot f_i(t - t_i) \frac{\theta}{\theta_i} \end{equation}

where \( \delta_n \) is the weight corresponding to \( f_n \).  Finally, we assume that a similar mixture of distributions can be determined for each channel:

\begin{equation} \mathcal{P}^n(t,\theta) = \sum_i \delta_i^n \cdot f_i^n(t - t_i) \frac{\theta}{\theta_i} \end{equation}

\subsection{Single Channel Setting}
We begin by considering the case where only one channel exists, so for now we will omit the superscript and refer to \(X^n = (\Omega^n,\mathcal{F}^n,\mathcal{P}^n) \) as \(X = (\Omega, \mathcal{F},\mathcal{P}) \).  Given a set of test data \( \hat{X} \), our goal is to estimate the probability distribution of \( X \) over some time window \( \mathcal{T}_{\hat{x}} \):

\begin{align*}
\mathbf{\hat{x}}_i &= (\hat{x}_i,t_i) \\
\hat{X} &= [ \mathbf{\hat{x}}_0,\hdots,\mathbf{\hat{x}}_k ] \\
\mathcal{T}_{\hat{x}} &= (t_{\hat{x}}, \theta_{\hat{x}} ), \quad t_{\hat{x}} < \min_t \hat{X} < \max_t \hat{X} < t_{\hat{x}} + \theta_{\hat{x}}
\end{align*}

We begin by defining the subset of the training observations which fall into each time window, scaled and shifted to the interval \( t \in [0,1) \):

\begin{equation} \label{eq:Ssingle} S_{t,\theta} = \left[ \left( x_i,\frac{t_i - t}{\theta} \right) \Big| \quad x_i \in X, \ t \le t_i < t+\theta \right] \end{equation}
\[ \mathcal{S} = 
\begin{bmatrix} 
S_{t_0,\theta_0} & \hdots & S_{t_0,\theta_z} \\
\vdots & \ddots & \vdots \\
S_{t_\ell, \theta_0} & \hdots & S_{t_\ell, \theta_z} \\
\end{bmatrix}  
\]

We treat \( \mathcal{S} \) as a random process:

\[ \mathcal{S} = [ \Omega^\mathcal{S}, \mathcal{F}^\mathcal{S},\mathcal{P}^\mathcal{S}] \]

 and observe that \( \hat{X} \) can be treated as an observation of \( \mathcal{S} \):

\begin{equation} \label{eq:SySingle} 
S_{\hat{x}} =  \left[ \left( x_i,\frac{t_i - t_{\hat{x}}}{\theta_{\hat{x}}} \right) \Big| \quad (x_i,t_i) \in \hat{X} \right] 
\end{equation}

We can now frame the task of estimating the probability distribution of \( \hat{X} \) as a task of estimating a probability density function \( \varphi \) for the random process \( \mathcal{S} \): 

\begin{equation} \label{eq:PrS}
\Pr( X = \mathbf{x} \ | \ \hat{X} ) \mapsto \ \Pr( \mathcal{S} = \{ S_{\hat{x}} \cup \mathbf{x} \} ) \ \simeq \ \varphi( \mathbf{x}, S_{\hat{x}} )
\end{equation}


\subsection{Multiple Channel Setting}

In order to extend this result to settings in which multiple channels exist, we return to \ref{eq:Ssingle} and extend the scope to multiple channels:

\begin{equation} \label{eq:Smultiple} S_{t,\theta} = \left[ \left( x_i^n,\frac{t_i - t}{\theta} \right) \Big| \quad x_i^n \in \mathbf{X}, \ t \le t_i < t+\theta \right] \end{equation}

In this case, we treat each channel as an orthonormal basis of the abstract space \( \Omega^\mathcal{S} \).  Equation \ref{eq:SySingle} is likewise extended in the same manner:

\begin{equation} \label{eq:SyMultiple} 
S_{\hat{x}} =  \left[ \left( x_i^n,\frac{t_i - t_{\hat{x}}}{\theta_{\hat{x}}} \right) \Big| \quad (x_i^n,t_i) \in \mathbf{\hat{X}} \right] 
\end{equation}

\section{ Parzen Window Estimation}

One method of evaluating \ref{eq:PrS} is by using the Parzen Window method.  We choose the Parzen Window method because it allows us to estimate probabilities of unordered sets, provided they have an addition operation and a kernel function exists to provide a distance metric.  

\subsection{ Single Channel Parzen Window}

We will again begin by considering the single-channel case, then extend the resulting equations as necessary.  The Parzen Window method requires the definition of a metric over the abstract space \( \Omega^\mathcal{S} \).  Such a metric can be defined using the symmetric Kullbeck Liebler divergence with probability measures \( \phi_n(\mathbf{x}) \simeq Pr(X = \mathbf{x} \ | \ S_n) \):

\begin{align} \label{eq:KLsingle}
D_{KL}(S_n\|S_m) &= \sum_{\mathbf{x} \in \{ S_n \cup S_m \} } \phi_n( \mathbf{x} ) \log \frac{ \phi_n(\mathbf{x}) }{ \phi_m( \mathbf{x} ) } \nonumber \\
&= -\sum_{\mathbf{x} \in \{ S_n \cup S_m \} } \phi_n(\mathbf{x}) \log \phi_m(\mathbf{x}) + \sum_{\mathbf{x} \in \{ S_n \cup S_m \} } \phi_n(\mathbf{x}) \log \phi_n(\mathbf{x}) \\
\|S_n - S_m\|_{KL} &= D_{KL}(S_n\|S_m) + D_{KL}(S_m\|S_n)
\end{align}
% Need to discuss why the union of n&m is sufficient to evaluate the KL divergence

The Parzen Window estimation of \( \mathcal{P}^\mathcal{S} \) is defined as:

\begin{equation} \label{eq:PrSParzenSingle}
\Pr( \mathcal{S} = S ) \simeq \sum_{i \in \mathcal{S}}  \frac{1}{|\mathcal{S}| } K_\gamma( S, S_i ) 
\end{equation}

where \( | \cdot | \) denotes the cardinality of \( ( \cdot ) \) and \( K \) is some kernel function, for instance the Radial Basis Function:

\begin{equation} \label{eq:KRBFparzen}
K_\gamma( S_i, S_j ) =  \frac{1}{\gamma \sqrt{2\pi} } e^{-\frac{1}{\gamma} \|S_i - S_j \|_{KL}^2}
\end{equation}

The same method can be used to estimate \( \phi_n(\mathbf{x}) \) for a given subset \( S_n \):

\begin{equation} \label{eq:PhiSingle}
\phi_n( \mathbf{x} ) = \sum_{\mathbf{x}_i \in S_n} \frac{1}{|S_n|}  K_\gamma (\mathbf{x}, \mathbf{x}_i ) 
\end{equation}

Substituting \ref{eq:PrSParzenSingle} into \ref{eq:PrS} our probability distribution estimate becomes:

\begin{equation} \label{eq:VarphiParzenSingle}
\varphi_P(\mathbf{x}, S) = \sum_{i \in \mathcal{S}} \frac{1}{|\mathcal{S}|} K_\gamma ( S_{\mathbf{x}}, S_i )
\end{equation}
\[ S_{\mathbf{x}} = \{ S \cup \mathbf{x} \}\]


\subsection{ Multiple Channel Parzen Window}

Extending the Parzen Window approach requires the realization that \ref{eq:KLsingle} requires that \( S_n \) and \( S_m \) both be defined over the same abstract space \footnote{ For instance if \( X_n \in \mathbb{R}^2 \) and \( X_m \in \mathbb{R}^3 \), it is impossible to calculate \( \phi_n\big( (x_m,t) \big) \) because the quantituy \( \| x_m - x_n \|^2 \) from using \ref{eq:PhiSingle} is ambiguous.}.  As mentioned earlier, in the Multiple Channel context each channel is treated as an orthonormal basis of \( \Omega^\mathcal{S} \).  An obvious approach to defining a measure over \( \Omega^\mathcal{S} \) for mutliple channels is to use the Euclidean norm of the Kullbeck Liebler divergence of each channel considered independently:

\[ S_n^c = [ \mathbf{x} \ | \ \mathbf{x} \in \{ S_n \cap X^c \} ] \]

This requires the following minor extension of \ref{eq:KRBFparzen}:

\begin{equation} \label{eq:KRBFParzenMultiple}
K_\gamma( S_i, S_j ) =  \prod_c \frac{1}{\gamma \sqrt{2\pi} } e^{-\frac{1}{\gamma} \|S_i^c - S_j^c \|_{KL}^2 }
\end{equation}


\section{ Support Vector Estimation}

The Parzen Window method is neither sparse nor computationally efficient, and as the number of observations grows, these deficiencies quickly become prohibitive.  We now investigate the use of Support Vector Machines to generate \( \varphi(\mathbf{x}, S) \).

\subsection{ Random Process Estimation }
Support Vector Machines are usually used to estimate probability distributions by solving the related problem of estimating the cumulative distribution function of the random variable in question.  This reduces the problem to one of estimating a non-linear mapping from observations to cumulative distribution values, which can be formulated as an optimization problem over a linear operator equation.  Unfortunately, these methods depend on the ability to calculate an empirical distribution for each observation:

\begin{equation} F_\ell(x) = \frac{1}{\ell} \sum_i \theta(x-x_i) \end{equation}

where \( \theta(x) \) is the indicator function.  To evaluate this function, the abstract space \( \Omega^\mathcal{S} \) must be ordered.  While we have described a distance metric over \( \Omega^\mathcal{S} \), it is not clear what a meaningful ordering relation would be.

Rather than calculating the cumulative probability distribution of \( \Omega^\mathcal{S} \), we begin with the assumption that the Parzen Window estimate of the probability distribution is accurate and attempt to minimize the square loss between the Support Vector estimate and the Parzen Window estimate.  Because we are hoping to generate a sparse representation of the probability distribution, we add a regularizing term \( \Omega \) which penalizes similar \( S \).  The Support Vector approach seeks a solution in the following form:

\begin{align} \label{eq:SVResultSingle}
\varphi_{SV}( \mathbf{x}, S : \beta) &= \sum_{i} \beta_i K_\gamma( S_\mathbf{x}, S_i ) \\
S_\mathbf{x} &= \{ S \cup \mathbf{x} \} \nonumber
\end{align}

So we can express the Support Vector optimization problem as:

\begin{align*}
W(\beta) &= \sum_{\mathbf{x} \in X } \Big( \varphi_{P}(\mathbf{x}, S) - \varphi_{SV}(\mathbf{x}, S : \beta ) \Big)^2 + \lambda \Omega(\beta,S) \\
&= \sum_{ \mathbf{x} \in X } \Big( \varphi_{SV}(\mathbf{x},S : \beta)^2 - 2 \varphi_{SV}(\mathbf{x},S : \beta) \varphi_P(\mathbf{x},S)  \Big)  + \lambda \Omega( \beta, S ) \\
&= \sum_{ \mathbf{x} \in X } \left( \Big( \sum_i \beta_i K_\gamma( S_{\mathbf{x}}, S_i ) \Big) \cdot \Big( \sum_j \beta_j K_\gamma( S_{\mathbf{x}}, S_j ) \Big) - 2 \Big( \sum_i \beta_i K_\gamma( S_{\mathbf{x}}, S_i ) \Big) \cdot \Big( \sum_j \frac{1}{|\mathcal{S}|} K_\gamma( S_{\mathbf{x}}, S_j ) \Big)   \right) + \lambda \Omega( \beta,S ) \\
&= \sum_{i,j} \beta_i \beta_j \sum_{\mathbf{x} \in X} K_\gamma(S_{\mathbf{x}},S_i) K_\gamma(S_{\mathbf{x}},S_j) + \sum_i \beta_i \sum_{\mathbf{x} \in X } \left( \lambda K_\gamma(S_{\mathbf{x}},S_i)^{-1} - \frac{2}{|\mathcal{S}|} \sum_j K_\gamma(S_{\mathbf{x}},S_i) K_\gamma(S_{\mathbf{x}},S_j) \right)
\end{align*}

\begin{equation} \label{eq:SVProcessSingle} \text{subject to} \quad \sum_i \beta_i = 1, \quad \beta_i \ge 0, \ i=1,\hdots,|\mathcal{S}|
\end{equation}

Notice the regularizer selected is defined as:

\begin{equation}
\Omega(\beta,S) = \sum_i \beta_i \sum_{\mathbf{x} \in X} K_\gamma(S_{\mathbf{x}},S_i)^{-1}
\end{equation}

Equation \ref{eq:SVProcessSingle} is a quadratic optimiztion problem defined as:

\begin{align*}
W(\beta) &= \frac{1}{2} \beta^T P \beta + q^T \beta \\
P_{i,j} &= \sum_{\mathbf{x} \in X } K_\gamma(S_{\mathbf{x}},S_i) K_\gamma(S_{\mathbf{x}},S_j) \\
q_i &= \sum_{\mathbf{x} \in X } \left( \lambda K_\gamma(S_{\mathbf{x}},S_i)^{-1} - \frac{1}{|\mathcal{S}|} \sum_j K_\gamma(S_{\mathbf{x}},S_i) K_\gamma(S_{\mathbf{x}},S_j) \right) \\
\end{align*}
\begin{align}
P &= \langle \mathbf{K}^T \cdot \mathbf{K} \rangle  \label{eq:SVPSingle} \\
q &= \lambda \langle \mathbf{K}^T \cdot \mathbf{1}_{( |K|, 1 )} \rangle^{-1} - \frac{1}{ | \mathcal{S} | } \langle \mathbf{K}^T \cdot \mathbf{K} \cdot \mathbf{1}_{( |K|, 1 )} \rangle
\end{align}

\subsection{  Random Process Kernel Definition }

When developing the Parzen Window algorithm, we used kernel function \ref{eq:KLsingle} which is based on the Kullbeck Liebler divergence of a probability estimate \ref{eq:PhiSingle} defined at the sets being evaluated.  Our motivation in developing a Support Vector approach is to generate sparse representations of \( \mathcal{P}^{\mathcal{S}} \), in part to reduce the computational demands of evaluating \( \varphi( \mathbf{x}, S) \) for test data sets.  Unfortunately, kernel function \ref{eq:KLsingle} requires probability estimates of both sets being compared - it would be helpful to develop a kernel function capable of evaluating a distance between a test set \( S_{\hat{X}} \) and a training set \( S_i \) without first calculating an estimate of the probability distribution of \( S_{\hat{X}} \).

Note that \ref{eq:SVProcessSingle} is formulated in such a way that the first kernel argument is always \( S_{\mathbf{x}} \), which allows us to define a kernel which takes a set of points as its first argument and a probability distribution as its second.  The definition of \ref{eq:SVPSingle} also allows us to define non-symmetric kernels without sacrificing convexity or monotonicity in the optimization objective function.  We can therefore use any measure of the divergence between a set of points and a probability distribution.  We consider the Renyi entropy with \( \alpha = 1 \) and select the Shannon Entropy as our kernel metric, as it is equivalent the the Kullback Lieblier divergence.

\begin{equation}
\| X - \phi \|_{H} = \sum_{x \in X} \phi(x) \log \phi(x)
\end{equation}

Recalling that \( S_\mathbf{x} = \{ S \cup \mathbf{x} \} \), we observe that:

\begin{equation}
\| S_{\mathbf{x}} - S_i \|_{H} = \| S - S_i \|_{H} + \| \mathbf{x} - S_i \|_{H}
\end{equation}

We restate the kernel function as:

\begin{equation}
K_\gamma( X, S_i ) =  \frac{1}{\gamma \sqrt{2\pi} } e^{-\frac{1}{\gamma} \|X - S_i \|_{H}^2}
\end{equation}

In situations where probabilities are being calculated for a uniform set of points in \( \Omega^{\mathcal{S}} \), this can considerably reduce the computational time needed to evaluate \( \mathbf{K} \), as \( \|\mathbf{x} - S_i \|_H \) can be computed once and then added to each \( \|S - S_i\|_H, S \in \mathcal{S} \).


\subsection{ Random Vector Estimation }
 
Evaluating \ref{eq:SVProcessSingle} over a training set \( X \) produces a set \( \mathcal{S}_{SV} \in \mathcal{S} \) referred to as Support Vectors.  While \ref{eq:SVProcessSingle} produces a sparse representation of \( \mathcal{P}^\mathcal{S} \) by eliminating some \( S \), further sparseness can be achieved by refining the definition of \( \phi(\mathbf{x}) \) to allow the elimination of some \( \mathbf{x} \) in each \( S \in \mathcal{S}_{SV} \).  In the context of computing the kernel matrices used in optimizing \ref{eq:SVProcessSingle} the Parzen Window definition of \( \phi(\mathbf{x}) \) is computationally acceptable, as it results in a good approximation with a minimal amount of computation \footnote{ The Parzen Window estimate at a point requires distance computations for each observation and a summation.  By contrast, the SV estimate requires the same number of distance computations as well as the solving of a quadratic optimization problem whose complexity increases exponentially with the number of observations. }.  In the context of evaluating \ref{eq:SVResultSingle} over a testing set \( \hat{X} \), the Parzen Window algorithm for \( \phi(\mathbf{x}) \) is sub-optimal due to the fact that it requires the full set of observations for each \( S \in \mathcal{S}_{SV} \).  In this context, a sparse algorithm for \( \phi(\mathbf{x}) \) would reduce both the data required to store \( \mathcal{S}_{SV} \) and the computational resources required to evaluate \ref{eq:DKLemp}.

We return now to the empirical cumulative distribution function method of Support Vector density estimation and search for a solution in the following form, which is able to use multiple non-symmetric kernel functions simultaneously:

\begin{equation} \label{eq:SVDensity}
\phi(x) = \sum_{i=1}^\ell \left( \beta_i^1 \mathcal{K}_1(x_i,x) + ... + \beta_i^\kappa \mathcal{K}_\kappa( x_i,x) \right)
\end{equation}

We select values of \( \beta \) which minimize the square loss of the empirical cumulative probability distribution using some regularizer:

\begin{align}
&\min \left( \sum_{i=1}^\ell \left( y_i - \sum_{j=1}^\ell \sum_{n=1}^\kappa \beta_j^n k_n(x_i, x_j) \right)^2 + \lambda \sum_{i=1}^\ell \sum_{n=1}^\kappa \frac{1}{\gamma_n} \beta_i^n \right) \\
&\text{subject to} \quad \sum_{i=1}^\ell \sum_{n=1}^\kappa \beta_i^n = 1, \quad \beta_i \ge 0 \\
\end{align}

given a kernel function \( k(x,x') \) from the sigmoid family to approximate the cumulative probability distribution and it's derivative which we refer to as the cross-kernel \( \mathcal{K}(x,x') \) which we use to construct an estimate of the probability distribution:

\begin{align}
&k(x,x') = \frac{1}{1+e^{-\gamma(x-x')} } \\
&\mathcal{K}(x,x') = -\frac{\gamma}{2 + e^{\gamma(x-x')} + e^{-\gamma(x-x')} } 
\end{align}


\subsection{Data Pre-Processing}
% logistic function using mean and sd to put most training points between .1 and .9

% upper and lower bounds on \(\int \Delta \) as non-stationary data detections mechanism

% logistic function from delta using mean and sd in same way if data non-stationary

% iterative integration process until stationary data found ?

\section{Results}
The architecture has been tested against several data sets.  In all cases the system parameters are left unchanged to eliminate the possbility of optimizing the system performance to best match the known outcomes.

\subsection{Eunite Competition Data}


\subsection{Santa Fe Data}


\subsection{CATS Benchmark Data}


\subsection{Results Summary}


\section{Further Research}

\subsection{Data Retention}
% this can probably be worked into the other 3 sections, but for now I'll put it here


\subsection{Ensemble System}
% more analysis of recent data, some type of transfer from short to long-term 'memory'

% heirarchical system?

% what parameters change?


\subsection{Weighted Influence}
% a weight term could be added to the regularizer to control the influence of specific sets.  This could provide more information in 'important' situations, which could be useful in motivated learning.

\subsection{ Sliding Prediction Offset }
% we're currently predicting each point from a given window.  It might be better to iteratively match estimates to the full training set - this is the shadow input idea again.

% The basic idea here is that the optimization is intended to punish similar patterns - this means that the prediction algorithm is almost guaranteed to have irregular accuracy as the prediction window moves from one estimate to another (ie, the inbetween states will have lower accuracy than either termination state).

% We're already assuming that the output is a linear combination of SV's, what if we looked at this in the context of real-time processing and evaluated each SV at each point in time, but also included SV's from different points in time and allowed some type of competition between them, say restricting \sum \beta = 1 at all points in time, but allowing overlap somehow?

% Basically, the current formulation assumes we have a test set that is static - we need to re-think the system under the assumption that the test set is dynamic, and we want to reduce the computation at each set using information from previous sets (this likewise can reduce the number of SV's required, as periodic signals need only be represented once).



\section{Conclusion}
Eat it, bitches


\nocite{Moreno03}

\bibliographystyle{plain}
\bibliography{Research/research.bib}

\end{document}
