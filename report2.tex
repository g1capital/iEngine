\documentclass[10pt]{article}

\usepackage{amsmath}
\usepackage{amsfonts}

\usepackage{hyperref}
\usepackage[all]{hypcap}



\begin{document}
\title{Parzen-SVM: An Approach to Timeseries Analysis Using Support Vector Machines based on Parzen Windows}
\author{Ryan Michael\\ \texttt{kerinin@gmail.com}}
\maketitle

\begin{abstract}

\end{abstract}

\tableofcontents

\section{Introduction}

\subsection{Existing Work}
 %http://en.wikipedia.org/wiki/Expectation-maximization_algorithm
%http://en.wikipedia.org/wiki/Stationary_process
%http://en.wikipedia.org/wiki/Ergodicity
%http://en.wikipedia.org/wiki/Mixing_(mathematics)
%http://en.wikipedia.org/wiki/Lyapunov_exponent
%http://en.wikipedia.org/wiki/Recurrence_quantification_analysis
%http://en.wikipedia.org/wiki/Recurrence_plot
%http://en.wikipedia.org/wiki/Autoregressive_fractionally_integrated_moving_average
%http://en.wikipedia.org/wiki/Autocorrelation
%\url{http://en.wikipedia.org/wiki/Linear_discriminant_analysis}


\subsubsection{Hidden Markov Model}
% cannot account for future states - only capable of prediction
% weak, short-term memory

\subsubsection{Box-Jenkins}
% intended for simplistic processes with well-understood stationarity and periodicity
% http://en.wikipedia.org/wiki/Box-Jenkins

\subsubsection{Spectral Analysis}
% assumes some type of frequency-domain decomposition.  Frequency-domain signal representations do not do a very good job predicting time-domain values. 

\subsubsection{Shrinking-\(\epsilon\) SVM Regression}
% theoretical foundation weak; only compensates for the relevance of recent data.  See Markhov problem


\subsection{General Overview}
The goal is to create a method of statistical inference capable of processing timeseries data which is both multi-variate and exhibits different behaviors at different times.  This type of data is common, and developing a robust method of analysis has applications in many domains.  The general approach is to create a series of estimates using subsets of the observed data, and to then combine these estimates in an intelligent manner which captures the relevance of each estimate to the current prediction task.  By using a set of 'typical' estimates, we are able to reduce the computational demands of the system, as each estimate is a condensed representation of the data from which it was derived.  This approach also allows us to reduce data redundancy by only using distinct estimates.

The most basic operation used in this system is the estimation of probability densities.  Based on a set of observations drawn from some random process, we generate an estimate of the underlying probability distribution.  This estimate tells us the probability of each point in the input space being observed.  Areas of the input space in which a dense set of observations are observed are given high probability, while areas of the input space with few observations are given low probability.

We assume that observations are pulled from multiple independent sources, all of which respond to some underlying phenomena.  For instance one set of observations could be from a microphone and another from a light detector.  We do not know how the inputs are related or to what extent their behavior changes over time.  For example one input could be a steady sine wave and another could be based on the decay of a radioactive isotope.  In the former case previous observations of the source are useful, in the latter they're not.  Alternately two inputs could be light detectors in the same room or they could be on different continents; in the former their input would be highly similar, in the latter not as much.

\section{Problem Setting}
We begin with a hidden random variable 

\[ X = (\Omega,\mathcal{F},\mathcal{P}) \]

Our knowledge of \( X \) comes from a set of independant sources which we treat as random processes generated by \( X \):

\[ X^n : \Omega \mapsto \Omega^n \in \mathbb{R}^d \times \mathbb{R}_+  \]
\[ X^n = (\Omega^n,\mathcal{F}^n,\mathcal{P}^n) \]
\[ \mathbf{X} = [X^0,...,X^c]   \]

We refer to each of these sources as a channel, and refer to each channel as the \( i^\text{th} \) element of the set \( \mathbf{X} \):

\[ C^n = [\Omega^n,\mathcal{F}^n,\mathcal{P}^n] \]

For each channel we are given a set of \( \ell \) observations of dimension \( d \):

\[ \mathbf{x}_i^n = (x_i^n, t_i^n ) \]
\[ X^n = \left[ \mathbf{x}_0^n,\hdots,\mathbf{x}_\ell^n \right] \in \mathbb{R}^d \times \mathbb{R}_+ \]

We define a set of time values and durations:

\[  \mathcal{T} = [ t_0,\hdots,t_\ell ] \]

\[ \Theta = [ \theta_0,\hdots,\theta_z ] \]

We assume that \( \mathcal{P} \) can be approximated in a given time window using a shifted set of weighted distributions defined over some set of time intervals.

\begin{equation} \mathcal{P}(t,\theta) = \sum_i \delta_i \cdot f_i(t - t_i) \end{equation}

where \( \delta_n \) is the weight corresponding to \( f_n \).  Finally, we assume that a similar mixture of distributions can be determined for each channel:

\begin{equation} \mathcal{P}^n(t,\theta) = \sum_i \delta_i^n \cdot f_i^n(t - t_i) \end{equation}

\subsection{Single Channel Setting}
We begin by considering the case where only one channel exists, so for now we will omit the superscript and refer to \(X^n = (\Omega^n,\mathcal{F}^n,\mathcal{P}^n) \) as \(X = (\Omega, \mathcal{F},\mathcal{P}) \).  Given a set of test data \( \hat{X} \), our goal is to estimate the probability distribution of \( X \) over some time window:

\begin{align*}
\mathbf{\hat{x}}_i &= (\hat{x}_i,t_i) \\
\hat{X} &= [ \mathbf{\hat{x}}_0,\hdots,\mathbf{\hat{x}}_k ] \\
&(t_{\hat{x}}, \theta_{\hat{x}} ), \quad t_{\hat{x}} < \min_t \hat{X} < \max_t \hat{X} < t_{\hat{x}} + \theta_{\hat{x}}
\end{align*}

We begin by defining the subset of the training observations which fall into each time window, shifted to the interval \( t \in [0,\theta) \):

\begin{equation} \label{eq:Ssingle} S_{t,\theta} = \left[ \left( x_i,t_i - t \right) | \quad x_i \in X, \ t \le t_i < t+\theta \right] \end{equation}
\[ \mathcal{S} = 
\begin{bmatrix} 
S_{t_0,\theta_0} & \hdots & S_{t_0,\theta_z} \\
\vdots & \ddots & \vdots \\
S_{t_\ell, \theta_0} & \hdots & S_{t_\ell, \theta_z} \\
\end{bmatrix}  
\]

We treat \( \mathcal{S} \) as a random process:

\[ \mathcal{S} = [ \Omega^\mathcal{S}, \mathcal{F}^\mathcal{S},\mathcal{P}^\mathcal{S}] \]

 and observe that \( \hat{X} \) can be treated as an observation of \( \mathcal{S} \):

\begin{equation} \label{eq:SySingle} 
S_{\hat{x}} =  \left[ \left( x_i,t_i - t_{\hat{x}} \right) | \quad (x_i,t_i) \in \hat{X} \right] 
\end{equation}

We can now frame the task of estimating the probability distribution of \( \hat{X} \) as a task of estimating a probability density function \( \varphi \) for the random process \( \mathcal{S} \): 

\begin{equation} \label{eq:PrS}
\Pr( X = \mathbf{x} \ | \ \hat{X} ) \mapsto \ \Pr( \mathcal{S} = \{ S_{\hat{x}} \cup \mathbf{x} \} ) \ \simeq \ \varphi( \mathbf{x}, S_{\hat{x}} )
\end{equation}


\subsection{Multiple Channel Setting}

In order to extend this result to settings in which multiple channels exist, we return to \ref{eq:Ssingle} and extend the scope to multiple channels:

\begin{equation} \label{eq:Smultiple} S_{t,\theta} = \big[ \left( x_i^n,t_i - t \right) | \quad x_i^n \in \mathbf{X}, \ t \le t_i < t+\theta \big] \end{equation}

In this case, we treat each channel as an orthonormal basis of the abstract space \( \Omega^\mathcal{S} \).  Equation \ref{eq:SySingle} is likewise extended in the same manner:

\begin{equation} \label{eq:SyMultiple} 
S_{\hat{x}} =  \left[ \left( x_i^n,t_i - t_{\hat{x}} \right) | \quad (x_i^n,t_i) \in \mathbf{\hat{X}} \right] 
\end{equation}


\section{ Parzen Window Estimation}

One method of evaluating \ref{eq:PrS} is by using the Parzen Window method.  We choose the Parzen Window method because it allows us to estimate probabilities of unordered sets, provided they have an addition operation and a kernel function exists to provide a distance metric.  

\subsection{ Single Channel Parzen Window}

We will again begin by considering the single-channel case, then extend the resulting equations as necessary.  The Parzen Window method requires the definition of a metric over the abstract space \( \Omega^\mathcal{S} \).  Such a metric can be defined using the symmetric Kullbeck Liebler divergence with probability measures \( \phi_n(\mathbf{x}) \simeq Pr(X = \mathbf{x} \ | \ S_n) \):

\begin{align} \label{eq:KLsingle}
D_{KL}(S_n\|S_m) &= \sum_{\mathbf{x} \in \{ S_n \cup S_m \} } \phi_n( \mathbf{x} ) \log \frac{ \phi_n(\mathbf{x}) }{ \phi_m( \mathbf{x} ) } \nonumber \\
&= -\sum_{\mathbf{x} \in \{ S_n \cup S_m \} } \phi_n(\mathbf{x}) \log \phi_m(\mathbf{x}) + \sum_{\mathbf{x} \in \{ S_n \cup S_m \} } \phi_n(\mathbf{x}) \log \phi_n(\mathbf{x}) \\
\|S_n - S_m\|_{KL} &= D_{KL}(S_n\|S_m) + D_{KL}(S_m\|S_n)
\end{align}
% Need to discuss why the union of n&m is sufficient to evaluate the KL divergence

The Parzen Window estimation of \( \mathcal{P}^\mathcal{S} \) is defined as:

\begin{equation} \label{eq:PrSParzenSingle}
\Pr( \mathcal{S} = S ) \simeq \sum_{i \in \mathcal{S}}  \frac{1}{|\mathcal{S}| } K_\gamma( S, S_i ) 
\end{equation}

where \( | \cdot | \) denotes the cardinality of \( ( \cdot ) \) and \( K \) is some kernel function, for instance the Radial Basis Function:

\begin{equation} \label{eq:KRBFparzen}
K_\gamma( S_i, S_j ) =  \frac{1}{\gamma \sqrt{2\pi} } e^{-\frac{1}{\gamma} \|S_i - S_j \|_{KL}^2}
\end{equation}

The same method can be used to estimate \( \phi_n(\mathbf{x}) \) for a given subset \( S_n \):

\begin{equation} \label{eq:PhiSingle}
\phi_n( \mathbf{x} ) = \sum_{\mathbf{x}_i \in S_n} \frac{1}{|S_n|}  K_\gamma (\mathbf{x}, \mathbf{x}_i ) 
\end{equation}

Substituting \ref{eq:PrSParzenSingle} into \ref{eq:PrS} our probability distribution estimate becomes:

\begin{equation} \label{eq:VarphiParzenSingle}
\varphi_P(\mathbf{x}, S) = \sum_{i \in \mathcal{S}} \frac{1}{|\mathcal{S}|} K_\gamma ( S_{\mathbf{x}}, S_i )
\end{equation}
\[ S_{\mathbf{x}} = \{ S \cup \mathbf{x} \}\]


\subsection{ Multiple Channel Parzen Window}

Extending the Parzen Window approach requires the realization that \ref{eq:KLsingle} requires that \( S_n \) and \( S_m \) both be defined over the same abstract space \footnote{ For instance if \( X_n \in \mathbb{R}^2 \) and \( X_m \in \mathbb{R}^3 \), it is impossible to calculate \( \phi_n\big( (x_m,t) \big) \) because the quantituy \( \| x_m - x_n \|^2 \) from using \ref{eq:PhiSingle} is ambiguous.}.  As mentioned earlier, in the Multiple Channel context each channel is treated as an orthonormal basis of \( \Omega^\mathcal{S} \).  An obvious approach to defining a measure over \( \Omega^\mathcal{S} \) for mutliple channels is to use the Euclidean norm of the Kullbeck Liebler divergence of each channel considered independently:

\[ S_n^c = [ \mathbf{x} \ | \ \mathbf{x} \in \{ S_n \cap X^c \} ] \]

This requires the following minor extension of \ref{eq:KRBFparzen}:

\begin{equation} \label{eq:KRBFParzenMultiple}
K_\gamma( S_i, S_j ) =  \prod_c \frac{1}{\gamma \sqrt{2\pi} } e^{-\frac{1}{\gamma} \|S_i^c - S_j^c \|_{KL}^2 }
\end{equation}


\subsection{Sliding Parzen Windows}

Observe that subsets do not exist in isolation; it is possible for the time windows defined for two subsets to overlap.  This makes it possible for the estimates at at two such subsets to take advantage of this shared information.  This can be accomplished by modifying \ref{eq:PrSParzenSingle} to include not only sets offset to the beginnig of the window being predicted, but at each start point defined as well:

\[
S_{t,\theta}^\tau =  \left[ \left( x_i, 2t_i - t - \tau \right) | \quad (x_i^n,t_i) \in S_{t,\theta} \right] 
\]

\[
\mathcal{S}^{\mathcal{T}} = [ S_{t,\theta}^\tau | \quad \tau \in \mathcal{T}, \ (t,\theta) \in (\mathcal{T},\Theta) ]
\]

\begin{equation} \label{eq:PrSParzenSlidingSingle}
\Pr( \mathcal{S} = S ) \simeq \sum_{i \in \mathcal{S}}  \frac{1}{|\mathcal{S}^{\mathcal{T}}| } K_\gamma( S, S_i ) 
\end{equation}

In the context of Parzen Windows, this defintion increases computational demands and provides little increase in predictive power, however we shall see that it provides an important starting point for improving system performance.


\section{ Support Vector Estimation}

The Parzen Window method is neither sparse nor computationally efficient, and as the number of observations grows, these deficiencies quickly become prohibitive.  We now investigate the use of Support Vector Machines to generate \( \varphi(\mathbf{x}, S) \).

\subsection{ Random Process Estimation }
Support Vector Machines are usually used to estimate probability distributions by solving the related problem of estimating the cumulative distribution function of the random variable in question.  This reduces the problem to one of estimating a non-linear mapping from observations to cumulative distribution values, which can be formulated as an optimization problem over a linear operator equation.  Unfortunately, these methods depend on the ability to calculate an empirical distribution for each observation:

\begin{equation} F_\ell(x) = \frac{1}{\ell} \sum_i \theta(x-x_i) \end{equation}

where \( \theta(x) \) is the indicator function.  To evaluate this function, the abstract space \( \Omega^\mathcal{S} \) must be ordered.  While we have described a distance metric over \( \Omega^\mathcal{S} \), it is not clear what a meaningful ordering relation would be.

Rather than calculating the cumulative probability distribution of \( \Omega^\mathcal{S} \), we begin with the assumption that the Parzen Window estimate of the probability distribution is accurate and attempt to minimize the square loss between the Support Vector estimate and the Parzen Window estimate.  Because we are hoping to generate a sparse representation of the probability distribution, we add a regularizing term \( \Omega \) which penalizes similar \( S \).  The Support Vector approach seeks a solution in the following form:

\begin{align} \label{eq:SVResultSingle}
\varphi_{SV}( \mathbf{x}, S : \beta) &= \sum_{i \in \mathcal{S}} \beta_i K_\gamma( S_\mathbf{x}, S_i ) \\
S_\mathbf{x} &= \{ S \cup \mathbf{x} \} \nonumber
\end{align}

So we can express the Support Vector optimization problem as:

\begin{align*}
W(\beta) &= \sum_{\mathbf{x} \in X } \Big( \varphi_{P}(\mathbf{x}, S) - \varphi_{SV}(\mathbf{x}, S : \beta ) \Big)^2 + \lambda \Omega(\beta,S) \\
&= \sum_{ \mathbf{x} \in X } \Big( \varphi_{SV}(\mathbf{x},S : \beta)^2 - 2 \varphi_{SV}(\mathbf{x},S : \beta) \varphi_P(\mathbf{x},S)  \Big)  + \lambda \Omega( \beta, S ) \\
&= \sum_{ \mathbf{x} \in X } \left( \Big( \sum_{i \in \mathcal{S}} \beta_i K_\gamma( S_{\mathbf{x}}, S_i ) \Big) \cdot \Big( \sum_{j \in \mathcal{S}} \beta_j K_\gamma( S_{\mathbf{x}}, S_j ) \Big) \\
& \qquad \qquad \qquad \qquad - 2 \Big( \sum_{i \in \mathcal{S}} \beta_i K_\gamma( S_{\mathbf{x}}, S_i ) \Big) \cdot \Big( \sum_{j \in \mathcal{S}} \frac{1}{|\mathcal{S}|} K_\gamma( S_{\mathbf{x}}, S_j ) \Big)   \right) + \lambda \Omega( \beta,S ) \\
&= \sum_{i,j \in \mathcal{S}} \beta_i \beta_j \sum_{\mathbf{x} \in X} K_\gamma(S_{\mathbf{x}},S_i) K_\gamma(S_{\mathbf{x}},S_j) \\
& \qquad \qquad \qquad \qquad + \sum_{i \in \mathcal{S}} \beta_i \sum_{\mathbf{x} \in X } \left( \lambda K_\gamma(S_{\mathbf{x}},S_i)^{-1} - \frac{2}{|\mathcal{S}|} \sum_{j \in \mathcal{S}} K_\gamma(S_{\mathbf{x}},S_i) K_\gamma(S_{\mathbf{x}},S_j) \right)
\end{align*}

\begin{equation} \label{eq:SVProcessSingle} \text{subject to} \quad \sum_i \beta_i = 1, \quad \beta_i \ge 0, \ i=1,\hdots,|\mathcal{S}|
\end{equation}

Notice the regularizer selected is defined as:

\begin{equation}
\Omega(\beta,S) = \sum_{i \in \mathcal{S}} \beta_i \sum_{\mathbf{x} \in X} K_\gamma(S_{\mathbf{x}},S_i)^{-1}
\end{equation}

Equation \ref{eq:SVProcessSingle} is a quadratic optimiztion problem defined as:

\begin{align*}
W(\beta) &= \frac{1}{2} \beta^T P \beta + q^T \beta \\
P_{i,j} &= \sum_{\mathbf{x} \in X } K_\gamma(S_{\mathbf{x}},S_i) K_\gamma(S_{\mathbf{x}},S_j) \\
q_i &= \sum_{\mathbf{x} \in X } \left( \lambda K_\gamma(S_{\mathbf{x}},S_i)^{-1} - \frac{1}{|\mathcal{S}|} \sum_{j \in \mathcal{S}} K_\gamma(S_{\mathbf{x}},S_i) K_\gamma(S_{\mathbf{x}},S_j) \right) \\
\end{align*}
\begin{align}
P &= \langle \mathbf{K}^T \cdot \mathbf{K} \rangle  \label{eq:SVPSingle} \\
q &= \lambda \langle \mathbf{K}^T \cdot \mathbf{1}_{( |K|, 1 )} \rangle^{-1} - \frac{1}{ | \mathcal{S} | } \langle \mathbf{K}^T \cdot \mathbf{K} \cdot \mathbf{1}_{( |K|, 1 )} \rangle
\end{align}


\subsection{ Sliding Windows }

Modifying the SV algorithm to use sliding time windows allows the system to take advantage of periodicity in the observed data and reduces redundancy.  Adapting the earlier Parzen Window approach, we search for a solution in the following form:

\[
\mathcal{S}^{\mathcal{T}} = [ S_{t,\theta}^\tau | \quad \tau \in \mathcal{T}, \ (t,\theta) \in (\mathcal{T},\Theta) ]
\]

\begin{align} \label{eq:SVResultSlidingSingle}
\varphi_{SV}( \mathbf{x}, S : \beta) &= \sum_{i \in \mathcal{S}^\mathcal{T}} \beta_i K_\gamma( S_\mathbf{x}, S_i ) 
\end{align}

The optimization problem must be modified as follows:

\begin{align*}
W(\beta) &= \sum_{ \mathbf{x} \in X } \Big( \varphi_{SV}(\mathbf{x},S : \beta)^2 - 2 \varphi_{SV}(\mathbf{x},S : \beta) \varphi_P(\mathbf{x},S)  \Big)  + \lambda \Omega( \beta, S ) \\
&= \sum_{ \mathbf{x} \in X } \left( \sum_{i \in \mathcal{S}^{\mathcal{T}}} \beta_i K_\gamma( S_{\mathbf{x}}, S_i ) \right) \cdot \left( \sum_{j \in \mathcal{S}^{\mathcal{T}}} \beta_j K_\gamma( S_{\mathbf{x}}, S_j ) \right) \\
& \qquad \qquad \qquad \qquad - 2 \left( \sum_{i \in \mathcal{S}^{\mathcal{T}}} \beta_i K_\gamma( S_{\mathbf{x}}, S_i ) \right) \cdot \left( \sum_{j \in \mathcal{S}^{\mathcal{T}}} \frac{1}{|\mathcal{S}^{\mathcal{T}}|} K_\gamma( S_{\mathbf{x}}, S_j ) \right)  + \lambda \Omega( \beta,S ) \\
&= \sum_{i,j \in \mathcal{S}^{\mathcal{T}}} \beta_i \beta_j \sum_{\mathbf{x} \in X} K_\gamma(S_{\mathbf{x}},S_i) K_\gamma(S_{\mathbf{x}},S_j) \\
& \qquad \qquad \qquad \qquad + \sum_{i \in \mathcal{S}^{\mathcal{T}}} \beta_i \sum_{\mathbf{x} \in X } \left( \lambda K_\gamma(S_{\mathbf{x}},S_i)^{-1} - \frac{2}{|\mathcal{S}^{\mathcal{T}}|} \sum_{j \in \mathcal{S}^{\mathcal{T}}} K_\gamma(S_{\mathbf{x}},S_i) K_\gamma(S_{\mathbf{x}},S_j) \right)
\end{align*}

\begin{equation} \text{subject to} \quad \sum_i \beta_i = 1, \quad \beta_i \ge 0, \ i=1,\hdots,|\mathcal{S}^{\mathcal{T}}|
\end{equation}

This means that using sliding windows increases \( P \) by a factor of \( \ell^2 \) and \( q \) by a factor or \( \ell \).  Fortunately, most of the added kernel values will be \(0\) if we restrict \(\theta_\text{max}\) to some reasonable subset of the observed duration.


\subsection{ Parzen-SVM Decomposition }

Due to the simplicity of the constraints in the optimization problem, it is possible to use the decomposition method of Osuna to reduce the memory requirements of the Parzen-SVM algorithm.

\subsubsection{ Sub-Problem Definition }
The decomposition algorithm breaks \(X\) into two working sets \(B,N\), and attempts to optimize \(B\) while keeping \(N\) fixed.  This results in the following iterative optimization problem where \(\boldsymbol{\beta}^k\) denotes the result of the previous iteration:

\begin{align*}
W(\boldsymbol{\beta}_B) &= \frac{1}{2} 
\begin{bmatrix} \boldsymbol{\beta}_B^T & ( \boldsymbol{\beta}_N^k )^T \end{bmatrix} 
\begin{bmatrix} P_{BB} & P_{BN} \\ P_{NB} & P_{NN} \end{bmatrix} 
\begin{bmatrix} \boldsymbol{\beta}_B \\ \boldsymbol{\beta}_N^k \end{bmatrix} - 
\begin{bmatrix} q_B^T & q_N^T \end{bmatrix} 
\begin{bmatrix} \boldsymbol{\beta}_B \\ \boldsymbol{\beta}_N^k \end{bmatrix} \\
&= \frac{1}{2} \boldsymbol{\beta}_B^T P_{BB} \boldsymbol{\beta}_B - ( -q_B + P_{BN} \boldsymbol{\beta}_N^k )^T \boldsymbol{\beta}_B 
\\
&= \frac{1}{2} 
\begin{bmatrix} \beta_i & \beta_j \end{bmatrix}
\begin{bmatrix} P_{ii} & P_{ij} \\ P_{ij} & P_{jj} \end{bmatrix}
\begin{bmatrix} \beta_i \\ \beta_j \end{bmatrix}
- ( -q_B + P_{BN} \boldsymbol{\beta}_N^k )^T
\begin{bmatrix} \beta_i \\ \beta_j \end{bmatrix}
\end{align*}

\begin{equation}
\text{subject to} \quad 0 \le \beta_i, \beta_j, \quad \beta_i + \beta_j = 1 - \mathbf{1}^T \boldsymbol{\beta}_N^k
\end{equation}

\subsubsection{ Working Set Selection }

Select

\begin{align}
& i \in \text{arg} \max_t \left\{ -\nabla f( \boldsymbol{\beta}^k )_t \ | \quad t \in I( \boldsymbol{\beta}^k ) \right\} \\
& j \in \text{arg} \min_t \left\{ -\frac{b_{it}^2}{a_{it}} \ | \quad t \in I( \boldsymbol{\beta}^k ), \quad -\nabla f(\boldsymbol{\beta}^k)_t < -\nabla f(\boldsymbol{\beta}^k)_i \right\}
\end{align}

Where
% This needs to be checked for the new optimization scenario

\begin{align}
I( \boldsymbol{\beta} ) & \equiv \{ t \ | \quad \beta_t < 1 \quad \text{or} \quad \beta_t > 0 \} \\
& a_{it} = P_{ii} + P_{tt} - 2P_{it} \\
& \bar{a}_{it} = \begin{cases}
a_{it} & \text{if} \ a_{it} > 0 \\
\delta & \text{otherwise} \\
\end{cases} \\
& b_{it} = -\nabla f(\boldsymbol{\beta}^k)_i + \nabla f(\boldsymbol{\beta}^k)_t \\
\nabla f( \boldsymbol{\beta} )_i & \equiv P_i \boldsymbol{\beta} - q_i
\end{align}


\subsubsection{ Stopping Condition }

\begin{equation}
\max_{i \in I(\boldsymbol{\alpha}^k)} -\nabla f( \boldsymbol{\alpha} )_i + \min_{j \in I(\boldsymbol{\alpha}^k)} \nabla f( \boldsymbol{\alpha} )_j \le \epsilon
\end{equation}

\subsection{  Random Process Kernel Definition }

When developing the Parzen Window algorithm, we used kernel function \ref{eq:KLsingle} which is based on the Kullbeck Liebler divergence of a probability estimate \ref{eq:PhiSingle} defined at the sets being evaluated.  Our motivation in developing a Support Vector approach is to generate sparse representations of \( \mathcal{P}^{\mathcal{S}} \), in part to reduce the computational demands of evaluating \( \varphi( \mathbf{x}, S) \) for test data sets.  Unfortunately, kernel function \ref{eq:KLsingle} requires probability estimates of both sets being compared - it would be helpful to develop a kernel function capable of evaluating a distance between a test set \( S_{\hat{X}} \) and a training set \( S_i \) without first calculating an estimate of the probability distribution of \( S_{\hat{X}} \).

Note that \ref{eq:SVProcessSingle} is formulated in such a way that the first kernel argument is always \( S_{\mathbf{x}} \), which allows us to define a kernel which takes a set of points as its first argument and a probability distribution as its second.  The definition of \ref{eq:SVPSingle} also allows us to define non-symmetric kernels without sacrificing convexity or monotonicity in the optimization objective function.  We can therefore use any measure of the divergence between a set of points and a probability distribution.  We consider the Renyi entropy with \( \alpha = 1 \) and select the Shannon Entropy as our kernel metric, as it is equivalent the the Kullback Lieblier divergence.

\begin{equation}
\| X - \phi \|_{H} = \sum_{x \in X} \phi(x) \log \phi(x)
\end{equation}

Recalling that \( S_\mathbf{x} = \{ S \cup \mathbf{x} \} \), we observe that:

\begin{equation}
\| S_{\mathbf{x}} - S_i \|_{H} = \| S - S_i \|_{H} + \| \mathbf{x} - S_i \|_{H}
\end{equation}

We restate the kernel function as:

\begin{equation}
K_\gamma( X, S_i ) =  \frac{1}{\gamma \sqrt{2\pi} } e^{-\frac{1}{\gamma} \|X - S_i \|_{H}^2}
\end{equation}

In situations where probabilities are being calculated for a uniform set of points in \( \Omega^{\mathcal{S}} \), this can considerably reduce the computational time needed to evaluate \( \mathbf{K} \), as \( \|\mathbf{x} - S_i \|_H \) can be computed once and then added to each \( \|S - S_i\|_H, S \in \mathcal{S} \).


\subsection{ Random Vector Estimation }
 
Evaluating \ref{eq:SVProcessSingle} over a training set \( X \) produces a set \( \mathcal{S}_{SV} \in \mathcal{S} \) referred to as Support Vectors.  While \ref{eq:SVProcessSingle} produces a sparse representation of \( \mathcal{P}^\mathcal{S} \) by eliminating some \( S \), further sparseness can be achieved by refining the definition of \( \phi(\mathbf{x}) \) to allow the elimination of some \( \mathbf{x} \) in each \( S \in \mathcal{S}_{SV} \).  In the context of computing the kernel matrices used in optimizing \ref{eq:SVProcessSingle} the Parzen Window definition of \( \phi(\mathbf{x}) \) is computationally acceptable, as it results in a good approximation with a minimal amount of computation \footnote{ The Parzen Window estimate at a point requires distance computations for each observation and a summation.  By contrast, the SV estimate requires the same number of distance computations as well as the solving of a quadratic optimization problem whose complexity increases exponentially with the number of observations. }.  In the context of evaluating \ref{eq:SVResultSingle} over a testing set \( \hat{X} \), the Parzen Window algorithm for \( \phi(\mathbf{x}) \) is sub-optimal due to the fact that it requires the full set of observations for each \( S \in \mathcal{S}_{SV} \).  In this context, a sparse algorithm for \( \phi(\mathbf{x}) \) would reduce both the data required to store \( \mathcal{S}_{SV} \) and the computational resources required to evaluate \ref{eq:DKLemp}.

We return now to the empirical cumulative distribution function method of Support Vector density estimation and search for a solution in the following form, which is able to use multiple non-symmetric kernel functions simultaneously:

\begin{equation} \label{eq:SVDensity}
\phi(x) = \sum_{i=1}^\ell \left( \beta_i^1 \mathcal{K}_1(x_i,x) + ... + \beta_i^\kappa \mathcal{K}_\kappa( x_i,x) \right)
\end{equation}

We select values of \( \beta \) which minimize the square loss of the empirical cumulative probability distribution using some regularizer:

\begin{align}
&\min \left( \sum_{i=1}^\ell \left( y_i - \sum_{j=1}^\ell \sum_{n=1}^\kappa \beta_j^n k_n(x_i, x_j) \right)^2 + \lambda \sum_{i=1}^\ell \sum_{n=1}^\kappa \frac{1}{\gamma_n} \beta_i^n \right) \\
&\text{subject to} \quad \sum_{i=1}^\ell \sum_{n=1}^\kappa \beta_i^n = 1, \quad \beta_i \ge 0 \\
\end{align}

given a kernel function \( k(x,x') \) from the sigmoid family to approximate the cumulative probability distribution and it's derivative which we refer to as the cross-kernel \( \mathcal{K}(x,x') \) which we use to construct an estimate of the probability distribution:

\begin{align}
&k(x,x') = \frac{1}{1+e^{-\gamma(x-x')} } \\
&\mathcal{K}(x,x') = -\frac{\gamma}{2 + e^{\gamma(x-x')} + e^{-\gamma(x-x')} } 
\end{align}

\section{Results}
The architecture has been tested against several data sets.  In all cases the system parameters are left unchanged to eliminate the possbility of optimizing the system performance to best match the known outcomes.

\subsection{Eunite Competition Data}


\subsection{Santa Fe Data}


\subsection{CATS Benchmark Data}


\subsection{Results Summary}


\section{Further Research}

\subsection{Data Pre-Processing}
% logistic function using mean and sd to put most training points between .1 and .9

% upper and lower bounds on \(\int \Delta \) as non-stationary data detections mechanism

% logistic function from delta using mean and sd in same way if data non-stationary

% iterative integration process until stationary data found ?


\subsection{Ensemble System}
% more analysis of recent data, some type of transfer from short to long-term 'memory'

% heirarchical system?

% what parameters change?


\subsection{Weighted Influence}
% a weight term could be added to the regularizer to control the influence of specific sets.  This could provide more information in 'important' situations, which could be useful in motivated learning.


\section{Conclusion}
Eat it, bitches


\nocite{Moreno03}

\bibliographystyle{plain}
\bibliography{Research/research.bib}

\end{document}
