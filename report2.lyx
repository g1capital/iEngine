#LyX 1.6.1 created this file. For more info see http://www.lyx.org/
\lyxformat 345
\begin_document
\begin_header
\textclass article
\begin_preamble



\usepackage{amsfonts}
\end_preamble
\use_default_options false
\language english
\inputencoding auto
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\paperfontsize 10
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 0
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\author "" 
\author "" 
\end_header

\begin_body

\begin_layout Title
Parzen-SVM: An Approach to Timeseries Analysis Using Support Vector Machines
 based on Parzen Windows
\end_layout

\begin_layout Author
Ryan Michael
\begin_inset Newline newline
\end_inset

 
\family typewriter
kerinin@gmail.com
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Subsection
Existing Work
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Expectation-maximization_algorithm
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Stationary_process
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Ergodicity
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Mixing_(mathematics)
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Lyapunov_exponent
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Recurrence_quantification_analysis
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Recurrence_plot
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Autoregressive_fractionally_integrated_moving_averag
e
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Autocorrelation
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
url{http://en.wikipedia.org/wiki/Linear_discriminant_analysis}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Hidden Markov Model
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% cannot account for future states - only capable of prediction
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% weak, short-term memory
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Box-Jenkins
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% intended for simplistic processes with well-understood stationarity and
 periodicity
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% http://en.wikipedia.org/wiki/Box-Jenkins
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Spectral Analysis
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% assumes some type of frequency-domain decomposition.
  Frequency-domain signal representations do not do a very good job predicting
 time-domain values.
 
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Shrinking-
\begin_inset Formula $\epsilon$
\end_inset

 SVM Regression
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% theoretical foundation weak; only compensates for the relevance of recent
 data.
  See Markhov problem
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
General Overview
\end_layout

\begin_layout Standard
The goal is to create a method of statistical inference capable of processing
 timeseries data which is both multi-variate and exhibits different behaviors
 at different times.
 This type of data is common, and developing a robust method of analysis
 has applications in many domains.
 The general approach is to create a series of estimates using subsets of
 the observed data, and to then combine these estimates in an intelligent
 manner which captures the relevance of each estimate to the current prediction
 task.
 By using a set of 'typical' estimates, we are able to reduce the computational
 demands of the system, as each estimate is a condensed representation of
 the data from which it was derived.
 This approach also allows us to reduce data redundancy by only using distinct
 estimates.
\end_layout

\begin_layout Standard
The most basic operation used in this system is the estimation of probability
 densities.
 Based on a set of observations drawn from some random process, we generate
 an estimate of the underlying probability distribution.
 This estimate tells us the probability of each point in the input space
 being observed.
 Areas of the input space in which a dense set of observations are observed
 are given high probability, while areas of the input space with few observation
s are given low probability.
\end_layout

\begin_layout Standard
We assume that observations are pulled from multiple independent sources,
 all of which respond to some underlying phenomena.
 For instance one set of observations could be from a microphone and another
 from a light detector.
 We do not know how the inputs are related or to what extent their behavior
 changes over time.
 For example one input could be a steady sine wave and another could be
 based on the decay of a radioactive isotope.
 In the former case previous observations of the source are useful, in the
 latter they're not.
 Alternately two inputs could be light detectors in the same room or they
 could be on different continents; in the former their input would be highly
 similar, in the latter not as much.
\end_layout

\begin_layout Section
Problem Setting
\end_layout

\begin_layout Standard
We begin with a hidden random variable
\end_layout

\begin_layout Standard
\begin_inset Formula \[
X=(\Omega,\mathcal{F},\mathcal{P})\]

\end_inset


\end_layout

\begin_layout Standard
Our knowledge of 
\begin_inset Formula $X$
\end_inset

 comes from a set of independant sources which we treat as random processes
 generated by 
\begin_inset Formula $X$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \[
X^{n}:\Omega\mapsto\Omega^{n}\in\mathbb{R}^{d}\times\mathbb{R}_{+}\]

\end_inset

 
\begin_inset Formula \[
X^{n}=(\Omega^{n},\mathcal{F}^{n},\mathcal{P}^{n})\]

\end_inset

 
\begin_inset Formula \[
\mathbf{X}=[X^{0},...,X^{c}]\]

\end_inset


\end_layout

\begin_layout Standard
We refer to each of these sources as a channel, and refer to each channel
 as the 
\begin_inset Formula $i^{\text{th}}$
\end_inset

 element of the set 
\begin_inset Formula $\mathbf{X}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \[
C^{n}=[\Omega^{n},\mathcal{F}^{n},\mathcal{P}^{n}]\]

\end_inset


\end_layout

\begin_layout Standard
For each channel we are given a set of 
\begin_inset Formula $\ell$
\end_inset

 observations of dimension 
\begin_inset Formula $d$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\mathbf{x}_{i}^{n}=(x_{i}^{n},t_{i}^{n})\]

\end_inset

 
\begin_inset Formula \[
X^{n}=\left[\mathbf{x}_{0}^{n},\ldots,\mathbf{x}_{\ell}^{n}\right]\in\mathbb{R}^{d}\times\mathbb{R}_{+}\]

\end_inset


\end_layout

\begin_layout Standard
We define a set of time values and durations:
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\mathcal{T}=[t_{0},\ldots,t_{\ell}]\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \[
\Theta=[\theta_{0},\ldots,\theta_{z}]\]

\end_inset


\end_layout

\begin_layout Standard
We assume that 
\begin_inset Formula $\mathcal{P}$
\end_inset

 can be approximated in a given time window using a shifted set of weighted
 distributions defined over some set of time intervals.
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\mathcal{P}(t,\theta)=\sum_{i}\delta_{i}\cdot f_{i}(t-t_{i})\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\delta_{n}$
\end_inset

 is the weight corresponding to 
\begin_inset Formula $f_{n}$
\end_inset

.
 Finally, we assume that a similar mixture of distributions can be determined
 for each channel:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\mathcal{P}^{n}(t,\theta)=\sum_{i}\delta_{i}^{n}\cdot f_{i}^{n}(t-t_{i})\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
Single Channel Setting
\end_layout

\begin_layout Standard
We begin by considering the case where only one channel exists, so for now
 we will omit the superscript and refer to 
\begin_inset Formula $X^{n}=(\Omega^{n},\mathcal{F}^{n},\mathcal{P}^{n})$
\end_inset

 as 
\begin_inset Formula $X=(\Omega,\mathcal{F},\mathcal{P})$
\end_inset

.
 Given a set of test data 
\begin_inset Formula $\hat{X}$
\end_inset

, our goal is to estimate the probability distribution of 
\begin_inset Formula $X$
\end_inset

 over some time window:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\mathbf{\hat{x}}_{i} & =(\hat{x}_{i},t_{i})\\
\hat{X} & =[\mathbf{\hat{x}}_{0},\ldots,\mathbf{\hat{x}}_{k}]\\
 & (t_{\hat{x}},\theta_{\hat{x}}),\quad t_{\hat{x}}<\min_{t}\hat{X}<\max_{t}\hat{X}<t_{\hat{x}}+\theta_{\hat{x}}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We begin by defining the subset of the training observations which fall
 into each time window, shifted to the interval 
\begin_inset Formula $t\in[0,\theta)$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
S_{t,\theta}=\left[\left(x_{i},t_{i}-t\right)|\quad x_{i}\in X,\ 0\le t_{i}-t<\theta\right]\label{eq:Ssingle}\end{equation}

\end_inset

 
\begin_inset Formula \[
\mathcal{S}=\begin{bmatrix}S_{t_{0},\theta_{0}} & \cdots & S_{t_{0},\theta_{z}}\\
\vdots & \ddots & \vdots\\
S_{t_{\ell},\theta_{0}} & \cdots & S_{t_{\ell},\theta_{z}}\end{bmatrix}\]

\end_inset


\end_layout

\begin_layout Standard
We treat 
\begin_inset Formula $\mathcal{S}$
\end_inset

 as a random process:
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\mathcal{S}=[\Omega^{\mathcal{S}},\mathcal{F}^{\mathcal{S}},\mathcal{P}^{\mathcal{S}}]\]

\end_inset


\end_layout

\begin_layout Standard
and observe that 
\begin_inset Formula $\hat{X}$
\end_inset

 can be treated as an observation of 
\begin_inset Formula $\mathcal{S}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
S_{\hat{x}}=\left[\left(x_{i},t_{i}-\min_{t}\hat{X}\right)|\quad(x_{i},t_{i})\in\hat{X}\right]\label{eq:SySingle}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
We can now frame the task of estimating the probability distribution of
 
\begin_inset Formula $\hat{X}$
\end_inset

 as a task of estimating a probability density function 
\begin_inset Formula $\varphi$
\end_inset

 for the random process 
\begin_inset Formula $\mathcal{S}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\Pr(X=\mathbf{x}\ |\ \hat{X})\mapsto\ \Pr(\mathcal{S}=\{S_{\hat{x}}\cup\mathbf{x}\})\ \simeq\ \varphi(\mathbf{x},S_{\hat{x}})\label{eq:PrS}\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
Multiple Channel Setting
\end_layout

\begin_layout Standard
In order to extend this result to settings in which multiple channels exist,
 we return to 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Ssingle"

\end_inset

 and extend the scope to multiple channels:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
S_{t,\theta}=\left[\left(x_{i}^{n},t_{i}-t\right)|\quad x_{i}^{n}\in\mathbf{X},\ 0\le t_{i}-t<\theta\right]\label{eq:Smultiple}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
In this case, we treat each channel as an orthonormal basis of the abstract
 space 
\begin_inset Formula $\Omega^{\mathcal{S}}$
\end_inset

.
 Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:SySingle"

\end_inset

 is likewise extended in the same manner:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
S_{\hat{x}}=\left[\left(x_{i}^{n},t_{i}-\min_{t}\hat{X}\right)|\quad(x_{i}^{n},t_{i})\in\mathbf{\hat{X}}\right]\label{eq:SyMultiple}\end{equation}

\end_inset


\end_layout

\begin_layout Section
Parzen Window Estimation
\end_layout

\begin_layout Standard
One method of evaluating 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:PrS"

\end_inset

 is by using the Parzen Window method.
 We choose the Parzen Window method because it allows us to estimate probabiliti
es of unordered sets, provided they have an addition operation and a kernel
 function exists to provide a distance metric.
\end_layout

\begin_layout Subsection
 Single Channel Parzen Window
\end_layout

\begin_layout Standard
We will again begin by considering the single-channel case, then extend
 the resulting equations as necessary.
 The Parzen Window method requires the definition of a metric over the abstract
 space 
\begin_inset Formula $\Omega^{\mathcal{S}}$
\end_inset

.
 Such a metric can be defined using the symmetric Kullbeck Liebler divergence
 with probability measures 
\begin_inset Formula $\phi_{n}(\mathbf{x})\simeq Pr(X=\mathbf{x}\ |\ S_{n})$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align}
D_{KL}(S_{n}\|S_{m}) & =\sum_{\mathbf{x}\in\{S_{n}\cup S_{m}\}}\phi_{n}(\mathbf{x})\log\frac{\phi_{n}(\mathbf{x})}{\phi_{m}(\mathbf{x})}\label{eq:KLsingle}\\
 & =-\sum_{\mathbf{x}\in\{S_{n}\cup S_{m}\}}\phi_{n}(\mathbf{x})\log\phi_{m}(\mathbf{x})+\sum_{\mathbf{x}\in\{S_{n}\cup S_{m}\}}\phi_{n}(\mathbf{x})\log\phi_{n}(\mathbf{x})\\
\|S_{n}-S_{m}\|_{KL} & =D_{KL}(S_{n}\|S_{m})+D_{KL}(S_{m}\|S_{n})\end{align}

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% Need to discuss why the union of n&m is sufficient to evaluate the KL
 divergence
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
The Parzen Window estimation of 
\begin_inset Formula $\mathcal{P}^{\mathcal{S}}$
\end_inset

 is defined as:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\Pr(\mathcal{S}=S)\simeq\sum_{i\in\mathcal{S}}\frac{1}{|\mathcal{S}|}K_{\gamma}(S,S_{i})\label{eq:PrSParzenSingle}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $|\cdot|$
\end_inset

 denotes the cardinality of 
\begin_inset Formula $(\cdot)$
\end_inset

 and 
\begin_inset Formula $K$
\end_inset

 is some kernel function, for instance the Radial Basis Function:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
K_{\gamma}(S_{i},S_{j})=\frac{1}{\gamma\sqrt{2\pi}}e^{-\frac{1}{\gamma}\|S_{i}-S_{j}\|_{KL}^{2}}\label{eq:KRBFparzen}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The same method can be used to estimate 
\begin_inset Formula $\phi_{n}(\mathbf{x})$
\end_inset

 for a given subset 
\begin_inset Formula $S_{n}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\phi_{n}(\mathbf{x})=\sum_{\mathbf{x}_{i}\in S_{n}}\frac{1}{|S_{n}|}K_{\gamma}(\mathbf{x},\mathbf{x}_{i})\label{eq:PhiSingle}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Substituting 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:PrSParzenSingle"

\end_inset

 into 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:PrS"

\end_inset

 our probability distribution estimate becomes:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\varphi_{P}(\mathbf{x},S)=\sum_{i\in\mathcal{S}}\frac{1}{|\mathcal{S}|}K_{\gamma}(S_{\mathbf{x}},S_{i})\label{eq:VarphiParzenSingle}\end{equation}

\end_inset

 
\begin_inset Formula \[
S_{\mathbf{x}}=\{S\cup\mathbf{x}\}\]

\end_inset


\end_layout

\begin_layout Subsection
 Multiple Channel Parzen Window
\end_layout

\begin_layout Standard
Extending the Parzen Window approach requires the realization that 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:KLsingle"

\end_inset

 requires that 
\begin_inset Formula $S_{n}$
\end_inset

 and 
\begin_inset Formula $S_{m}$
\end_inset

 both be defined over the same abstract space 
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
 For instance if 
\begin_inset Formula $X_{n}\in\mathbb{R}^{2}$
\end_inset

 and 
\begin_inset Formula $X_{m}\in\mathbb{R}^{3}$
\end_inset

, it is impossible to calculate 
\begin_inset Formula $\phi_{n}\big((x_{m},t)\big)$
\end_inset

 because the quantituy 
\begin_inset Formula $\|x_{m}-x_{n}\|^{2}$
\end_inset

 from using 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:PhiSingle"

\end_inset

 is ambiguous.
\end_layout

\end_inset

.
 As mentioned earlier, in the Multiple Channel context each channel is treated
 as an orthonormal basis of 
\begin_inset Formula $\Omega^{\mathcal{S}}$
\end_inset

.
 An obvious approach to defining a measure over 
\begin_inset Formula $\Omega^{\mathcal{S}}$
\end_inset

 for mutliple channels is to use the Euclidean norm of the Kullbeck Liebler
 divergence of each channel considered independently:
\end_layout

\begin_layout Standard
\begin_inset Formula \[
S_{n}^{c}=S_{n}\cap X^{c}\]

\end_inset


\end_layout

\begin_layout Standard
This requires the following minor extension of 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:KRBFparzen"

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
K_{\gamma}(S_{i},S_{j})=\prod_{c}\frac{1}{\gamma\sqrt{2\pi}}e^{-\frac{1}{\gamma}\|S_{i}^{c}-S_{j}^{c}\|_{KL}^{2}}\label{eq:KRBFParzenMultiple}\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
Sliding Parzen Windows
\end_layout

\begin_layout Standard
Observe that subsets do not exist in isolation; it is possible for the time
 windows defined for two subsets to overlap.
 This makes it possible for the estimates at at two such subsets to take
 advantage of this shared information.
 This can be accomplished by modifying 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:PrSParzenSingle"

\end_inset

 to include sets with arbitrary offsets 
\begin_inset Formula $\tau$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Formula \[
S_{t,\theta}^{\tau}=\left[\left(x_{i},t_{i}-t+\tau\right)|\quad(x_{i}^{n},t_{i})\in S_{t,\theta}\right]\]

\end_inset


\end_layout

\begin_layout Standard
Within these bounds, we can evaluate our kernel function at each step of
 arbitrarily small value 
\begin_inset Formula $\eta$
\end_inset

 and sum the results.
 For computational reasons we establish the following bounds on the offset
 which essentially require that the two subsets share at least one point
 in 
\begin_inset Formula $t$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\varphi_{Pslide}(\mathbf{x},S) & =\frac{1}{|\mathcal{S}|}\sum_{j\in\mathcal{S}}\frac{1}{|T|}\sum_{\tau\in T}\ K_{\gamma}(S_{\mathbf{x}},S_{j}^{\tau}),\\
T & =\left[-\theta_{j},\ -\theta_{j}+\eta,\ -\theta_{j}+2n,\ldots,\ \theta_{S\cup\mathbf{x}}+\theta_{j}\right]\\
\theta_{S\cup\mathbf{x}} & \ge\left(|S\cup\mathbf{x}|_{\max_{t}}-|S\cup\mathbf{x}|_{\min_{t}}\right)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Where Taking the limit as 
\begin_inset Formula $\eta\mapsto0$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\lim_{\eta\mapsto0}\varphi_{Pslide}(\mathbf{x},S) & =\frac{1}{|\mathcal{S}|}\sum_{j\in\mathcal{S}}\int_{-\theta_{j}}^{\theta_{S\cup\mathbf{x}}+\theta_{j}}K_{\gamma}(S_{\mathbf{x}},S_{j}^{\eta})d\eta\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
What if we define 
\begin_inset Formula $|\mathcal{S}|$
\end_inset

 to contain a single S which encompasses the entire training set? In this
 case our problem reduces to the integral of the kernel over the range of
 X.
 This seems like the ideal solution - it reduces computational demands for
 any system with more than 2 window sizes and allows arbitrary pattern length.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
If we define a cross kernel function 
\begin_inset Formula $k_{\gamma}$
\end_inset

 to be the antiderivative of 
\begin_inset Formula $K_{\gamma}$
\end_inset

, this simplifies to:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\lim_{\eta\mapsto0}\varphi_{Pslide}(\mathbf{x},S) & =\frac{1}{|\mathcal{S}|}\sum_{j\in\mathcal{S}}k_{\gamma}\left(S_{\mathbf{x}},S_{j}^{\theta_{S\cup\mathbf{x}}+\theta_{j}}\right)-k_{\gamma}\left(S_{\mathbf{x}},S_{j}^{-\theta_{j}}\right)\end{align*}

\end_inset


\end_layout

\begin_layout Section
Support Vector Estimation
\end_layout

\begin_layout Standard
The Parzen Window method is neither sparse nor computationally efficient,
 and as the number of observations grows, these deficiencies quickly become
 prohibitive.
 We now investigate the use of Support Vector Machines to generate 
\begin_inset Formula $\varphi(\mathbf{x},S)$
\end_inset

.
\end_layout

\begin_layout Subsection
 Random Process Estimation 
\end_layout

\begin_layout Standard
Support Vector Machines are usually used to estimate probability distributions
 by solving the related problem of estimating the cumulative distribution
 function of the random variable in question.
 This reduces the problem to one of estimating a non-linear mapping from
 observations to cumulative distribution values, which can be formulated
 as an optimization problem over a linear operator equation.
 Unfortunately, these methods depend on the ability to calculate an empirical
 distribution for each observation:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
F_{\ell}(x)=\frac{1}{\ell}\sum_{i}\theta(x-x_{i})\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\theta(x)$
\end_inset

 is the indicator function.
 To evaluate this function, the abstract space 
\begin_inset Formula $\Omega^{\mathcal{S}}$
\end_inset

 must be ordered.
 While we have described a distance metric over 
\begin_inset Formula $\Omega^{\mathcal{S}}$
\end_inset

, it is not clear what a meaningful ordering relation would be.
\end_layout

\begin_layout Standard
Rather than calculating the cumulative probability distribution of 
\begin_inset Formula $\Omega^{\mathcal{S}}$
\end_inset

, we begin with the assumption that the Parzen Window estimate of the probabilit
y distribution is accurate and attempt to minimize the square loss between
 the Support Vector estimate and the Parzen Window estimate.
 Because we are hoping to generate a sparse representation of the probability
 distribution, we add a regularizing term 
\begin_inset Formula $\Omega$
\end_inset

 which penalizes similar 
\begin_inset Formula $S$
\end_inset

.
 The Support Vector approach seeks a solution in the following form:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align}
\varphi_{SV}(\mathbf{x},S:\beta) & =\sum_{i\in\mathcal{S}}\beta_{i}K_{\gamma}(S_{\mathbf{x}},S_{i})\label{eq:SVResultSingle}\\
S_{\mathbf{x}} & =\{S\cup\mathbf{x}\}\end{align}

\end_inset


\end_layout

\begin_layout Standard
So we can express the Support Vector optimization problem as:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
W(\beta) & =\sum_{S\in\mathcal{S}}\left(\varphi_{P}(S)-\varphi_{SV}(S:\beta)\right)^{2}+\lambda\Omega(\beta,S)\\
 & =\sum_{S\in S}\left(\varphi_{SV}(S:\beta)^{2}-2\varphi_{SV}(S:\beta)\varphi_{P}(S)\right)+\lambda\Omega(\beta,S)\\
 & =\sum_{S\in\mathcal{S}}\left(\left(\sum_{i\in\mathcal{S}}\beta_{i}K_{\gamma}(S,S_{i})\right)\cdot\left(\sum_{j\in\mathcal{S}}\beta_{j}K_{\gamma}(S,S_{j})\right)\right)+\lambda\Omega(\beta,S)\\
 & =\sum_{i,j\in\mathcal{S}}\beta_{i}\beta_{j}\sum_{S\in\mathcal{S}}K_{\gamma}(S,S_{i})K_{\gamma}(S,S_{j})\\
 & \qquad\qquad\qquad\qquad+\sum_{i\in\mathcal{S}}\beta_{i}\sum_{S\in\mathcal{S}}\left(\lambda K_{\gamma}(S,S_{i})^{-1}-\frac{2}{|\mathcal{S}|}\sum_{j\in\mathcal{S}}K_{\gamma}(S,S_{i})K_{\gamma}(S,S_{j})\right)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\text{subject to}\quad\sum_{i}\beta_{i}=1,\quad\beta_{i}\ge0,\ i=1,\ldots,|\mathcal{S}|\label{eq:SVProcessSingle}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Notice the regularizer selected is defined as:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\Omega(\beta,S)=\sum_{i\in\mathcal{S}}\beta_{i}\sum_{S\in\mathcal{S}}K_{\gamma}(S,S_{i})^{-1}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:SVProcessSingle"

\end_inset

 is a quadratic optimiztion problem defined as:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
W(\beta) & =\frac{1}{2}\beta^{T}P\beta+q^{T}\beta\\
P_{i,j} & =\sum_{S\in\mathcal{S}}K_{\gamma}(S,S_{i})K_{\gamma}(S,S_{j})\\
q_{i} & =\sum_{S\in\mathcal{S}}\left(\lambda K_{\gamma}(S,S_{i})^{-1}-\frac{1}{|\mathcal{S}|}\sum_{j\in\mathcal{S}}K_{\gamma}(S,S_{i})K_{\gamma}(S,S_{j})\right)\end{align*}

\end_inset

 
\begin_inset Formula \begin{align}
P & =\langle\mathbf{K}^{T}\cdot\mathbf{K}\rangle\label{eq:SVPSingle}\\
q & =\lambda\langle\mathbf{K}^{T}\cdot\mathbf{1}_{(|\mathcal{S}|,1)}\rangle^{-1}-\frac{1}{|\mathcal{S}|}\langle\mathbf{K}^{T}\cdot\mathbf{K}\cdot\mathbf{1}_{(|\mathcal{S}|,1)}\rangle\end{align}

\end_inset


\end_layout

\begin_layout Subsection
Sliding Windows 
\end_layout

\begin_layout Standard
The approach described thus far has some drawbacks.
 The prediction horizon is limited to the interval 
\begin_inset Formula $[\min_{t}\hat{X},\ \min_{t}\hat{X}+\theta)$
\end_inset

.
 This can be ameliorated by shifting the prediction horizon along the test
 set 
\begin_inset Formula $\hat{X}$
\end_inset

, however this requires a method of choosing which starting point to use.
 A second limitation is that it fails to take advantage of the time-independent
 distribution of 
\begin_inset Formula $X$
\end_inset

 in for points outside the 'forecasting horizon' defined by the largest
 time window and the extents of 
\begin_inset Formula $X$
\end_inset

 in the time dimension.
 
\end_layout

\begin_layout Standard
Modifying the SV algorithm to use sliding time windows addresses both of
 these concerns.
 As in the Sliding Parzen Window approach, we use a kernel function 
\begin_inset Formula $K_{\gamma}$
\end_inset

 and a corresponding cross kernel 
\begin_inset Formula $k_{\gamma}$
\end_inset

 which is the kernel's antiderivative.
 We search for a solution in the following form:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\varphi_{SV}(\mathbf{x},S:\beta) & =\sum_{i\in\mathcal{S}}\beta_{i}\ \int_{-\theta_{j}}^{\theta_{S\cup\mathbf{x}}+\theta_{j}}K_{\gamma}(S_{\mathbf{x}},S_{i}^{\eta})\ d\eta\\
 & =\sum_{i\in\mathcal{S}}\beta_{i}\left(k_{\gamma}\left(S_{\mathbf{x}},S_{i}^{\theta_{S\cup\mathbf{x}}+\theta_{i}}\right)-k_{\gamma}\left(S_{\mathbf{x}},S_{i}^{-\theta_{i}}\right)\right)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
In this case, we leave our optimization problem as previously defined and
 search for 
\begin_inset Formula $\beta$
\end_inset

 using the training data and 
\begin_inset Formula $K_{\gamma}$
\end_inset

.
\end_layout

\begin_layout Subsubsection
Sliding Window Kernel Selection
\end_layout

\begin_layout Standard
Our kernel should satisfy two primary constraints; it should provide an
 adequate kernel for Parzen Window estimation and it should have a known
 antiderivative.
 Borrowing from Vapnik then, we use the following kernel and cross-kernel
 functions:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
K_{\gamma}(x,x') & =-\frac{\gamma}{2+e^{\gamma(x-x')}+e^{-\gamma(x-x')}}\\
k_{\gamma}(x,x') & =\frac{1}{1+e^{-\gamma(x-x')}}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
It is worth noting that 
\begin_inset Formula $k_{\gamma}$
\end_inset

 can be interpreted as the cumulative distribution function of 
\begin_inset Formula $\mathcal{S}$
\end_inset

, however without an ordering operation over 
\begin_inset Formula $\Omega^{\mathcal{S}}$
\end_inset

, this interpretation cannot be justified formally.
\end_layout

\begin_layout Standard
We can continue to use the Kullback Liebler divergence as our difference
 function as it is a sufficient metric for Lebesgue integration.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Is there any reason we couldn't now consider the entire training set as
 a single S with theta=infty? 
\end_layout

\begin_layout Plain Layout
How would this affect the SVM optimization? In this case we would need a
 condensed representation of the training set which retains sequence probability
 (beta) and uniqueness (SV identity).
 The real question is if this approach (single large window) affects the
 optimization problem.
 
\end_layout

\begin_layout Plain Layout
It might be possible to sum the beta value of each subset on a per-observation
 basis; the beta sum would remain 1 and points in multiple windows would
 be made stronger and the result would be a single composite S w/ theta
 = range(S).
 This would need to be proven OK based on the solution function for the
 SVM problem.
\end_layout

\begin_layout Plain Layout
A better solution would be to change the SVM problem to take advantage of
 the sliding window idea directly somehow.
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Parzen-SVM Decomposition 
\end_layout

\begin_layout Standard
Due to the simplicity of the constraints in the optimization problem, it
 is possible to use the decomposition method of Osuna to reduce the memory
 requirements of the Parzen-SVM algorithm.
\end_layout

\begin_layout Subsubsection
 Sub-Problem Definition 
\end_layout

\begin_layout Standard
The decomposition algorithm breaks 
\begin_inset Formula $X$
\end_inset

 into two working sets 
\begin_inset Formula $B,N$
\end_inset

, and attempts to optimize 
\begin_inset Formula $B$
\end_inset

 while keeping 
\begin_inset Formula $N$
\end_inset

 fixed.
 This results in the following iterative optimization problem where 
\begin_inset Formula $\boldsymbol{\beta}^{k}$
\end_inset

 denotes the result of the previous iteration:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
W(\boldsymbol{\beta}_{B}) & =\frac{1}{2}\begin{bmatrix}\boldsymbol{\beta}_{B}^{T} & (\boldsymbol{\beta}_{N}^{k})^{T}\end{bmatrix}\begin{bmatrix}P_{BB} & P_{BN}\\
P_{NB} & P_{NN}\end{bmatrix}\begin{bmatrix}\boldsymbol{\beta}_{B}\\
\boldsymbol{\beta}_{N}^{k}\end{bmatrix}-\begin{bmatrix}q_{B}^{T} & q_{N}^{T}\end{bmatrix}\begin{bmatrix}\boldsymbol{\beta}_{B}\\
\boldsymbol{\beta}_{N}^{k}\end{bmatrix}\\
 & =\frac{1}{2}\boldsymbol{\beta}_{B}^{T}P_{BB}\boldsymbol{\beta}_{B}-(-q_{B}+P_{BN}\boldsymbol{\beta}_{N}^{k})^{T}\boldsymbol{\beta}_{B}\\
 & =\frac{1}{2}\begin{bmatrix}\beta_{i} & \beta_{j}\end{bmatrix}\begin{bmatrix}P_{ii} & P_{ij}\\
P_{ij} & P_{jj}\end{bmatrix}\begin{bmatrix}\beta_{i}\\
\beta_{j}\end{bmatrix}-(-q_{B}+P_{BN}\boldsymbol{\beta}_{N}^{k})^{T}\begin{bmatrix}\beta_{i}\\
\beta_{j}\end{bmatrix}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\text{subject to}\quad0\le\beta_{i},\beta_{j},\quad\beta_{i}+\beta_{j}=1-\mathbf{1}^{T}\boldsymbol{\beta}_{N}^{k}\end{equation}

\end_inset


\end_layout

\begin_layout Subsubsection
 Working Set Selection 
\end_layout

\begin_layout Standard
Select
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align}
 & i\in\text{arg}\max_{t}\left\{ -\nabla f(\boldsymbol{\beta}^{k})_{t}\ |\quad t\in I(\boldsymbol{\beta}^{k})\right\} \\
 & j\in\text{arg}\min_{t}\left\{ -\frac{b_{it}^{2}}{a_{it}}\ |\quad t\in I(\boldsymbol{\beta}^{k}),\quad-\nabla f(\boldsymbol{\beta}^{k})_{t}<-\nabla f(\boldsymbol{\beta}^{k})_{i}\right\} \end{align}

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% This needs to be checked for the new optimization scenario
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align}
I(\boldsymbol{\beta}) & \equiv\{t\ |\quad\beta_{t}<1\quad\text{or}\quad\beta_{t}>0\}\\
 & a_{it}=P_{ii}+P_{tt}-2P_{it}\\
 & \bar{a}_{it}=\begin{cases}
a_{it} & \text{if}\ a_{it}>0\\
\delta & \text{otherwise}\end{cases}\\
 & b_{it}=-\nabla f(\boldsymbol{\beta}^{k})_{i}+\nabla f(\boldsymbol{\beta}^{k})_{t}\\
\nabla f(\boldsymbol{\beta})_{i} & \equiv P_{i}\boldsymbol{\beta}-q_{i}\end{align}

\end_inset


\end_layout

\begin_layout Subsubsection
 Stopping Condition 
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\max_{i\in I(\boldsymbol{\alpha}^{k})}-\nabla f(\boldsymbol{\alpha})_{i}+\min_{j\in I(\boldsymbol{\alpha}^{k})}\nabla f(\boldsymbol{\alpha})_{j}\le\epsilon\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
 Random Process Kernel Definition 
\end_layout

\begin_layout Standard
When developing the Parzen Window algorithm, we used kernel function 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:KLsingle"

\end_inset

 which is based on the Kullbeck Liebler divergence of a probability estimate
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:PhiSingle"

\end_inset

 defined at the sets being evaluated.
 Our motivation in developing a Support Vector approach is to generate sparse
 representations of 
\begin_inset Formula $\mathcal{P}^{\mathcal{S}}$
\end_inset

, in part to reduce the computational demands of evaluating 
\begin_inset Formula $\varphi(\mathbf{x},S)$
\end_inset

 for test data sets.
 Unfortunately, kernel function 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:KLsingle"

\end_inset

 requires probability estimates of both sets being compared - it would be
 helpful to develop a kernel function capable of evaluating a distance between
 a test set 
\begin_inset Formula $S_{\hat{X}}$
\end_inset

 and a training set 
\begin_inset Formula $S_{i}$
\end_inset

 without first calculating an estimate of the probability distribution of
 
\begin_inset Formula $S_{\hat{X}}$
\end_inset

.
\end_layout

\begin_layout Standard
Note that 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:SVProcessSingle"

\end_inset

 is formulated in such a way that the first kernel argument is always 
\begin_inset Formula $S_{\mathbf{x}}$
\end_inset

, which allows us to define a kernel which takes a set of points as its
 first argument and a probability distribution as its second.
 The definition of 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:SVPSingle"

\end_inset

 also allows us to define non-symmetric kernels without sacrificing convexity
 or monotonicity in the optimization objective function.
 We can therefore use any measure of the divergence between a set of points
 and a probability distribution.
 We consider the Renyi entropy with 
\begin_inset Formula $\alpha=1$
\end_inset

 and select the Shannon Entropy as our kernel metric, as it is equivalent
 the the Kullback Lieblier divergence.
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\|X-\phi\|_{H}=\sum_{x\in X}\phi(x)\log\phi(x)\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Recalling that 
\begin_inset Formula $S_{\mathbf{x}}=\{S\cup\mathbf{x}\}$
\end_inset

, we observe that:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\|S_{\mathbf{x}}-S_{i}\|_{H}=\|S-S_{i}\|_{H}+\|\mathbf{x}-S_{i}\|_{H}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
We restate the kernel function as:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
K_{\gamma}(X,S_{i})=\frac{1}{\gamma\sqrt{2\pi}}e^{-\frac{1}{\gamma}\|X-S_{i}\|_{H}^{2}}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
In situations where probabilities are being calculated for a uniform set
 of points in 
\begin_inset Formula $\Omega^{\mathcal{S}}$
\end_inset

, this can considerably reduce the computational time needed to evaluate
 
\begin_inset Formula $\mathbf{K}$
\end_inset

, as 
\begin_inset Formula $\|\mathbf{x}-S_{i}\|_{H}$
\end_inset

 can be computed once and then added to each 
\begin_inset Formula $\|S-S_{i}\|_{H},S\in\mathcal{S}$
\end_inset

.
\end_layout

\begin_layout Subsection
 Random Vector Estimation 
\end_layout

\begin_layout Standard
Evaluating 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:SVProcessSingle"

\end_inset

 over a training set 
\begin_inset Formula $X$
\end_inset

 produces a set 
\begin_inset Formula $\mathcal{S}_{SV}\in\mathcal{S}$
\end_inset

 referred to as Support Vectors.
 While 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:SVProcessSingle"

\end_inset

 produces a sparse representation of 
\begin_inset Formula $\mathcal{P}^{\mathcal{S}}$
\end_inset

 by eliminating some 
\begin_inset Formula $S$
\end_inset

, further sparseness can be achieved by refining the definition of 
\begin_inset Formula $\phi(\mathbf{x})$
\end_inset

 to allow the elimination of some 
\begin_inset Formula $\mathbf{x}$
\end_inset

 in each 
\begin_inset Formula $S\in\mathcal{S}_{SV}$
\end_inset

.
 In the context of computing the kernel matrices used in optimizing 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:SVProcessSingle"

\end_inset

 the Parzen Window definition of 
\begin_inset Formula $\phi(\mathbf{x})$
\end_inset

 is computationally acceptable, as it results in a good approximation with
 a minimal amount of computation 
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
 The Parzen Window estimate at a point requires distance computations for
 each observation and a summation.
 By contrast, the SV estimate requires the same number of distance computations
 as well as the solving of a quadratic optimization problem whose complexity
 increases exponentially with the number of observations.
 
\end_layout

\end_inset

.
 In the context of evaluating 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:SVResultSingle"

\end_inset

 over a testing set 
\begin_inset Formula $\hat{X}$
\end_inset

, the Parzen Window algorithm for 
\begin_inset Formula $\phi(\mathbf{x})$
\end_inset

 is sub-optimal due to the fact that it requires the full set of observations
 for each 
\begin_inset Formula $S\in\mathcal{S}_{SV}$
\end_inset

.
 In this context, a sparse algorithm for 
\begin_inset Formula $\phi(\mathbf{x})$
\end_inset

 would reduce both the data required to store 
\begin_inset Formula $\mathcal{S}_{SV}$
\end_inset

 and the computational resources required to evaluate 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:DKLemp"

\end_inset

.
\end_layout

\begin_layout Standard
We return now to the empirical cumulative distribution function method of
 Support Vector density estimation and search for a solution in the following
 form, which is able to use multiple non-symmetric kernel functions simultaneous
ly:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\phi(x)=\sum_{i=1}^{\ell}\left(\beta_{i}^{1}\mathcal{K}_{1}(x_{i},x)+...+\beta_{i}^{\kappa}\mathcal{K}_{\kappa}(x_{i},x)\right)\label{eq:SVDensity}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
We select values of 
\begin_inset Formula $\beta$
\end_inset

 which minimize the square loss of the empirical cumulative probability
 distribution using some regularizer:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align}
 & \min\left(\sum_{i=1}^{\ell}\left(y_{i}-\sum_{j=1}^{\ell}\sum_{n=1}^{\kappa}\beta_{j}^{n}k_{n}(x_{i},x_{j})\right)^{2}+\lambda\sum_{i=1}^{\ell}\sum_{n=1}^{\kappa}\frac{1}{\gamma_{n}}\beta_{i}^{n}\right)\\
 & \text{subject to}\quad\sum_{i=1}^{\ell}\sum_{n=1}^{\kappa}\beta_{i}^{n}=1,\quad\beta_{i}\ge0\end{align}

\end_inset


\end_layout

\begin_layout Standard
given a kernel function 
\begin_inset Formula $k(x,x')$
\end_inset

 from the sigmoid family to approximate the cumulative probability distribution
 and it's derivative which we refer to as the cross-kernel 
\begin_inset Formula $\mathcal{K}(x,x')$
\end_inset

 which we use to construct an estimate of the probability distribution:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align}
 & k(x,x')=\frac{1}{1+e^{-\gamma(x-x')}}\\
 & \mathcal{K}(x,x')=-\frac{\gamma}{2+e^{\gamma(x-x')}+e^{-\gamma(x-x')}}\end{align}

\end_inset


\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Standard
The architecture has been tested against several data sets.
 In all cases the system parameters are left unchanged to eliminate the
 possbility of optimizing the system performance to best match the known
 outcomes.
\end_layout

\begin_layout Subsection
Eunite Competition Data
\end_layout

\begin_layout Subsection
Santa Fe Data
\end_layout

\begin_layout Subsection
CATS Benchmark Data
\end_layout

\begin_layout Subsection
Results Summary
\end_layout

\begin_layout Section
Further Research
\end_layout

\begin_layout Subsection
Data Pre-Processing
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% logistic function using mean and sd to put most training points between
 .1 and .9
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% upper and lower bounds on 
\backslash
(
\backslash
int 
\backslash
Delta 
\backslash
) as non-stationary data detections mechanism
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% logistic function from delta using mean and sd in same way if data non-station
ary
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% iterative integration process until stationary data found ?
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Ensemble System
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% more analysis of recent data, some type of transfer from short to long-term
 'memory'
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% heirarchical system?
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% what parameters change?
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Weighted Influence
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% a weight term could be added to the regularizer to control the influence
 of specific sets.
  This could provide more information in 'important' situations, which could
 be useful in motivated learning.
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Standard
Eat it, bitches
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
nocite{Moreno03}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
bibliographystyle{plain}
\end_layout

\end_inset

 
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "Research/research.bib"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
