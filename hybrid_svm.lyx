#LyX 1.6.7 created this file. For more info see http://www.lyx.org/
\lyxformat 345
\begin_document
\begin_header
\textclass article
\use_default_options true
\language english
\inputencoding auto
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\paperfontsize default
\spacing onehalf
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\leftmargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\author "" 
\author "" 
\end_header

\begin_body

\begin_layout Title
Hybrid SVM
\end_layout

\begin_layout Section
Original Formulation
\end_layout

\begin_layout Standard
The original formulation was to minimize 
\begin_inset Formula $F$
\end_inset

 subject to five inequality constraints, where 
\begin_inset Formula $\xi$
\end_inset

 is the distance from each point to the hyperplane (minus insensitive zone
 
\begin_inset Formula $\varepsilon$
\end_inset

).
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
F(\xi,\xi^{*}) & =\sum_{i=1}^{\ell}\xi_{i}^{*}+\sum_{i=1}^{\ell}\xi_{i}\\
y_{i}-(w\cdot x_{i})-b & \le\varepsilon+\xi_{i}^{*}\\
(w\cdot x_{i})+b-y_{i} & \le\varepsilon+\xi_{i}\\
\xi_{i}^{*} & \ge\ 0\\
\xi_{i} & \ge\ 0\\
(w\cdot w) & \le c_{n}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
This can be expressed as a Lagrange functional
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
L(w,\xi^{*},\xi;\alpha^{*},\alpha,C^{*},\gamma,\gamma^{*}) & =\sum_{i=1}^{\ell}(\xi_{i}^{*}+\xi_{i})-\sum_{i=1}^{\ell}\alpha_{i}\left[y_{i}-(w\cdot x_{i})-b+\varepsilon+\xi_{i}\right]\\
 & \qquad-\sum_{i=1}^{\ell}\alpha_{i}^{*}\left[(w\cdot x_{i})+b-y_{i}+\varepsilon+\xi_{i}^{*}\right]-\frac{C^{*}}{2}(c_{n}-(w\cdot w))\\
 & \qquad-\sum_{i=1}^{\ell}(\gamma_{i}^{*}\xi_{i}^{*}+\gamma_{i}\xi_{i})\end{align*}

\end_inset


\end_layout

\begin_layout Standard
To find a maximum of the Lagrange function, we set the partial derivative
 over the variables we want to maximize to 0, which should yield a system
 of partial derivatives over the other variables.
 This is due to the fact that if we're at a maximum, the derivative should
 be 0.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\frac{\partial F}{\partial w}=0 & =-\frac{\partial}{\partial w}\left(\sum_{i=1}^{\ell}\alpha_{i}\left[y_{i}-(w\cdot x_{i})-b+\varepsilon+\xi_{i}\right]\right)\\
 & \qquad-\frac{\partial}{\partial w}\left(\sum_{i=1}^{\ell}\alpha_{i}^{*}\left[(w\cdot x_{i})+b-y_{i}+\varepsilon+\xi_{i}\right]\right)-\frac{\partial}{\partial w}\left(\frac{C^{*}}{2}(c_{n}-(w\cdot w))\right)\\
 & =\sum_{i=1}^{\ell}\alpha_{i}x_{i}-\sum_{i=1}^{\ell}\alpha_{i}^{*}x_{i}-wC^{*}\\
wC^{*} & =\sum_{i=1}^{\ell}(\alpha_{i}-\alpha_{i}^{*})x_{i}\\
w & =\sum_{i=1}^{\ell}\frac{\alpha_{i}-\alpha_{i}^{*}}{C^{*}}x_{i}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\frac{\partial F}{\partial\xi^{*}}=0 & =\frac{\partial}{\partial\xi^{*}}\left(\sum_{i=1}^{\ell}(\xi_{i}^{*}+\xi_{i})\right)-\frac{\partial}{\partial\xi^{*}}\left(\sum_{i=1}^{\ell}\alpha_{i}^{*}\left[(w\cdot x_{i})+b-y_{i}+\varepsilon+\xi_{i}^{*}\right]\right)-\frac{\partial}{\partial\xi^{*}}\left(\sum_{i=1}^{\ell}(\gamma_{i}^{*}\xi_{i}^{*}+\gamma_{i}\xi_{i})\right)\\
 & =1-\sum_{i=1}^{\ell}\alpha_{i}^{*}-\sum_{i=1}^{\ell}\gamma_{i}^{*}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\frac{\partial F}{\partial\xi}=0 & =\frac{\partial}{\partial\xi}\left(\sum_{i=1}^{\ell}(\xi_{i}^{*}+\xi_{i})\right)-\frac{\partial}{\partial\xi}\left(\sum_{i=1}^{\ell}\alpha_{i}\left[y_{i}-(w\cdot x_{i})-b+\varepsilon+\xi_{i}\right]\right)-\frac{\partial}{\partial\xi}\left(\sum_{i=1}^{\ell}(\gamma_{i}^{*}\xi_{i}^{*}+\gamma_{i}\xi_{i})\right)\\
 & =1-\sum_{i=1}^{\ell}\alpha_{i}-\sum_{i=1}^{\ell}\gamma_{i}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The last two can be combined
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
1-\sum_{i=1}^{\ell}\alpha_{i}^{*}-\sum_{i=1}^{\ell}\gamma_{i}^{*} & =1-\sum_{i=1}^{\ell}\alpha_{i}^{*}-\sum_{i=1}^{\ell}\gamma_{i}^{*}\\
\sum_{i=1}^{\ell}\alpha_{i}^{*}+\sum_{i=1}^{\ell}\gamma_{i}^{*} & =\sum_{i=1}^{\ell}\alpha_{i}^{*}+\sum_{i=1}^{\ell}\gamma_{i}^{*}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Vapnik makes second term on each side go away somehow, leaving the following
 two conditions:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
w= & \sum_{i=1}^{\ell}\frac{\alpha_{i}^{*}-\alpha_{i}}{C^{*}}x_{i}\\
\sum_{i=1}^{\ell}\alpha_{i}^{*}= & \sum_{i=1}^{\ell}\alpha_{i}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Next, he puts those into the original lagrange (WTF?), apparently by eliminating
 the variables we just took partials for and substituting for 
\begin_inset Formula $w$
\end_inset


\begin_inset Formula \begin{align*}
W(\alpha^{*},\alpha,C^{*})= & \sum_{i=1}^{\ell}\alpha_{i}\left[y_{i}-(w\cdot x_{i})+\varepsilon\right]-\sum_{i=1}^{\ell}\alpha_{i}^{*}\left[(w\cdot x_{i})-y_{i}+\varepsilon\right]-\frac{C^{*}}{2}(c_{n}-(w\cdot w))\\
= & \sum_{i=1}^{\ell}\alpha_{i}\left[y_{i}-\left(\sum_{j=1}^{\ell}\frac{\alpha_{j}^{*}-\alpha_{j}}{C^{*}}x_{j}\cdot x_{i}\right)+\varepsilon\right]-\sum_{i=1}^{\ell}\alpha_{i}^{*}\left[\left(\sum_{i=j}^{\ell}\frac{\alpha_{j}^{*}-\alpha_{j}}{C^{*}}x_{j}\cdot x_{i}\right)-y_{i}+\varepsilon\right]\\
 & \qquad-\frac{C^{*}}{2}(c_{n}-\left(\sum_{j=1}^{\ell}\frac{\alpha_{j}^{*}-\alpha_{j}}{C^{*}}x_{j}\cdot\sum_{i=1}^{\ell}\frac{\alpha_{i}^{*}-\alpha_{i}}{C^{*}}x_{i}\right)\\
= & \sum_{i=1}^{\ell}\alpha_{i}y_{i}-\sum_{i,j=1}^{\ell}\frac{\alpha_{i}(\alpha_{j}^{*}-\alpha_{j})}{C^{*}}x_{j}\cdot x_{i}+\sum_{i=1}^{\ell}\alpha_{i}\varepsilon-\sum_{i,j=1}^{\ell}\frac{\alpha_{i}^{*}(\alpha_{j}^{*}-\alpha_{j})}{C^{*}}x_{j}\cdot x_{i}+\sum_{i=1}^{\ell}\alpha_{i}^{*}y_{i}\\
 & \qquad-\sum_{i=1}^{\ell}\alpha_{i}^{*}\varepsilon-\frac{c_{n}C^{*}}{2}-\frac{1}{2C^{*}}\sum_{i,j=1}^{\ell}\left(\alpha_{j}^{*}-\alpha_{j}\right)\left(\alpha_{i}^{*}-\alpha_{i}\right)\left(x_{i}\cdot x_{j}\right)\\
= & \sum_{i=1}^{\ell}\left(\alpha_{i}y_{i}+\alpha_{i}^{*}y_{i}+\alpha_{i}\varepsilon-\alpha_{i}^{*}\varepsilon\right)-\sum_{i,j=1}^{\ell}\left[\frac{\alpha_{i}(\alpha_{j}^{*}-\alpha_{j})}{C^{*}}x_{j}\cdot x_{i}+\frac{\alpha_{i}^{*}(\alpha_{j}^{*}-\alpha_{j})}{C^{*}}x_{j}\cdot x_{i}\right]\\
 & \qquad-\frac{c_{n}C^{*}}{2}-\frac{1}{2C^{*}}\sum_{i,j=1}^{\ell}\left(\alpha_{j}^{*}-\alpha_{j}\right)\left(\alpha_{i}^{*}-\alpha_{i}\right)\left(x_{i}\cdot x_{j}\right)\\
= & \varepsilon\sum_{i=1}^{\ell}\left(\alpha_{i}+\alpha_{i}^{*}\right)+\sum_{i=1}^{\ell}y_{i}\left(\alpha_{i}^{*}-\alpha_{i}\right)-\frac{1}{2C^{*}}\sum_{i,j=1}^{\ell}\left(\alpha_{j}^{*}-\alpha_{j}\right)\left(\alpha_{i}^{*}-\alpha_{i}\right)\left(x_{i}\cdot x_{j}\right)-\frac{c_{n}C^{*}}{2}\\
 & \qquad-\sum_{i,j=1}^{\ell}\left(\frac{\alpha_{i}(\alpha_{j}^{*}-\alpha_{j})}{C^{*}}x_{j}\cdot x_{i}+\frac{\alpha_{i}^{*}(\alpha_{j}^{*}-\alpha_{j})}{C^{*}}x_{j}\cdot x_{i}\right)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
So far, so good.
 I'm messing up some signs, but basically on track.
 Vapnik manages to get rid of that last line too.
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\sum_{i,j=1}^{\ell}\frac{\alpha_{i}(\alpha_{j}^{*}-\alpha_{j})}{C^{*}}x_{j}\cdot x_{i}+\frac{\alpha_{i}^{*}(\alpha_{j}^{*}-\alpha_{j})}{C^{*}}x_{j}\cdot x_{i}= & \sum_{i,j=1}^{\ell}\frac{x_{j}\cdot x_{i}}{C^{*}}\left(\alpha_{i}(\alpha_{j}^{*}-\alpha_{j})+\alpha_{i}^{*}(\alpha_{j}^{*}-\alpha_{j}\right)\\
= & \sum_{i,j=1}^{\ell}\frac{x_{j}\cdot x_{i}}{C^{*}}\left(\alpha_{i}\alpha_{j}^{*}-\alpha_{i}\alpha_{j}+\alpha_{i}^{*}\alpha_{j}^{*}-\alpha_{i}^{*}\alpha_{j}\right)\\
=\frac{1}{C^{*}} & \sum_{i,j=1}^{\ell}\left(\alpha_{i}+\alpha_{i}^{*}\right)\left(\alpha_{j}^{*}-\alpha_{j}\right)\left(x_{i}\cdot x_{j}\right)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
And there it is.
 Probably messed up some signs along the way, but I'm pretty sure that adds
 to the previous term of the same form.
 In any case, our final optimization problem looks like this;
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
W(\alpha,\alpha^{*},C^{*})= & -\varepsilon\sum_{i=1}^{\ell}\left(\alpha_{i}+\alpha_{i}^{*}\right)+\sum_{i=1}^{\ell}y_{i}\left(\alpha_{i}^{*}-\alpha_{i}\right)\\
 & -\frac{1}{2C^{*}}\sum_{i,j=1}^{\ell}\left(\alpha_{i}^{*}-\alpha_{i}\right)\left(\alpha_{j}^{*}-\alpha_{j}\right)\left(x_{i}\cdot x_{j}\right)-\frac{c_{n}C^{*}}{2}\\
\sum_{i=1}^{\ell}\alpha_{i}^{*}= & \sum_{i=1}^{\ell}\alpha_{i}\\
0\le\alpha_{i}^{*}\le1, & \quad i=1,...,\ell\\
0\le\alpha_{i}\le1, & \quad i=1,...,\ell\\
C^{*}\ge & \ 0\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Moving from a linear decision function to a kernel, we can simply replace
 
\begin_inset Formula $x_{i}\cdot x_{j}$
\end_inset

 with 
\begin_inset Formula $K(x_{i},x_{j})$
\end_inset

.
 In the end, we're going to combine 
\begin_inset Formula $\alpha,\alpha^{*}$
\end_inset

 into a single multiplier 
\begin_inset Formula $\beta$
\end_inset

, so depending on how we define 
\begin_inset Formula $\beta$
\end_inset

 it looks like we can eliminate some of the messy 
\begin_inset Formula $C$
\end_inset

 terms.
 This gives us a final kernel estimation optimization problem of
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
W= & -\varepsilon\sum_{i=1}^{\ell}\left(\alpha_{i}^{*}+\alpha_{i}\right)+\sum_{i=1}^{\ell}y_{i}\left(\alpha_{i}^{*}-\alpha_{i}\right)-\frac{1}{2}\sum_{i,j=1}^{\ell}\left(\alpha_{i}^{*}-\alpha_{i}\right)\left(\alpha_{j}^{*}-\alpha_{j}\right)K(x_{i},x_{j})\\
\sum_{i=1}^{\ell}\alpha_{i}^{*}= & \sum_{i=1}^{\ell}\alpha_{i}\\
0\le\alpha_{i}^{*}\le1, & \quad i=1,...,\ell\\
0\le\alpha_{i}\le1, & \quad i=1,...,\ell\\
C^{*}\ge & \ 0\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Which generates estimates in the form 
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
f(x;\ v,\beta)= & \sum_{i=1}^{N}\beta_{i}K(x,v_{i})+b\\
\beta_{i}= & \alpha_{i}^{*}-\alpha_{i}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Here, 
\begin_inset Formula $v$
\end_inset

 is the set of support vectors, and 
\begin_inset Formula $x$
\end_inset

 is the point at which we wish to determine 
\begin_inset Formula $y$
\end_inset

.
 Not sure how to calculate 
\begin_inset Formula $b$
\end_inset

, but that's a fairly minor issue, I think.
 
\end_layout

\begin_layout Section
Hybrid Estimator
\end_layout

\begin_layout Standard
Starting at the same point, but we now include a summation over multiple
 predictors in the constraints
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
F(\xi,\xi^{*}) & =\sum_{i=1}^{\ell}\xi_{i}^{*}+\sum_{i=1}^{\ell}\xi_{i}\\
y_{i}-\sum_{a}(w^{a}\cdot x_{i}^{a})-b & \le\varepsilon+\xi_{i}^{*}\\
\sum_{a}(w^{a}\cdot x_{i}^{a})+b-y_{i} & \le\varepsilon+\xi_{i}\\
\xi_{i}^{*} & \ge\ 0\\
\xi_{i} & \ge\ 0\\
\sum_{a}(w^{a}\cdot w^{a}) & \le c_{n}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Go see why that constraint exists - I really don't understand the role of
 
\begin_inset Formula $c_{n}$
\end_inset

, or what the inner product means, or where 
\begin_inset Formula $C^{*}$
\end_inset

 comes from (I think it's a Lagrange multiplier).
 One thing to note is that the dot product of two vectors 
\begin_inset Formula $A=[a_{1},...a_{n}],\ B=[b_{1},...,b_{n}]$
\end_inset

 is the sum of the element-wise product 
\begin_inset Formula $A\cdot B=\sum a_{i}b_{i}$
\end_inset

.
 What this means is that if we can construct 
\begin_inset Formula $W=[w^{1},...,w^{n}]$
\end_inset

, than the sum of the dot product of 
\begin_inset Formula $w^{a}$
\end_inset

 is the same as the dot product of 
\begin_inset Formula $W$
\end_inset

, ie 
\begin_inset Formula $\sum_{a}w^{a}\cdot w^{a}=W\cdot W$
\end_inset

.
 So we might be able to eliminate that complexity.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The final constraint isn't exactly clear.
 I want the final result to be centered at 
\begin_inset Formula $b$
\end_inset

, and I don't see any reason to calculate the inner product of all the 
\begin_inset Formula $w^{a}$
\end_inset

, so for now I'll just sum them all up.
 Reconstructing the Lagrange we have
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
L(w,\xi^{*},\xi;\alpha^{*},\alpha,C^{*},\gamma,\gamma^{*}) & =\sum_{i=1}^{\ell}(\xi_{i}^{*}+\xi_{i})-\sum_{i=1}^{\ell}\sum_{a}\alpha_{i}^{a}\left[y_{i}-(w^{a}\cdot x_{i}^{a})-b+\varepsilon+\xi_{i}\right]\\
 & \qquad-\sum_{i=1}^{\ell}\sum_{a}\alpha_{i}^{a*}\left[(w^{a}\cdot x_{i}^{a})+b-y_{i}+\varepsilon+\xi_{i}^{*}\right]-\frac{C^{*}}{2}(c_{n}-\sum_{a}(w^{a}\cdot w^{a}))\\
 & \qquad-\sum_{i=1}^{\ell}(\gamma_{i}^{*}\xi_{i}^{*}+\gamma_{i}\xi_{i})\end{align*}

\end_inset


\end_layout

\begin_layout Standard
One point to consider is that the last term expands out to the following
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
-\sum_{i,j=1}^{\ell}\left(\alpha_{j}^{*}-\alpha_{j}\right)\left(\alpha_{i}^{*}-\alpha_{i}\right)\left(x_{i}\cdot x_{j}\right)= & -\sum_{i,j=1}^{\ell}\left(\alpha_{j}^{*}-\alpha_{j}\right)\left(\alpha_{i}^{*}-\alpha_{i}\right)K(x_{i},x_{j})\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Does this make sense as a summation, something like
\end_layout

\begin_layout Standard
\begin_inset Formula $ $
\end_inset


\begin_inset Formula \[
-\sum_{a}\sum_{i,j=1}^{\ell}\beta_{i}^{a}\beta_{j}^{a}K(x_{i}^{a},x_{j}^{a})\]

\end_inset


\end_layout

\begin_layout Standard
I don't see any obvious reason it wouldn't...
\end_layout

\begin_layout Subsection
What if we treat each 
\begin_inset Formula $w^{a}$
\end_inset

 as an independent variable / constraint?
\end_layout

\begin_layout Standard
Recap: the Lagrange function allows you to express a constrained optimization
 problem as an optimization problem over a single equation.
 This is done by finding the points with zero gradient in the Lagrange functiona
l.
 First, construct the Lagrange functional, then take the partial derivatives.
 Hopefully the partials can be combined in such a way that they can be solved.
 IE; the partials form a series of equations which we wish to solve.
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
F(\xi,\xi^{*}) & =\sum_{i=1}^{\ell}\xi_{i}^{*}+\sum_{i=1}^{\ell}\xi_{i}\\
y_{i}-\sum_{a}(w^{a}\cdot x_{i}^{a})-b & \le\varepsilon+\xi_{i}^{*}\\
\sum_{a}(w^{a}\cdot x_{i}^{a})+b-y_{i} & \le\varepsilon+\xi_{i}\\
(w^{a}\cdot w^{a}) & \le c_{n}^{a}\quad\forall a\\
\sum_{a}c_{n}^{a} & \le\bar{c}_{n}\\
\xi_{i} & \ge\ 0\\
\xi_{i}^{*} & \ge\ 0\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Note that under the KKT conditions, 
\begin_inset Formula $g(x)\le c\rightarrow c-g(x)$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
F(w,\xi,\xi^{*};\ \alpha,\alpha^{*},C,\mathcal{C},\gamma,\gamma^{*})= & \sum_{i=1}^{\ell}\xi_{i}^{*}+\sum_{i=1}^{\ell}\xi_{i}\\
 & \qquad-\sum_{i}\alpha_{i}^{a*}\left(\sum_{a}(w^{a}\cdot x_{i}^{a})+b-y_{i}+\varepsilon+\xi_{i}^{*}\right)\\
 & \qquad-\sum_{i}\alpha_{i}^{a}\left(y_{i}-\sum_{a}(w^{a}\cdot x_{i}^{a})-b+\varepsilon+\xi_{i}\right)\\
 & \qquad-\sum_{a}C^{a}\left(c_{n}^{a}-\left(w^{a}\cdot w^{a}\right)\right)\\
 & \qquad-\mathcal{C}\left(\bar{c}_{n}-\sum_{a}c_{n}^{a}\right)\\
 & \qquad-\sum_{i}\gamma_{i}\xi_{i}-\sum_{i}\gamma_{i}^{*}\xi_{i}^{*}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
In this case, we have 
\begin_inset Formula $w,\xi,\xi^{*}$
\end_inset

 as free variables and the rest are Lagrange multipliers.
 For this next bit, it turns out that 
\begin_inset Formula $\frac{d}{dx}\left(a\cdot b\right)=a'\cdot b+a\cdot b'$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\frac{\partial F}{\partial w^{a}}=0= & -\frac{\partial}{\partial w^{a}}\left[\sum_{i}\alpha_{i}^{a*}\left(\sum_{a}(w^{a}\cdot x_{i}^{a})+b-y_{i}+\varepsilon+\xi_{i}^{*}\right)\right]\\
 & \qquad-\frac{\partial}{\partial w^{a}}\left[\sum_{i}\alpha_{i}^{a}\left(y_{i}-\sum_{a}(w^{a}\cdot x_{i}^{a})-b+\varepsilon+\xi_{i}\right)\right]\\
 & \qquad-\frac{\partial}{\partial w^{a}}\left[\sum_{a}C^{a}\left(c_{n}^{a}-\left(w^{a}\cdot w^{a}\right)\right)\right]\\
= & -\sum_{i}\alpha_{i}^{a*}\frac{\partial}{\partial w^{a}}\left(w^{a}\cdot x_{i}^{a}\right)+\sum_{i}\alpha_{i}^{a}\frac{\partial}{\partial w^{a}}\left(w^{a}\cdot x_{i}^{a}\right)-C^{a}\frac{\partial}{\partial w^{a}}\left(w^{a}\cdot w^{a}\right)\\
= & -\sum_{i}\alpha_{i}^{a*}\left(\left(1\cdot x_{i}^{a}\right)+\left(w^{a}\cdot0\right)\right)+\sum_{i}\alpha_{i}^{a}\left(\left(1\cdot x_{i}^{a}\right)+\left(w^{a}\cdot0\right)\right)\\
 & \qquad-C^{a}\left(\left(1\cdot w^{a}\right)+\left(w^{a}\cdot1\right)\right)\\
= & -\sum_{i}\alpha_{i}^{a*}x_{i}^{a}+\sum_{i}\alpha_{i}^{a}x_{i}^{a}-2C^{a}w^{a}\\
2C^{a}w^{a}= & -\sum_{i}\alpha_{i}^{a*}x_{i}^{a}+\sum_{i}\alpha_{i}^{a}x_{i}^{a}\\
w^{a}= & \sum_{i}\frac{\alpha_{i}^{a}-\alpha_{i}^{a*}}{2C^{a}}x_{i}^{a}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
THAT's the correct derivation.
 Fucking A.
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\frac{\partial F}{\partial\xi}=0= & \frac{\partial}{\partial\xi}\left[\sum_{i}\xi_{i}\right]-\frac{\partial}{\partial\xi}\left[\sum_{i}\alpha_{i}^{a}\left(y_{i}-\sum_{a}(w^{a}\cdot x_{i}^{a})-b+\varepsilon+\xi_{i}\right)\right]-\frac{\partial}{\partial\xi}\left[\sum_{i}\gamma_{i}\xi_{i}\right]\\
= & \ell-\sum_{i}\alpha_{i}^{a}-\sum_{i}\gamma_{i}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\frac{\partial F}{\partial\xi^{*}}=0= & \frac{\partial}{\partial\xi^{*}}\left[\sum_{i}\xi_{i}^{*}\right]-\frac{\partial}{\partial\xi^{*}}\left[\sum_{i}\alpha_{i}^{a*}\left(\sum_{a}(w^{a}\cdot x_{i}^{a})+b-y_{i}+\varepsilon+\xi_{i}^{*}\right)\right]-\frac{\partial}{\partial\xi^{*}}\left[\sum_{i}\gamma_{i}^{*}\xi_{i}^{*}\right]\\
= & \ell-\sum_{i}\alpha_{i}^{a*}-\sum_{i}\gamma_{i}^{*}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\ell-\sum_{i}\alpha_{i}^{a}-\sum_{i}\gamma_{i}= & \ell-\sum_{i}\alpha_{i}^{a*}-\sum_{i}\gamma_{i}^{*}\\
-\sum_{i}\alpha_{i}^{a}-\sum_{i}\gamma_{i}= & \sum_{i}\alpha_{i}^{a*}-\sum_{i}\gamma_{i}^{*}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Same as before, but watch this!
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\frac{\partial F}{\partial b}=0= & -\frac{\partial}{\partial b}\left[\sum_{i}\alpha_{i}^{a*}\left(\sum_{a}(w^{a}\cdot x_{i}^{a})+b-y_{i}+\varepsilon+\xi_{i}^{*}\right)\right]\\
 & \qquad-\frac{\partial}{\partial b}\left[\sum_{i}\alpha_{i}^{a}\left(y_{i}-\sum_{a}(w^{a}\cdot x_{i}^{a})-b+\varepsilon+\xi_{i}\right)\right]\\
= & -\sum_{i}\alpha_{i}^{a*}+\sum_{i}\alpha_{i}^{a}\\
\sum_{i}\alpha_{i}^{a}= & \sum_{i}\alpha_{i}^{a*}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
YEAH BITCHES.
 Putting that into the previous one:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
-\sum_{i}\alpha_{i}^{a}-\sum_{i}\gamma_{i}= & \sum_{i}\alpha_{i}^{a*}-\sum_{i}\gamma_{i}^{*}\\
-\sum_{i}\alpha_{i}^{a}-\sum_{i}\gamma_{i}= & \sum_{i}\alpha_{i}^{a}-\sum_{i}\gamma_{i}^{*}\\
\sum_{i}\gamma_{i}^{*}-\sum_{i}\gamma_{i}= & 2\sum_{i}\alpha_{i}^{a}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Now for 
\begin_inset Formula $C^{a}$
\end_inset

.
 This can probably be expressed with or without the summation.
 The result is the same, it's really a semantic issue.
 Either you have a single constraing defined over a summation or you have
 multiple constraints defined for 
\begin_inset Formula $a$
\end_inset

, in the Lagrange functional the two are interchangeable.
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\frac{\partial F}{\partial C^{a}}=0= & -\frac{\partial}{\partial C^{a}}\left[\sum_{a}C^{a}\left(c_{n}^{a}-\left(w^{a}\cdot w^{a}\right)\right)\right]\\
= & -\sum_{a}\left(c_{n}^{a}-\left(w^{a}\cdot w^{a}\right)\right)\\
\sum_{a}c_{n}^{a}= & \sum_{a}w^{a}\cdot w^{a}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Vapnik mentions 'taking into account the Kuhn Tucker conditions' when eliminatin
g terms from 
\begin_inset Formula $L$
\end_inset

.
\end_layout

\begin_layout Subsubsection
Known Good Equalities
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\sum_{i}\alpha_{i}^{a}= & \sum_{i}\alpha_{i}^{a*}\\
w^{a}= & \sum_{i}\frac{\alpha_{i}^{a}-\alpha_{i}^{a*}}{2C^{a}}x_{i}^{a}\\
\sum_{i}\gamma_{i}^{*}-\sum_{i}\gamma_{i}= & 2\sum_{i}\alpha_{i}^{a}\\
\sum_{a}c_{n}^{a}= & \sum_{a}w^{a}\cdot w^{a}\end{align*}

\end_inset


\end_layout

\end_body
\end_document
