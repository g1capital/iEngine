#LyX 1.6.5 created this file. For more info see http://www.lyx.org/
\lyxformat 345
\begin_document
\begin_header
\textclass article
\use_default_options true
\language english
\inputencoding auto
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\paperfontsize default
\spacing onehalf
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\leftmargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\author "" 
\author "" 
\end_header

\begin_body

\begin_layout Title
Hybrid SVM
\end_layout

\begin_layout Section
Direct-Regression SVM
\end_layout

\begin_layout Standard
The classical SVR problem is framed as finding a function 
\begin_inset Formula $f(x)=(w\cdot x)+b$
\end_inset

.
 While this is useful, it relies on determining the hyperplane 
\begin_inset Formula $w$
\end_inset

 directly.
 A simpler approach would be to use the set of observations directly, and
 to use the kernel function as an averaging tool; 
\begin_inset Formula $f(x)=\sum y_{i}\beta_{i}K(x,x_{i})$
\end_inset

.
 This appraoch has the benefit of being able to use 
\begin_inset Formula $y$
\end_inset

 as a source of information, so the optimization is simplified from finding
 a hyperplane to finding a combination of observations.
 
\end_layout

\begin_layout Standard
Vapnik points out that this function 
\begin_inset Formula $f(x)$
\end_inset

 is generated as a by-product of estimating a conditional PDF using a loss
 function based on the CDF of the distribution and the estimator.
 We'll use his general approach as a starting point, namely his use of bounded
 loss as a constraint.
 Vapnik uses the regularizer 
\begin_inset Formula $\Omega(\beta)=\sum\beta_{i}\beta_{j}K(x_{i},x_{j})$
\end_inset

 as the minimization target, due the the fact that he's trying to establish
 a PDF, and the only way to create sparsity is to eliminate 'similar' points.
 In our case, we're only trying to estimate the regression function, and
 we want to use as few points as possible.
 As a result, we can probaly just minimize 
\begin_inset Formula $\beta$
\end_inset

 - this will give us most compact set of points that generates the hyperplane.
 Hopefully.
 I'm keeping the regularizer notation, but this problem may not even be
 ill-posed.
\end_layout

\begin_layout Standard
Minize
\begin_inset Note Note
status open

\begin_layout Plain Layout
Take a look at Vapnik p.235 - I think this is the 'P' method of regularization,
 and we need to use a different 
\begin_inset Formula $\Omega(f)$
\end_inset

.
 Also p.256 for discussion of what we're using
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
W_{\gamma}(\beta)=\Omega(f)= & \sum_{i=1}^{\ell}\beta_{i}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Subject to
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\sup_{x}\left|\left(A_{\ell}f\right)(x,y)-F_{\ell}(x,y)\right|= & \sigma_{\ell}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Because we're not trying to generate a PDF, there's no need to constrain
 
\begin_inset Formula $\sum\beta$
\end_inset

.
 We can eliminate the supremum function by making the constraing an inequality
 constraint
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\sigma_{\ell}^{i}\ge & \left|\left(A_{\ell}f\right)(x_{i},y_{i})-F_{\ell}(x_{i},y_{i})\right|\quad i=1,...,\ell\\
= & \left|\left(\sum_{j=1}^{\ell}y_{j}\beta_{j}K(x_{i},x_{j})\right)-y_{i}\right|\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We can eliminate the norm by splitting this into positive and negative componten
ts; the different is now upper bounded by 
\begin_inset Formula $\sigma$
\end_inset

 and lower bounded by 
\begin_inset Formula $-\sigma$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\sigma_{\ell}^{i}\ge & \left(\sum_{j=1}^{\ell}y_{j}\beta_{j}K(x_{i},x_{j})\right)-y_{i}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
-\sigma_{\ell}^{i}\le & \left(\sum_{j=1}^{\ell}y_{j}\beta_{j}K(x_{i},x_{j})\right)-y_{i}\\
\sigma_{\ell}^{i}\ge & y_{i}-\left(\sum_{j=1}^{\ell}y_{j}\beta_{j}K(x_{i},x_{j})\right)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
This can be expressed as a linear optimization task quite easily.
 The form is defined as
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\text{Minimize }\quad W(\beta)= & c^{T}\beta\\
\text{Subject to}\quad\qquad G\beta & \preceq h\\
A\beta & =b\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We can formuate our problem as follows:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
c_{i}= & 1\\
G_{i,j}^{+}= & y_{j}K(x_{i},x_{j})\\
G_{i,j}^{-}= & -y_{j}K(x_{i},x_{j})\\
h_{i}^{+}= & \sigma_{\ell}+y_{i}\\
h_{i}^{-}= & \sigma_{\ell}-y_{i}\\
A,b= & \emptyset\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
It's possible that you'll need to use 
\begin_inset Formula $y_{i}K(x_{i},x_{j})$
\end_inset

 - not sure if there's a difference
\end_layout

\end_inset


\end_layout

\begin_layout Section
Hybrid Direct Regression SVM
\end_layout

\begin_layout Standard
The next step is to extend this to generating estimates over observations
 in disjoint subspaces.
 The previous formulation assumes that 
\begin_inset Formula $K(x_{i},x_{j})$
\end_inset

 was valid for all 
\begin_inset Formula $i,j$
\end_inset

.
 We'll not consider cases where this isn't true, or where we would like
 to use different kernel functions in different subspaces.
 To do this we'll, simply sum over each of the disjoint subspaces.
 We'll assume that for a set of subspaces 
\begin_inset Formula $a=(1,...,m)$
\end_inset

, we have 
\begin_inset Formula $\ell$
\end_inset

 observations, and that the conditional variable 
\begin_inset Formula $y$
\end_inset

 is shared between subspaces.
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
W(\beta)=\Omega(f)= & \sum_{a}\sum_{i=1}^{\ell}\beta_{i}^{a}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\sigma_{\ell}^{i}\ge & \left(\sum_{a}\sum_{j=1}^{\ell}y_{j}\beta_{j}^{a}K^{a}(x_{i}^{a},x_{j}^{a})\right)-y_{i}\\
\sigma_{\ell}^{i}\ge & y_{i}-\left(\sum_{a}\sum_{j=1}^{\ell}y_{j}\beta_{j}^{a}K^{a}(x_{i}^{a},x_{j}^{a})\right)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\bar{c}_{i,a}= & 1\\
\bar{G}_{i,j,a}^{+}= & y_{j}K^{a}(x_{i}^{a},x_{j}^{a})\\
\bar{G}_{i,j,a}^{-}= & -y_{j}K^{a}(x_{i}^{a},x_{j}^{a})\\
\bar{h}_{i,a}^{+}= & \sigma_{\ell}+y_{i}\\
\bar{h}_{i,a}^{-}= & \sigma_{\ell}-y_{i}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The quadratic optimization problem requires 
\begin_inset Formula $P$
\end_inset

 to be positive semi-definite.
 We'll do this by constructing the diagonal matrix of 
\begin_inset Formula $P,G$
\end_inset

 and tiling 
\begin_inset Formula $q,h$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
c_{a\ell+i}= & \bar{c}_{i,a}\\
G_{a\ell+i,a\ell+j}= & \bar{G}_{i,j,a}\\
h_{a\ell+i}= & \bar{h}_{i,a}\\
A,b= & \emptyset\end{align*}

\end_inset


\end_layout

\begin_layout Standard
This allows us to optimize over multiple kernel functions and select SV's
 based on a mixture of kernels, selecting the optimal local kernel.
 Alternately, it allows us to restrict the VC Dimension of a set of multi-dimens
ional observations by considering only specific subsets of the dimensions.
 
\end_layout

\begin_layout Subsection
Direct Optimization of Risk
\end_layout

\begin_layout Standard
The only 'hard' parameter is 
\begin_inset Formula $\sigma_{\ell}$
\end_inset

.
 To optimize over 
\begin_inset Formula $\sigma_{\ell}$
\end_inset

, we would need to include the risk functional in the optimization functional.
 
\end_layout

\begin_layout Standard
Let's see if this is feasible.
 One formula for the risk is
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
R(\alpha_{\ell})= & R_{\text{emp}}(\alpha_{\ell})+\frac{BE}{2}\left(1+\sqrt{1+\frac{4R_{\text{emp}}(\alpha_{\ell})}{BE}}\right)\\
E= & 4\frac{h(\ln\frac{2\ell}{h}+1)-\ln(\eta/4)}{\ell}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
This would require the Taylor expanson of one term
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\sqrt{1+x}= & \sum_{n=0}^{\infty}\frac{(-1)^{n}(2n)!}{(1-2n)(n!)^{2}(4^{n})}x^{n}=1+\frac{1}{2}x-\frac{1}{8}x^{2}+\frac{1}{16}x^{3}+\ldots\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We could concievably approximate this by setting 
\begin_inset Formula $n$
\end_inset

 to some large number, but that doesn't seem ideal.
 There's also the problem of the exponent - we can't handle an exponent
 above 2 with a QP optimization.
 So we could use the first 3 terms, but who knows how well that would work.
 In this case 
\begin_inset Formula $h$
\end_inset

 would become an optimization parameter.
 This would require that we keep the inequality constraint and construct
 a function 
\begin_inset Formula $\varphi(h,\gamma)\rightarrow\sigma$
\end_inset

.
 I think we should probably start by implementing the 'regular' version,
 then see if the Taylor version works.
 If we do this, we'd need to do something like this:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
W(\alpha_{\ell},h)= & \lambda\Omega(f)+(1-\lambda)R(\alpha)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Vapnik says to minimize the Risk, one must find the minimal of the sum of
 the empirical risk and the confidence interval, and to do so me must make
 the VC Dimension a controlling variable.
 This doesn't clarify if the VC Dimension can be optimized in a QP problem
 though.
 
\end_layout

\begin_layout Standard
The reason this won't work is that it requires optimizing 
\begin_inset Formula $h\ln h$
\end_inset

, which doesn't fit into our nice clean QP framework.
 It's possible that we can express the minimization task as a convex twice
 differentiable function, in which case we may be able to use nonlinear
 convex optimization (see CVXOPT).
 It would seem that the risk is both those things (decreases with decreasing
 
\begin_inset Formula $h$
\end_inset

, increasing 
\begin_inset Formula $\ell$
\end_inset

), so maybe this is worth considering once the basics are done.
\end_layout

\begin_layout Section
Original Formulation
\end_layout

\begin_layout Standard
The original formulation was to minimize 
\begin_inset Formula $F$
\end_inset

 subject to five inequality constraints, where 
\begin_inset Formula $\xi$
\end_inset

 is the distance from each point to the hyperplane (minus insensitive zone
 
\begin_inset Formula $\varepsilon$
\end_inset

).
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
F(\xi,\xi^{*}) & =\sum_{i=1}^{\ell}\xi_{i}^{*}+\sum_{i=1}^{\ell}\xi_{i}\\
y_{i}-(w\cdot x_{i})-b & \le\varepsilon+\xi_{i}^{*}\\
(w\cdot x_{i})+b-y_{i} & \le\varepsilon+\xi_{i}\\
\xi_{i}^{*} & \ge\ 0\\
\xi_{i} & \ge\ 0\\
(w\cdot w) & \le c_{n}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
This can be expressed as a Lagrange functional
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
L(w,\xi^{*},\xi;\alpha^{*},\alpha,C^{*},\gamma,\gamma^{*}) & =\sum_{i=1}^{\ell}(\xi_{i}^{*}+\xi_{i})-\sum_{i=1}^{\ell}\alpha_{i}\left[y_{i}-(w\cdot x_{i})-b+\varepsilon+\xi_{i}\right]\\
 & \qquad-\sum_{i=1}^{\ell}\alpha_{i}^{*}\left[(w\cdot x_{i})+b-y_{i}+\varepsilon+\xi_{i}^{*}\right]-\frac{C^{*}}{2}(c_{n}-(w\cdot w))\\
 & \qquad-\sum_{i=1}^{\ell}(\gamma_{i}^{*}\xi_{i}^{*}+\gamma_{i}\xi_{i})\end{align*}

\end_inset


\end_layout

\begin_layout Standard
To find a maximum of the Lagrange function, we set the partial derivative
 over the variables we want to maximize to 0, which should yield a system
 of partial derivatives over the other variables.
 This is due to the fact that if we're at a maximum, the derivative should
 be 0.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\frac{\partial F}{\partial w}=0 & =-\frac{\partial}{\partial w}\left(\sum_{i=1}^{\ell}\alpha_{i}\left[y_{i}-(w\cdot x_{i})-b+\varepsilon+\xi_{i}\right]\right)\\
 & \qquad-\frac{\partial}{\partial w}\left(\sum_{i=1}^{\ell}\alpha_{i}^{*}\left[(w\cdot x_{i})+b-y_{i}+\varepsilon+\xi_{i}\right]\right)-\frac{\partial}{\partial w}\left(\frac{C^{*}}{2}(c_{n}-(w\cdot w))\right)\\
 & =\sum_{i=1}^{\ell}\alpha_{i}x_{i}-\sum_{i=1}^{\ell}\alpha_{i}^{*}x_{i}-wC^{*}\\
wC^{*} & =\sum_{i=1}^{\ell}(\alpha_{i}-\alpha_{i}^{*})x_{i}\\
w & =\sum_{i=1}^{\ell}\frac{\alpha_{i}-\alpha_{i}^{*}}{C^{*}}x_{i}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\frac{\partial F}{\partial\xi^{*}}=0 & =\frac{\partial}{\partial\xi^{*}}\left(\sum_{i=1}^{\ell}(\xi_{i}^{*}+\xi_{i})\right)-\frac{\partial}{\partial\xi^{*}}\left(\sum_{i=1}^{\ell}\alpha_{i}^{*}\left[(w\cdot x_{i})+b-y_{i}+\varepsilon+\xi_{i}^{*}\right]\right)-\frac{\partial}{\partial\xi^{*}}\left(\sum_{i=1}^{\ell}(\gamma_{i}^{*}\xi_{i}^{*}+\gamma_{i}\xi_{i})\right)\\
 & =1-\sum_{i=1}^{\ell}\alpha_{i}^{*}-\sum_{i=1}^{\ell}\gamma_{i}^{*}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\frac{\partial F}{\partial\xi}=0 & =\frac{\partial}{\partial\xi}\left(\sum_{i=1}^{\ell}(\xi_{i}^{*}+\xi_{i})\right)-\frac{\partial}{\partial\xi}\left(\sum_{i=1}^{\ell}\alpha_{i}\left[y_{i}-(w\cdot x_{i})-b+\varepsilon+\xi_{i}\right]\right)-\frac{\partial}{\partial\xi}\left(\sum_{i=1}^{\ell}(\gamma_{i}^{*}\xi_{i}^{*}+\gamma_{i}\xi_{i})\right)\\
 & =1-\sum_{i=1}^{\ell}\alpha_{i}-\sum_{i=1}^{\ell}\gamma_{i}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The last two can be combined
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
1-\sum_{i=1}^{\ell}\alpha_{i}^{*}-\sum_{i=1}^{\ell}\gamma_{i}^{*} & =1-\sum_{i=1}^{\ell}\alpha_{i}^{*}-\sum_{i=1}^{\ell}\gamma_{i}^{*}\\
\sum_{i=1}^{\ell}\alpha_{i}^{*}+\sum_{i=1}^{\ell}\gamma_{i}^{*} & =\sum_{i=1}^{\ell}\alpha_{i}^{*}+\sum_{i=1}^{\ell}\gamma_{i}^{*}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Vapnik makes second term on each side go away somehow, leaving the following
 two conditions:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
w= & \sum_{i=1}^{\ell}\frac{\alpha_{i}^{*}-\alpha_{i}}{C^{*}}x_{i}\\
\sum_{i=1}^{\ell}\alpha_{i}^{*}= & \sum_{i=1}^{\ell}\alpha_{i}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Next, he puts those into the original lagrange (WTF?), apparently by eliminating
 the variables we just took partials for and substituting for 
\begin_inset Formula $w$
\end_inset


\begin_inset Formula \begin{align*}
W(\alpha^{*},\alpha,C^{*})= & \sum_{i=1}^{\ell}\alpha_{i}\left[y_{i}-(w\cdot x_{i})+\varepsilon\right]-\sum_{i=1}^{\ell}\alpha_{i}^{*}\left[(w\cdot x_{i})-y_{i}+\varepsilon\right]-\frac{C^{*}}{2}(c_{n}-(w\cdot w))\\
= & \sum_{i=1}^{\ell}\alpha_{i}\left[y_{i}-\left(\sum_{j=1}^{\ell}\frac{\alpha_{j}^{*}-\alpha_{j}}{C^{*}}x_{j}\cdot x_{i}\right)+\varepsilon\right]-\sum_{i=1}^{\ell}\alpha_{i}^{*}\left[\left(\sum_{i=j}^{\ell}\frac{\alpha_{j}^{*}-\alpha_{j}}{C^{*}}x_{j}\cdot x_{i}\right)-y_{i}+\varepsilon\right]\\
 & \qquad-\frac{C^{*}}{2}(c_{n}-\left(\sum_{j=1}^{\ell}\frac{\alpha_{j}^{*}-\alpha_{j}}{C^{*}}x_{j}\cdot\sum_{i=1}^{\ell}\frac{\alpha_{i}^{*}-\alpha_{i}}{C^{*}}x_{i}\right)\\
= & \sum_{i=1}^{\ell}\alpha_{i}y_{i}-\sum_{i,j=1}^{\ell}\frac{\alpha_{i}(\alpha_{j}^{*}-\alpha_{j})}{C^{*}}x_{j}\cdot x_{i}+\sum_{i=1}^{\ell}\alpha_{i}\varepsilon-\sum_{i,j=1}^{\ell}\frac{\alpha_{i}^{*}(\alpha_{j}^{*}-\alpha_{j})}{C^{*}}x_{j}\cdot x_{i}+\sum_{i=1}^{\ell}\alpha_{i}^{*}y_{i}\\
 & \qquad-\sum_{i=1}^{\ell}\alpha_{i}^{*}\varepsilon-\frac{c_{n}C^{*}}{2}-\frac{1}{2C^{*}}\sum_{i,j=1}^{\ell}\left(\alpha_{j}^{*}-\alpha_{j}\right)\left(\alpha_{i}^{*}-\alpha_{i}\right)\left(x_{i}\cdot x_{j}\right)\\
= & \sum_{i=1}^{\ell}\left(\alpha_{i}y_{i}+\alpha_{i}^{*}y_{i}+\alpha_{i}\varepsilon-\alpha_{i}^{*}\varepsilon\right)-\sum_{i,j=1}^{\ell}\left[\frac{\alpha_{i}(\alpha_{j}^{*}-\alpha_{j})}{C^{*}}x_{j}\cdot x_{i}+\frac{\alpha_{i}^{*}(\alpha_{j}^{*}-\alpha_{j})}{C^{*}}x_{j}\cdot x_{i}\right]\\
 & \qquad-\frac{c_{n}C^{*}}{2}-\frac{1}{2C^{*}}\sum_{i,j=1}^{\ell}\left(\alpha_{j}^{*}-\alpha_{j}\right)\left(\alpha_{i}^{*}-\alpha_{i}\right)\left(x_{i}\cdot x_{j}\right)\\
= & \varepsilon\sum_{i=1}^{\ell}\left(\alpha_{i}+\alpha_{i}^{*}\right)+\sum_{i=1}^{\ell}y_{i}\left(\alpha_{i}^{*}-\alpha_{i}\right)-\frac{1}{2C^{*}}\sum_{i,j=1}^{\ell}\left(\alpha_{j}^{*}-\alpha_{j}\right)\left(\alpha_{i}^{*}-\alpha_{i}\right)\left(x_{i}\cdot x_{j}\right)-\frac{c_{n}C^{*}}{2}\\
 & \qquad-\sum_{i,j=1}^{\ell}\left(\frac{\alpha_{i}(\alpha_{j}^{*}-\alpha_{j})}{C^{*}}x_{j}\cdot x_{i}+\frac{\alpha_{i}^{*}(\alpha_{j}^{*}-\alpha_{j})}{C^{*}}x_{j}\cdot x_{i}\right)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
So far, so good.
 I'm messing up some signs, but basically on track.
 Vapnik manages to get rid of that last line too.
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\sum_{i,j=1}^{\ell}\frac{\alpha_{i}(\alpha_{j}^{*}-\alpha_{j})}{C^{*}}x_{j}\cdot x_{i}+\frac{\alpha_{i}^{*}(\alpha_{j}^{*}-\alpha_{j})}{C^{*}}x_{j}\cdot x_{i}= & \sum_{i,j=1}^{\ell}\frac{x_{j}\cdot x_{i}}{C^{*}}\left(\alpha_{i}(\alpha_{j}^{*}-\alpha_{j})+\alpha_{i}^{*}(\alpha_{j}^{*}-\alpha_{j}\right)\\
= & \sum_{i,j=1}^{\ell}\frac{x_{j}\cdot x_{i}}{C^{*}}\left(\alpha_{i}\alpha_{j}^{*}-\alpha_{i}\alpha_{j}+\alpha_{i}^{*}\alpha_{j}^{*}-\alpha_{i}^{*}\alpha_{j}\right)\\
=\frac{1}{C^{*}} & \sum_{i,j=1}^{\ell}\left(\alpha_{i}+\alpha_{i}^{*}\right)\left(\alpha_{j}^{*}-\alpha_{j}\right)\left(x_{i}\cdot x_{j}\right)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
And there it is.
 Probably messed up some signs along the way, but I'm pretty sure that adds
 to the previous term of the same form.
 In any case, our final optimization problem looks like this;
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
W(\alpha,\alpha^{*},C^{*})= & -\varepsilon\sum_{i=1}^{\ell}\left(\alpha_{i}+\alpha_{i}^{*}\right)+\sum_{i=1}^{\ell}y_{i}\left(\alpha_{i}^{*}-\alpha_{i}\right)\\
 & -\frac{1}{2C^{*}}\sum_{i,j=1}^{\ell}\left(\alpha_{i}^{*}-\alpha_{i}\right)\left(\alpha_{j}^{*}-\alpha_{j}\right)\left(x_{i}\cdot x_{j}\right)-\frac{c_{n}C^{*}}{2}\\
\sum_{i=1}^{\ell}\alpha_{i}^{*}= & \sum_{i=1}^{\ell}\alpha_{i}\\
0\le\alpha_{i}^{*}\le1, & \quad i=1,...,\ell\\
0\le\alpha_{i}\le1, & \quad i=1,...,\ell\\
C^{*}\ge & \ 0\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Moving from a linear decision function to a kernel, we can simply replace
 
\begin_inset Formula $x_{i}\cdot x_{j}$
\end_inset

 with 
\begin_inset Formula $K(x_{i},x_{j})$
\end_inset

.
 In the end, we're going to combine 
\begin_inset Formula $\alpha,\alpha^{*}$
\end_inset

 into a single multiplier 
\begin_inset Formula $\beta$
\end_inset

, so depending on how we define 
\begin_inset Formula $\beta$
\end_inset

 it looks like we can eliminate some of the messy 
\begin_inset Formula $C$
\end_inset

 terms.
 This gives us a final kernel estimation optimization problem of
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
W= & -\varepsilon\sum_{i=1}^{\ell}\left(\alpha_{i}^{*}+\alpha_{i}\right)+\sum_{i=1}^{\ell}y_{i}\left(\alpha_{i}^{*}-\alpha_{i}\right)-\frac{1}{2}\sum_{i,j=1}^{\ell}\left(\alpha_{i}^{*}-\alpha_{i}\right)\left(\alpha_{j}^{*}-\alpha_{j}\right)K(x_{i},x_{j})\\
\sum_{i=1}^{\ell}\alpha_{i}^{*}= & \sum_{i=1}^{\ell}\alpha_{i}\\
0\le\alpha_{i}^{*}\le1, & \quad i=1,...,\ell\\
0\le\alpha_{i}\le1, & \quad i=1,...,\ell\\
C^{*}\ge & \ 0\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Which generates estimates in the form 
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
f(x;\ v,\beta)= & \sum_{i=1}^{N}\beta_{i}K(x,v_{i})+b\\
\beta_{i}= & \alpha_{i}^{*}-\alpha_{i}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Here, 
\begin_inset Formula $v$
\end_inset

 is the set of support vectors, and 
\begin_inset Formula $x$
\end_inset

 is the point at which we wish to determine 
\begin_inset Formula $y$
\end_inset

.
 Not sure how to calculate 
\begin_inset Formula $b$
\end_inset

, but that's a fairly minor issue, I think.
 
\end_layout

\begin_layout Section
Hybrid Estimator
\end_layout

\begin_layout Standard
Starting at the same point, but we now include a summation over multiple
 predictors in the constraints
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
F(\xi,\xi^{*}) & =\sum_{i=1}^{\ell}\xi_{i}^{*}+\sum_{i=1}^{\ell}\xi_{i}\\
y_{i}-\sum_{a}(w^{a}\cdot x_{i}^{a})-b & \le\varepsilon+\xi_{i}^{*}\\
\sum_{a}(w^{a}\cdot x_{i}^{a})+b-y_{i} & \le\varepsilon+\xi_{i}\\
\xi_{i}^{*} & \ge\ 0\\
\xi_{i} & \ge\ 0\\
\sum_{a}(w^{a}\cdot w^{a}) & \le c_{n}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Go see why that constraint exists - I really don't understand the role of
 
\begin_inset Formula $c_{n}$
\end_inset

, or what the inner product means, or where 
\begin_inset Formula $C^{*}$
\end_inset

 comes from (I think it's a Lagrange multiplier).
 One thing to note is that the dot product of two vectors 
\begin_inset Formula $A=[a_{1},...a_{n}],\ B=[b_{1},...,b_{n}]$
\end_inset

 is the sum of the element-wise product 
\begin_inset Formula $A\cdot B=\sum a_{i}b_{i}$
\end_inset

.
 What this means is that if we can construct 
\begin_inset Formula $W=[w^{1},...,w^{n}]$
\end_inset

, than the sum of the dot product of 
\begin_inset Formula $w^{a}$
\end_inset

 is the same as the dot product of 
\begin_inset Formula $W$
\end_inset

, ie 
\begin_inset Formula $\sum_{a}w^{a}\cdot w^{a}=W\cdot W$
\end_inset

.
 So we might be able to eliminate that complexity.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The final constraint isn't exactly clear.
 I want the final result to be centered at 
\begin_inset Formula $b$
\end_inset

, and I don't see any reason to calculate the inner product of all the 
\begin_inset Formula $w^{a}$
\end_inset

, so for now I'll just sum them all up.
 Reconstructing the Lagrange we have
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
L(w,\xi^{*},\xi;\alpha^{*},\alpha,C^{*},\gamma,\gamma^{*}) & =\sum_{i=1}^{\ell}(\xi_{i}^{*}+\xi_{i})-\sum_{i=1}^{\ell}\sum_{a}\alpha_{i}^{a}\left[y_{i}-(w^{a}\cdot x_{i}^{a})-b+\varepsilon+\xi_{i}\right]\\
 & \qquad-\sum_{i=1}^{\ell}\sum_{a}\alpha_{i}^{a*}\left[(w^{a}\cdot x_{i}^{a})+b-y_{i}+\varepsilon+\xi_{i}^{*}\right]-\frac{C^{*}}{2}(c_{n}-\sum_{a}(w^{a}\cdot w^{a}))\\
 & \qquad-\sum_{i=1}^{\ell}(\gamma_{i}^{*}\xi_{i}^{*}+\gamma_{i}\xi_{i})\end{align*}

\end_inset


\end_layout

\begin_layout Standard
One point to consider is that the last term expands out to the following
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
-\sum_{i,j=1}^{\ell}\left(\alpha_{j}^{*}-\alpha_{j}\right)\left(\alpha_{i}^{*}-\alpha_{i}\right)\left(x_{i}\cdot x_{j}\right)= & -\sum_{i,j=1}^{\ell}\left(\alpha_{j}^{*}-\alpha_{j}\right)\left(\alpha_{i}^{*}-\alpha_{i}\right)K(x_{i},x_{j})\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Does this make sense as a summation, something like
\end_layout

\begin_layout Standard
\begin_inset Formula $ $
\end_inset


\begin_inset Formula \[
-\sum_{a}\sum_{i,j=1}^{\ell}\beta_{i}^{a}\beta_{j}^{a}K(x_{i}^{a},x_{j}^{a})\]

\end_inset


\end_layout

\begin_layout Standard
I don't see any obvious reason it wouldn't...
\end_layout

\begin_layout Subsection
What if we treat each 
\begin_inset Formula $w^{a}$
\end_inset

 as an independent variable / constraint?
\end_layout

\begin_layout Standard
Recap: the Lagrange function allows you to express a constrained optimization
 problem as an optimization problem over a single equation.
 This is done by finding the points with zero gradient in the Lagrange functiona
l.
 First, construct the Lagrange functional, then take the partial derivatives.
 Hopefully the partials can be combined in such a way that they can be solved.
 IE; the partials form a series of equations which we wish to solve.
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
F(\xi,\xi^{*}) & =\sum_{i=1}^{\ell}\xi_{i}^{*}+\sum_{i=1}^{\ell}\xi_{i}\\
y_{i}-\sum_{a}(w^{a}\cdot x_{i}^{a})-b & \le\varepsilon+\xi_{i}^{*}\\
\sum_{a}(w^{a}\cdot x_{i}^{a})+b-y_{i} & \le\varepsilon+\xi_{i}\\
(w^{a}\cdot w^{a}) & \le c_{n}^{a}\quad\forall a\\
\sum_{a}c_{n}^{a} & \le\bar{c}_{n}\\
\xi_{i} & \ge\ 0\\
\xi_{i}^{*} & \ge\ 0\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Note that under the KKT conditions, 
\begin_inset Formula $g(x)\le c\rightarrow c-g(x)$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
F(w,\xi,\xi^{*};\ \alpha,\alpha^{*},C,\mathcal{C},\gamma,\gamma^{*})= & \sum_{i=1}^{\ell}\xi_{i}^{*}+\sum_{i=1}^{\ell}\xi_{i}\\
 & \qquad-\sum_{i}\alpha_{i}^{a*}\left(\sum_{a}(w^{a}\cdot x_{i}^{a})+b-y_{i}+\varepsilon+\xi_{i}^{*}\right)\\
 & \qquad-\sum_{i}\alpha_{i}^{a}\left(y_{i}-\sum_{a}(w^{a}\cdot x_{i}^{a})-b+\varepsilon+\xi_{i}\right)\\
 & \qquad-\sum_{a}C^{a}\left(c_{n}^{a}-\left(w^{a}\cdot w^{a}\right)\right)\\
 & \qquad-\mathcal{C}\left(\bar{c}_{n}-\sum_{a}c_{n}^{a}\right)\\
 & \qquad-\sum_{i}\gamma_{i}\xi_{i}-\sum_{i}\gamma_{i}^{*}\xi_{i}^{*}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
In this case, we have 
\begin_inset Formula $w,\xi,\xi^{*}$
\end_inset

 as free variables and the rest are Lagrange multipliers.
 For this next bit, it turns out that 
\begin_inset Formula $\frac{d}{dx}\left(a\cdot b\right)=a'\cdot b+a\cdot b'$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\frac{\partial F}{\partial w^{a}}=0= & -\frac{\partial}{\partial w^{a}}\left[\sum_{i}\alpha_{i}^{a*}\left(\sum_{a}(w^{a}\cdot x_{i}^{a})+b-y_{i}+\varepsilon+\xi_{i}^{*}\right)\right]\\
 & \qquad-\frac{\partial}{\partial w^{a}}\left[\sum_{i}\alpha_{i}^{a}\left(y_{i}-\sum_{a}(w^{a}\cdot x_{i}^{a})-b+\varepsilon+\xi_{i}\right)\right]\\
 & \qquad-\frac{\partial}{\partial w^{a}}\left[\sum_{a}C^{a}\left(c_{n}^{a}-\left(w^{a}\cdot w^{a}\right)\right)\right]\\
= & -\sum_{i}\alpha_{i}^{a*}\frac{\partial}{\partial w^{a}}\left(w^{a}\cdot x_{i}^{a}\right)+\sum_{i}\alpha_{i}^{a}\frac{\partial}{\partial w^{a}}\left(w^{a}\cdot x_{i}^{a}\right)-C^{a}\frac{\partial}{\partial w^{a}}\left(w^{a}\cdot w^{a}\right)\\
= & -\sum_{i}\alpha_{i}^{a*}\left(\left(1\cdot x_{i}^{a}\right)+\left(w^{a}\cdot0\right)\right)+\sum_{i}\alpha_{i}^{a}\left(\left(1\cdot x_{i}^{a}\right)+\left(w^{a}\cdot0\right)\right)\\
 & \qquad-C^{a}\left(\left(1\cdot w^{a}\right)+\left(w^{a}\cdot1\right)\right)\\
= & -\sum_{i}\alpha_{i}^{a*}x_{i}^{a}+\sum_{i}\alpha_{i}^{a}x_{i}^{a}-2C^{a}w^{a}\\
2C^{a}w^{a}= & -\sum_{i}\alpha_{i}^{a*}x_{i}^{a}+\sum_{i}\alpha_{i}^{a}x_{i}^{a}\\
w^{a}= & \sum_{i}\frac{\alpha_{i}^{a}-\alpha_{i}^{a*}}{2C^{a}}x_{i}^{a}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
THAT's the correct derivation.
 Fucking A.
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\frac{\partial F}{\partial\xi}=0= & \frac{\partial}{\partial\xi}\left[\sum_{i}\xi_{i}\right]-\frac{\partial}{\partial\xi}\left[\sum_{i}\alpha_{i}^{a}\left(y_{i}-\sum_{a}(w^{a}\cdot x_{i}^{a})-b+\varepsilon+\xi_{i}\right)\right]-\frac{\partial}{\partial\xi}\left[\sum_{i}\gamma_{i}\xi_{i}\right]\\
= & \ell-\sum_{i}\alpha_{i}^{a}-\sum_{i}\gamma_{i}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\frac{\partial F}{\partial\xi^{*}}=0= & \frac{\partial}{\partial\xi^{*}}\left[\sum_{i}\xi_{i}^{*}\right]-\frac{\partial}{\partial\xi^{*}}\left[\sum_{i}\alpha_{i}^{a*}\left(\sum_{a}(w^{a}\cdot x_{i}^{a})+b-y_{i}+\varepsilon+\xi_{i}^{*}\right)\right]-\frac{\partial}{\partial\xi^{*}}\left[\sum_{i}\gamma_{i}^{*}\xi_{i}^{*}\right]\\
= & \ell-\sum_{i}\alpha_{i}^{a*}-\sum_{i}\gamma_{i}^{*}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\ell-\sum_{i}\alpha_{i}^{a}-\sum_{i}\gamma_{i}= & \ell-\sum_{i}\alpha_{i}^{a*}-\sum_{i}\gamma_{i}^{*}\\
-\sum_{i}\alpha_{i}^{a}-\sum_{i}\gamma_{i}= & \sum_{i}\alpha_{i}^{a*}-\sum_{i}\gamma_{i}^{*}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Same as before, but watch this!
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\frac{\partial F}{\partial b}=0= & -\frac{\partial}{\partial b}\left[\sum_{i}\alpha_{i}^{a*}\left(\sum_{a}(w^{a}\cdot x_{i}^{a})+b-y_{i}+\varepsilon+\xi_{i}^{*}\right)\right]\\
 & \qquad-\frac{\partial}{\partial b}\left[\sum_{i}\alpha_{i}^{a}\left(y_{i}-\sum_{a}(w^{a}\cdot x_{i}^{a})-b+\varepsilon+\xi_{i}\right)\right]\\
= & -\sum_{i}\alpha_{i}^{a*}+\sum_{i}\alpha_{i}^{a}\\
\sum_{i}\alpha_{i}^{a}= & \sum_{i}\alpha_{i}^{a*}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
YEAH BITCHES.
 Putting that into the previous one:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
-\sum_{i}\alpha_{i}^{a}-\sum_{i}\gamma_{i}= & \sum_{i}\alpha_{i}^{a*}-\sum_{i}\gamma_{i}^{*}\\
-\sum_{i}\alpha_{i}^{a}-\sum_{i}\gamma_{i}= & \sum_{i}\alpha_{i}^{a}-\sum_{i}\gamma_{i}^{*}\\
\sum_{i}\gamma_{i}^{*}-\sum_{i}\gamma_{i}= & 2\sum_{i}\alpha_{i}^{a}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Now for 
\begin_inset Formula $C^{a}$
\end_inset

.
 This can probably be expressed with or without the summation.
 The result is the same, it's really a semantic issue.
 Either you have a single constraing defined over a summation or you have
 multiple constraints defined for 
\begin_inset Formula $a$
\end_inset

, in the Lagrange functional the two are interchangeable.
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\frac{\partial F}{\partial C^{a}}=0= & -\frac{\partial}{\partial C^{a}}\left[\sum_{a}C^{a}\left(c_{n}^{a}-\left(w^{a}\cdot w^{a}\right)\right)\right]\\
= & -\sum_{a}\left(c_{n}^{a}-\left(w^{a}\cdot w^{a}\right)\right)\\
\sum_{a}c_{n}^{a}= & \sum_{a}w^{a}\cdot w^{a}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Vapnik mentions 'taking into account the Kuhn Tucker conditions' when eliminatin
g terms from 
\begin_inset Formula $L$
\end_inset

.
\end_layout

\begin_layout Subsubsection
Known Good Equalities
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\sum_{i}\alpha_{i}^{a}= & \sum_{i}\alpha_{i}^{a*}\\
w^{a}= & \sum_{i}\frac{\alpha_{i}^{a}-\alpha_{i}^{a*}}{2C^{a}}x_{i}^{a}\\
\sum_{i}\gamma_{i}^{*}-\sum_{i}\gamma_{i}= & 2\sum_{i}\alpha_{i}^{a}\\
\sum_{a}c_{n}^{a}= & \sum_{a}w^{a}\cdot w^{a}\end{align*}

\end_inset


\end_layout

\end_body
\end_document
