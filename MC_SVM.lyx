#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing onehalf
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Modulus of Continuity SVM
\end_layout

\begin_layout Section
Modulus of Continuity
\end_layout

\begin_layout Standard
We can establish bounds on the Risk using the modulus of continuity
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
R(f_{n})_{L_{1}}\le & R_{\text{emp}}(f_{n})_{L_{1}}+\frac{1}{\ell^{2}}\sum_{i,j=1}^{\ell}\left(\left|y_{i}-y_{j}\right|+\left|f_{n}(x_{i})-f_{n}(x_{j})\right|\right)\\
 & \qquad+\left(\omega(f_{n},h_{0})+C\right)\sqrt{\frac{1}{2\ell}\ln\frac{2}{\delta}}\\
C= & \left|f_{n}(x_{0})-f_{n}(x_{0}^{\prime})\right|+2\left\Vert f\right\Vert _{\infty}+2\sigma_{\varepsilon}\sqrt{\frac{1}{\delta}}\qquad x_{0},x_{0}^{\prime}\in[x_{1},...,x_{\ell}]\\
\omega(f,h)= & \max_{x,x+t\in X,|t|\le h}\left|f(x+t)-f(x)\right|\\
\min_{1\le i\le\ell}d(x_{i},x_{0})\le & h_{0}\le D(x_{1},...,x_{\ell})\\
d(x_{i},x) & =\left|x-x_{i}\right|\\
D(x_{1},...,x_{\ell}) & =\sup_{x\in X}\inf_{1\le i\le\ell}d(x_{i},x)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\left\Vert f\right\Vert _{\infty}$
\end_inset

 is the supremum norm of 
\begin_inset Formula $f$
\end_inset

, 
\begin_inset Formula $\delta\in[0,1]$
\end_inset

 is a confidence parameter, and 
\begin_inset Formula $\sigma_{\varepsilon}$
\end_inset

 is the variance of the noise.
 Since our objective is model selection (rather than estimating 
\begin_inset Formula $R$
\end_inset

 explicitly), we will eliminate terms which are unaffected by the choice
 of 
\begin_inset Formula $f_{n}$
\end_inset

, producing the MCIC decision function
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\text{MCIC}(f_{n})= & R_{\text{emp}}(f_{n})_{L_{1}}+\frac{\omega(f_{n},h_{0})}{3}\sqrt{\frac{1}{2N}\ln\frac{1}{\delta}}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We would like to frame the estimation problem as a convex optimization of
 the MCIC given a class of functions 
\begin_inset Formula $f$
\end_inset

 which implement regression.
 As such, we cannot use the given definition of 
\begin_inset Formula $\omega$
\end_inset

, as it requires determining the maximum of the difference between 
\begin_inset Formula $f$
\end_inset

 at different times.
 Instead, we can observe that the modulus of continuity determines a lower
 bound on a related quantity
\end_layout

\begin_layout Standard
\begin_inset Formula $ $
\end_inset


\begin_inset Formula 
\begin{align*}
\omega(f,h)\le\omega_{L_{1}}^{\prime}(f,h)= & \sum_{x\in X}\sum_{x+t\in X,t\le h}\left|f(x+t)-f(x)\right|
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
That is, the sum of all 
\begin_inset Formula $f(x+t)-f(x)$
\end_inset

 in 
\begin_inset Formula $X$
\end_inset

 where 
\begin_inset Formula $t\le h$
\end_inset

 is greater than 
\begin_inset Formula $\omega$
\end_inset

.
 This is obvious due to the fact that 
\begin_inset Formula $\omega\subset\omega^{\prime}$
\end_inset

.
 As a result, we can state the following:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
R(f_{n})_{L_{1}}\le & R_{\text{emp}}(f_{n})_{L_{1}}+\frac{1}{\ell^{2}}\sum_{i,j=1}^{\ell}\left(\left|y_{i}-y_{j}\right|+\left|f_{n}(x_{i})-f_{n}(x_{j})\right|\right)\\
 & \qquad+\left(\omega^{\prime}(f_{n},h_{0})+C\right)\sqrt{\frac{1}{2\ell}\ln\frac{2}{\delta}}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
In order to eliminate the absolute value operator, we'll consider the 
\begin_inset Formula $L_{2}$
\end_inset

 norm, giving us the following
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\omega(f,h)\le\omega_{L_{2}}^{\prime}(f,h)= & \sum_{x\in X}\sum_{x+t\in X,t\le h}\left(f(x+t)-f(x)\right)^{2}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
R(f_{n})_{L_{2}}\le & R_{\text{emp}}(f_{n})_{L_{2}}+\frac{1}{\ell^{2}}\sum_{i,j=1}^{\ell}\left(\left|y_{i}-y_{j}\right|_{L_{2}}+\left|f_{n}(x_{i})-f_{n}(x_{j})\right|_{L_{2}}\right)\\
 & \qquad+\left(\omega_{L_{2}}^{\prime}(f_{n},h_{0})+C\right)\sqrt{\frac{1}{2\ell}\ln\frac{2}{\delta}}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Which generates an equivalent decision function we'll call MCIC' (we'll
 assume that 
\begin_inset Formula $\omega^{\prime}=\omega_{L_{2}}^{\prime}$
\end_inset

 going forward)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\text{MCIC}^{\prime}(f_{n}) & =R_{\text{emp}}(f_{n})_{L_{2}}+\frac{\omega^{\prime}(f_{n},h_{0})}{3}\sqrt{\frac{1}{2N}\ln\frac{1}{\delta}}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
One challenge with this approach is that the risk bounds are clearly larger
 than necessary, due to the addition of elements in 
\begin_inset Formula $\omega^{\prime}$
\end_inset

 which are not the maximum.
\end_layout

\begin_layout Section
Optimization problem
\end_layout

\begin_layout Standard
We'll start with Vapnik's conditional PDF formulation again, but rather
 than constraining the loss to 
\begin_inset Formula $\sigma$
\end_inset

, we'll optimize for it directly using the MCIC and the regression function
 (since we don't need to retain the full density).
 We'll need to keep the constraint which forces the problem to be a density,
 even if the resulting density isn't the original density.
 This seems to be required for the regression function to hold
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\varphi(x)= & \sum_{i=1}^{\ell}y_{i}\beta_{i}K(x,x_{i})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\text{Minimize}\quad W(\beta)= & R_{\text{emp }}(\varphi)_{L_{2}}+\frac{\omega^{\prime}(\varphi,h_{0})}{3}\sqrt{\frac{1}{2\ell}\ln\frac{1}{\delta}}\\
\text{Subject To}\qquad1= & \sum_{i=1}^{\ell}\left(\frac{1}{\ell}\sum_{j=1}^{\ell}\beta_{j}K(x_{i},x_{j})\right)\\
\beta_{i}\ge & 0\quad i=1,...,\ell
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
R_{\text{emp }}(\varphi)_{L_{2}}= & \sum_{i=1}^{\ell}\left(y_{i}-\sum_{j=1}^{\ell}y_{j}\beta_{j}K(x_{i},x_{j})\right)^{2}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\omega^{\prime}(\varphi,h_{0})= & \sum_{i,j=1}^{\ell}\left(\varphi(x_{i})-\varphi(x_{j})\right)^{2}\theta_{h_{0}}(|x_{i}-x_{j}|)\\
\theta_{h_{0}}(x)= & \begin{cases}
1 & x\le h_{0}\\
0 & x>h_{0}
\end{cases}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
R_{\text{emp }}(\varphi)_{L_{2}}= & \sum_{i=1}^{\ell}\left(y_{i}-\sum_{j=1}^{\ell}y_{j}\beta_{j}K(x_{i},x_{j})\right)^{2}\\
= & \sum_{i=1}^{\ell}\left(y_{i}-\sum_{j=1}^{\ell}y_{j}\beta_{j}K(x_{i},x_{j})\right)\left(y_{i}-\sum_{j=1}^{\ell}y_{j}\beta_{j}K(x_{i},x_{j})\right)\\
= & \sum_{i=1}^{\ell}\left(y_{i}^{2}-2y_{i}\sum_{j=1}^{\ell}y_{j}\beta_{j}K(x_{i},x_{j})+\sum_{j,k=1}^{\ell}y_{j}y_{k}\beta_{j}\beta_{k}K(x_{i},x_{j})K(x_{i},x_{k})\right)\\
= & \sum_{i=1}^{\ell}y_{i}^{2}-2\sum_{i,j=1}^{\ell}y_{i}y_{j}\beta_{j}K(x_{i},x_{j})+\sum_{i,j,k=1}^{\ell}y_{j}y_{k}\beta_{j}\beta_{k}K(x_{i},x_{j})K(x_{i},x_{k})\\
= & \sum_{j,k=1}^{\ell}\beta_{j}\beta_{k}\sum_{i=1}^{\ell}y_{j}y_{k}K(x_{i},x_{j})K(x_{i},x_{k})-\sum_{j=1}^{\ell}\beta_{j}\sum_{i=1}^{\ell}2y_{i}y_{j}K(x_{i},x_{j})+\sum_{i=1}^{\ell}y_{i}^{2}\\
= & \sum_{i,j=1}^{\ell}\beta_{i}\beta_{j}\sum_{k=1}^{\ell}y_{i}y_{j}K(x_{i},x_{k})K(x_{j},x_{k})-\sum_{i=1}^{\ell}\beta_{i}\sum_{j=1}^{\ell}2y_{i}y_{j}K(x_{i},x_{j})+\sum_{i=1}^{\ell}y_{i}^{2}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\omega^{\prime}(\varphi,h_{0})= & \sum_{i,j=1}^{\ell}\left(\varphi(x_{i})-\varphi(x_{j})\right)^{2}\theta_{h_{0}}(|x_{i}-x_{j}|)\\
= & \sum_{i,j=1}^{\ell}\left(\varphi(x_{i})^{2}-2\varphi(x_{i})\varphi(x_{j})+\varphi(x_{j})^{2}\right)\theta_{h_{0}}(|x_{i}-x_{j}|)\\
= & \sum_{i,j=1}^{\ell}\left(\sum_{k=1}^{\ell}y_{k}\beta_{k}K(x_{i},x_{k})\sum_{l=1}^{\ell}y_{l}\beta_{l}K(x_{i},x_{l})-2\sum_{k=1}^{\ell}y_{k}\beta_{k}K(x_{i},x_{k})\sum_{l=1}^{\ell}y_{l}\beta_{l}K(x_{j},x_{l})+\sum_{k=1}y_{k}\beta_{k}K(x_{j},x_{k})\sum_{l=1}^{\ell}y_{l}\beta_{l}K(x_{j},x_{l})\right)\theta_{h_{0}}(|x_{i}-x_{j}|)\\
= & \sum_{i,j=1}^{\ell}\left(\sum_{k,l=1}^{\ell}y_{k}y_{l}\beta_{k}\beta_{l}K(x_{i},x_{k})K(x_{i},x_{l})-2\sum_{k,l=1}^{\ell}y_{k}y_{l}\beta_{k}\beta_{l}K(x_{i},x_{k})K(x_{j},x_{l})+\sum_{k,l=1}^{\ell}y_{k}y_{l}\beta_{k}\beta_{l}K(x_{j},x_{k})K(x_{j},x_{l})\right)\theta_{h_{0}}(|x_{i}-x_{j}|)\\
= & \sum_{k,l=1}^{\ell}\beta_{k}\beta_{l}\left(\sum_{i,j=1}^{\ell}\left(y_{k}y_{l}K(x_{i},x_{k})K(x_{i},x_{l})-2y_{k}y_{l}K(x_{i}x_{k})K(x_{j},x_{l})+y_{k}y_{l}K(x_{j},x_{k})K(x_{j},x_{l})\right)\theta_{h_{0}}(|x_{i}-x_{j}|)\right)\\
= & \sum_{k,l=1}^{\ell}\beta_{k}\beta_{l}\left(\sum_{i,j=1}^{\ell}\theta_{h_{0}}(|x_{i}-x_{j}|)y_{k}y_{l}\left(K(x_{i},x_{k})K(x_{i},x_{l})-2K(x_{i}x_{k})K(x_{j},x_{l})+K(x_{j},x_{k})K(x_{j},x_{l})\right)\right)\\
= & \sum_{i,j=1}^{\ell}\beta_{i}\beta_{j}\left(\sum_{k,l=1}^{\ell}\theta_{h_{0}}(|x_{k}-x_{l}|)y_{i}y_{j}\left(K(x_{i},x_{k})K(x_{j},x_{k})-2K(x_{i}x_{k})K(x_{j},x_{l})+K(x_{i},x_{l})K(x_{j},x_{l})\right)\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
1= & \sum_{i=1}^{\ell}\left(\frac{1}{\ell}\sum_{j=1}^{\ell}\beta_{j}K(x_{i},x_{j})\right)\\
= & \sum_{j=1}^{\ell}\beta_{j}\sum_{i=1}^{\ell}\frac{1}{\ell}K(x_{i},x_{j})\\
= & \sum_{i=1}^{\ell}\beta_{i}\sum_{j=1}^{\ell}\frac{1}{\ell}K(x_{i},x_{j})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
W(\beta)= & R_{\text{emp }}(\varphi)_{L_{2}}+\frac{\omega^{\prime}(\varphi,h_{0})}{3}\sqrt{\frac{1}{2\ell}\ln\frac{1}{\delta}}\\
= & \sum_{i,j=1}^{\ell}\beta_{i}\beta_{j}\sum_{k=1}^{\ell}y_{i}y_{j}K(x_{i},x_{k})K(x_{j},x_{k})-\sum_{i=1}^{\ell}\beta_{i}\sum_{j=1}^{\ell}2y_{i}y_{j}K(x_{i},x_{j})\\
 & \qquad+\sum_{i,j=1}^{\ell}\beta_{i}\beta_{j}\sum_{k,l=1}^{\ell}\Psi_{i,j,k,l}\frac{y_{i}y_{j}}{3}\sqrt{\frac{1}{2\ell}\ln\frac{1}{\delta}}\\
= & \sum_{i,j=1}^{\ell}\beta_{i}\beta_{j}\sum_{k=1}^{\ell}y_{i}y_{j}K(x_{i},x_{k})K(x_{j},x_{k})-\sum_{i=1}^{\ell}\beta_{i}\sum_{j=1}^{\ell}2y_{i}y_{j}K(x_{i},x_{j})\\
 & \qquad+\sum_{i,j=1}^{\ell}\beta_{i}\beta_{j}\left(\frac{y_{i}y_{j}}{3}\sqrt{\frac{1}{2\ell}\ln\frac{1}{\delta}}\sum_{k,l=1}^{\ell}\Psi_{i,j,k,l}\right)\\
= & \sum_{i,j=1}^{\ell}\beta_{i}\beta_{j}\left(\sum_{k=1}^{\ell}y_{i}y_{j}K(x_{i},x_{k})K(x_{j},x_{k})+\frac{y_{i}y_{j}}{3}\sqrt{\frac{1}{2\ell}\ln\frac{1}{\delta}}\sum_{k,l=1}^{\ell}\Psi_{i,j,k,l}\right)\\
 & \qquad-\sum_{i=1}^{\ell}\beta_{i}\sum_{j=1}^{\ell}2y_{i}y_{j}K(x_{i},x_{j})\\
\Psi_{i,j,k,l}= & \theta_{h_{0}}(|x_{k}-x_{l}|)\left(K(x_{i},x_{k})K(x_{j},x_{k})-2K(x_{i}x_{k})K(x_{j},x_{l})+K(x_{i},x_{l})K(x_{j},x_{l})\right)\\
= & \theta_{h_{0}}(|x_{k}-x_{l}|)\left(2K(x_{i},x_{k})K(x_{j},x_{k})-2K(x_{i},x_{k})K(x_{j},x_{l})\right)\\
= & 2\theta_{h_{0}}(|x_{k}-x_{l}|)K(x_{i},x_{k})\left(K(x_{j},x_{k})-K(x_{j},x_{l})\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\text{Minimize }\quad W(\beta)= & \frac{1}{2}\beta^{T}P\beta+q^{T}\beta\\
\text{Subject to}\quad\qquad G\beta & \preceq h\\
A\beta & =b
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
P_{i,j}= & 2y_{i}y_{j}\Phi_{i,j}+\frac{4y_{i}y_{j}\Psi_{i,j}}{3}\sqrt{\frac{1}{2\ell}\ln\frac{1}{\delta}}\\
q_{i}= & -\sum_{j=1}^{\ell}2y_{i}y_{j}K(x_{i},x_{j})\\
G_{i}= & -1\\
h_{i}= & 0\\
A_{i}= & \sum_{j=1}^{\ell}\frac{1}{\ell}K(x_{i},x_{j})\\
b= & 1\\
\Phi_{i,j}= & \sum_{k=1}^{\ell}K(x_{i},x_{k})K(x_{j},x_{k})\\
\Psi_{i,j}= & \sum_{k,l=1}^{\ell}\theta_{h_{0}}(|x_{k}-x_{l}|)\left(K(x_{i},x_{k})K(x_{j},x_{k})-K(x_{i},x_{k})K(x_{j},x_{l})\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Subsection
Matrix Representation
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
Q_{i,j}= & \sum_{k,l=1}^{\ell}\theta_{h_{0}}(|x_{k}-x_{l}|)y_{i}y_{j}\left(K(x_{i},x_{k})K(x_{j},x_{k})-2K(x_{i}x_{k})K(x_{j},x_{l})+K(x_{i},x_{l})K(x_{j},x_{l})\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Let's take the first term in there, 
\begin_inset Formula $K(x_{i},x_{k})$
\end_inset

.
 We want to sum this in the 
\begin_inset Formula $k$
\end_inset

 direction for each 
\begin_inset Formula $i,j$
\end_inset

 - so we can think of this as the kernel matrix with depth 
\begin_inset Formula $\ell$
\end_inset

, where the depth encodes 
\begin_inset Formula $K(x_{i},x_{k})$
\end_inset

 for each 
\begin_inset Formula $i,j$
\end_inset

 point.
 We'll want to construct that matrix and then sum along the 3rd axis.
 We can expand this some and define the depth points as 
\begin_inset Formula $K(x_{i},x_{k})K(x_{j},x_{k})$
\end_inset

 - this allows us to do the full first term, and the depth issue isn't a
 problem.
 So let's consider how we can accomplish that.
 We can start with the original matrix 
\begin_inset Formula $K$
\end_inset

 with shape 
\begin_inset Formula $(\ell,\ell)$
\end_inset

.
 To get the depth, we want to take a row of that array, turn it depth-wise
 so it stays a row, then tile it down.
 For the other term we do the same thing but with a column, then multiply
 them together.
\end_layout

\begin_layout Standard
Basically, we're going to want to use a 4d array.
 We'll sum along the 3rd axis to get 
\begin_inset Formula $k$
\end_inset

 and the 4th to get 
\begin_inset Formula $l$
\end_inset

.
 We should be able to do a mapping from 
\begin_inset Formula $(\ell_{i},\ell_{j})\rightarrow(\ell_{i},\ell_{j},\ell_{k},\ell_{l})$
\end_inset

 by inserting dimensions and broadcasting, so the first term would be something
 like 
\begin_inset Formula $K_{\ell,1,\ell,1}\cdot K_{1,\ell,\ell,1}$
\end_inset

.
 This broadcasts 
\begin_inset Formula $i$
\end_inset

 across 
\begin_inset Formula $j$
\end_inset

, but keeps 
\begin_inset Formula $k$
\end_inset

 aligned.
 If this works, we should be able to express the matrix as follows
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
Q_{i,j}=y_{i}y_{j} & \sum_{k,l=1}^{\ell}\theta_{h_{0}}(|X_{\ell,1}-X_{1,\ell}|)\left(K_{\ell,1,\ell,1}\cdot K_{1,\ell,\ell,1}-2K_{\ell,1,\ell,1}\cdot K_{1,\ell,1,\ell}+K_{\ell,1,1,\ell}\cdot K_{1,\ell,1,\ell}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Which is the sum along axes 3 and 4 of the resulting matrix (inside the
 summation).
\end_layout

\begin_layout Section
Quadratically Constrained Quadratic Optimization
\end_layout

\begin_layout Standard
We can avoid inflating 
\begin_inset Formula $R$
\end_inset

 by treating 
\begin_inset Formula $\omega$
\end_inset

 as an optimization parameter and constraining the solution by 
\begin_inset Formula $\omega$
\end_inset

.
 Unfortunately, this requires that we be able to incorporate mutiple optimizatio
n parameters in a single constraint.
 QCQP allows us to do this using the 
\begin_inset Formula $L_{2}$
\end_inset

 norm as before.
 We keep the same target functional
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\varphi(x)= & \sum_{i=1}^{\ell}y_{i}\beta_{i}K(x,x_{i})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
But now the optimization problem has an additional parameter 
\begin_inset Formula $\omega_{h_{0}}^{\prime}$
\end_inset

 and a set of new constraints
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\text{Minimize}\quad W(\beta)= & R_{\text{emp }}(\varphi)_{L_{2}}+\frac{\omega_{h_{0}}^{\prime}}{3}\sqrt{\frac{1}{2\ell}\ln\frac{1}{\delta}}\\
\text{Subject To}\qquad1= & \sum_{i=1}^{\ell}\left(\frac{1}{\ell}\sum_{j=1}^{\ell}\beta_{j}K(x_{i},x_{j})\right)\\
\omega_{h_{0}}^{\prime}\ge & \left(\varphi(x_{i})-\varphi(x_{j})\right)^{2}\theta_{h_{0}}(|x_{i}-x_{j}|)\quad\forall i,j\in[1,...,\ell]\\
\beta_{i}\ge & 0\quad i=1,...,\ell
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Where
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
R_{\text{emp }}(\varphi)_{L_{2}}= & \sum_{i=1}^{\ell}\left(y_{i}-\sum_{j=1}^{\ell}y_{j}\beta_{j}K(x_{i},x_{j})\right)^{2}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\theta_{h_{0}}(x)= & \begin{cases}
1 & x\le h_{0}\\
0 & x>h_{0}
\end{cases}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We can expand the inequality constraints the same way as before
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\omega_{h_{0}}^{\prime}= & \left(\varphi(x_{i})-\varphi(x_{j})\right)^{2}\theta_{h_{0}}(|x_{i}-x_{j}|)\\
= & \left(\varphi(x_{i})^{2}-2\varphi(x_{i})\varphi(x_{j})+\varphi(x_{j})^{2}\right)\theta_{h_{0}}(|x_{i}-x_{j}|)\\
= & \left(\sum_{k=1}^{\ell}y_{k}\beta_{k}K(x_{i},x_{k})\sum_{l=1}^{\ell}y_{l}\beta_{l}K(x_{i},x_{l})-2\sum_{k=1}^{\ell}y_{k}\beta_{k}K(x_{i},x_{k})\sum_{l=1}^{\ell}y_{l}\beta_{l}K(x_{j},x_{l})+\sum_{k=1}y_{k}\beta_{k}K(x_{j},x_{k})\sum_{l=1}^{\ell}y_{l}\beta_{l}K(x_{j},x_{l})\right)\theta_{h_{0}}(|x_{i}-x_{j}|)\\
= & \left(\sum_{k,l=1}^{\ell}y_{k}y_{l}\beta_{k}\beta_{l}K(x_{i},x_{k})K(x_{i},x_{l})-2\sum_{k,l=1}^{\ell}y_{k}y_{l}\beta_{k}\beta_{l}K(x_{i},x_{k})K(x_{j},x_{l})+\sum_{k,l=1}^{\ell}y_{k}y_{l}\beta_{k}\beta_{l}K(x_{j},x_{k})K(x_{j},x_{l})\right)\theta_{h_{0}}(|x_{i}-x_{j}|)\\
= & \sum_{k,l=1}^{\ell}\beta_{k}\beta_{l}\left(\left(y_{k}y_{l}K(x_{i},x_{k})K(x_{i},x_{l})-2y_{k}y_{l}K(x_{i}x_{k})K(x_{j},x_{l})+y_{k}y_{l}K(x_{j},x_{k})K(x_{j},x_{l})\right)\theta_{h_{0}}(|x_{i}-x_{j}|)\right)\\
= & \sum_{k,l=1}^{\ell}\beta_{k}\beta_{l}\left(\theta_{h_{0}}(|x_{i}-x_{j}|)y_{k}y_{l}\left(K(x_{i},x_{k})K(x_{i},x_{l})-2K(x_{i}x_{k})K(x_{j},x_{l})+K(x_{j},x_{k})K(x_{j},x_{l})\right)\right)\\
= & \sum_{i,j=1}^{\ell}\beta_{i}\beta_{j}\left(\theta_{h_{0}}(|x_{k}-x_{l}|)y_{i}y_{j}\left(K(x_{i},x_{k})K(x_{j},x_{k})-2K(x_{i}x_{k})K(x_{j},x_{l})+K(x_{i},x_{l})K(x_{j},x_{l})\right)\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Therefore, for we'll establish a set of 
\begin_inset Formula $\ell\times\ell$
\end_inset

 constraints defined as 
\begin_inset Formula $\omega_{h_{0},k,l}^{\prime}$
\end_inset

 for all 
\begin_inset Formula $k$
\end_inset

 and 
\begin_inset Formula $l$
\end_inset

 in 
\begin_inset Formula $[1,...,\ell]$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $ $
\end_inset


\begin_inset Formula 
\begin{align*}
\omega_{h_{0},k,l}^{\prime}= & \sum_{i,j=1}^{\ell}\beta_{i}\beta_{j}\left(\theta_{h_{0}}(|x_{k}-x_{l}|)y_{i}y_{j}\left(K(x_{i},x_{k})K(x_{j},x_{k})-2K(x_{i}x_{k})K(x_{j},x_{l})+K(x_{i},x_{l})K(x_{j},x_{l})\right)\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Section
Linear Optimization
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\varphi(x)= & \sum_{i=1}^{\ell}y_{i}\beta_{i}K(x,x_{i})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
But now the optimization problem has an additional parameter 
\begin_inset Formula $\omega_{h_{0}}^{\prime}$
\end_inset

 and a set of new constraints
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\text{Minimize}\quad W(\beta)= & \mathcal{R}+\mathcal{R}^{*}+\omega\Omega\\
\Omega= & \frac{1}{3}\sqrt{\frac{1}{2\ell}\ln\frac{1}{\delta}}\\
\text{Subject To}\qquad1= & \sum_{i=1}^{\ell}\left(\frac{1}{\ell}\sum_{j=1}^{\ell}\beta_{j}K(x_{i},x_{j})\right)\\
-\mathcal{R}^{*}\le & \sum_{i=1}^{\ell}\left(y_{i}-\sum_{j=1}^{\ell}y_{j}\beta_{j}K(x_{i},x_{j})\right)\le\mathcal{R}\\
\omega\ge & \varphi(x_{i})-\varphi(x_{j})\quad\forall i,j\in[1,...,\ell],\ |x_{i}-x_{j}|\le h_{0}\\
\beta_{i}\ge & 0\quad i=1,...,\ell\\
\mathcal{R}\ge & 0\\
\mathcal{R}^{*}\ge & 0\\
\mathcal{R}= & \mathcal{R}^{*}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Before taking the Lagrangian, let's clean up the constraints a bit
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\text{Minimize}\quad W(\beta)= & \mathcal{R}+\mathcal{R}^{*}+\omega\Omega & \Omega=\frac{1}{3}\sqrt{\frac{1}{2\ell}\ln\frac{1}{\delta}}\\
\\
\text{Subject To}\qquad0= & \sum_{i=1}^{\ell}\left(\frac{1}{\ell}\sum_{j=1}^{\ell}\beta_{j}K(x_{i},x_{j})\right)-1\\
0\ge & -\mathcal{R}^{*}-\sum_{i=1}^{\ell}\left(y_{i}-\sum_{j=1}^{\ell}y_{j}\beta_{j}K(x_{i},x_{j})\right)\\
= & \sum_{i=1}^{\ell}\left(\sum_{j=1}^{\ell}y_{j}\beta_{j}K(x_{i},x_{j})-y_{i}\right)-\mathcal{R}^{*}\\
0\ge & \sum_{i=1}^{\ell}\left(y_{i}-\sum_{j=1}^{\ell}y_{j}\beta_{j}K(x_{i},x_{j})\right)-\mathcal{R}\\
0\ge & \varphi(x_{i})-\varphi(x_{j})-\omega & \forall i,j\in[1,...,\ell],\ |x_{i}-x_{j}|\le h_{0}\\
0\ge & -\beta_{i}\quad & i=1,...,\ell\\
0\ge & -\mathcal{R}\\
0\ge & -\mathcal{R}^{*}\\
0= & \mathcal{R}-\mathcal{R}^{*}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
OK, now we can get to work
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
L= & \mathcal{R}+\mathcal{R}^{*}+\omega\Omega+\lambda^{0}\left(\sum_{i=1}^{\ell}\left(\sum_{j=1}^{\ell}y_{j}\beta_{j}K(x_{i},x_{j})-y_{i}\right)-\mathcal{R}^{*}\right)+\lambda^{1}\left(\sum_{i=1}^{\ell}\left(y_{i}-\sum_{j=1}^{\ell}y_{j}\beta_{j}K(x_{i},x_{j})\right)-\mathcal{R}\right)\\
 & \qquad\qquad+\sum_{|x_{i}-x_{j}|\le h_{0}}\lambda_{i,j}^{2}\left(\varphi(x_{i})-\varphi(x_{j})-\omega\right)-\sum_{i=1}^{\ell}\lambda_{i}^{3}\beta_{i}-\lambda^{4}\mathcal{R}-\lambda^{5}\mathcal{R}^{*}+\lambda^{6}\left(\sum_{i=1}^{\ell}\left(\frac{1}{\ell}\sum_{j=1}^{\ell}\beta_{j}K(x_{i},x_{j})\right)-1\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
That's as correct as I know how to make it.
 Now, we want to minimize with respect to the risk and modulus of continuity,
 so let's start there
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\frac{\partial}{\partial\mathcal{R}}= & 1-\lambda^{1}-\lambda^{4}\\
\frac{\partial}{\partial\mathcal{R}^{*}}= & 1-\lambda^{0}-\lambda^{5}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Not too interesting, let's try 
\begin_inset Formula $\omega$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\frac{\partial}{\partial\omega}= & \Omega-\sum_{|x_{i}-x_{j}|\le h_{0}}\lambda_{i,j}^{2}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
OK, so setting the partials to 0 gives use a couple equalities
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\lambda^{0}+\lambda^{5}= & \lambda^{1}+\lambda^{4}\\
\Omega= & \sum_{|x_{i}-x_{j}|\le h_{0}}\lambda_{i,j}^{2}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Not sure where to go from here.
 I think the conclusion is that instead of optimizing over a set of variables
 
\begin_inset Formula $[\mathcal{R},\mathcal{R}^{*},\omega,\beta_{i}]$
\end_inset

, we should just construct a hybrid optimization variable 
\begin_inset Formula $\alpha=[\mathcal{R},\mathcal{R}^{*},\omega,\beta_{i}]$
\end_inset

, and use zeros in our multipliers to accomplish the same thing.
 This should give us a LP problem subject to simple constraints
\end_layout

\begin_layout Section
Simplified LP
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\varphi(x)= & \sum_{i=1}^{\ell}y_{i}\beta_{i}K(x,x_{i})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\text{Minimize}\quad W(\beta)= & \mathcal{R}\\
\text{Subject To}\qquad1= & \sum_{i=1}^{\ell}\left(\frac{1}{\ell}\sum_{j=1}^{\ell}\beta_{j}K(x_{i},x_{j})\right)\\
-\mathcal{R}^{*}\le & \sum_{i=1}^{\ell}\left(y_{i}-\sum_{j=1}^{\ell}y_{j}\beta_{j}K(x_{i},x_{j})\right)\le\mathcal{R}\\
\beta_{i}\ge & 0\quad i=1,...,\ell\\
\mathcal{R}\ge & 0\\
\mathcal{R}^{*}\ge & 0\\
\mathcal{R}= & \mathcal{R}^{*}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
-\mathcal{R}^{*}\le & \sum_{i=1}^{\ell}\left(y_{i}-\sum_{j=1}^{\ell}y_{j}\beta_{j}K(x_{i},x_{j})\right)\le\mathcal{R}\\
-\mathcal{R}^{*}\le & \sum_{i=1}^{\ell}y_{i}-\sum_{i,j=1}^{\ell}y_{j}\beta_{j}K(x_{i},x_{j})\\
\sum_{i,j=1}^{\ell}y_{j}\beta_{j}K(x_{i},x_{j})-\sum_{i=1}^{\ell}y_{i}-\mathcal{R}^{*}\le & 0
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Which translates into
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\text{Minimize}\quad W(\beta)= & \mathcal{R}\\
\\
\text{Subject To}\qquad1= & \sum_{i=1}^{\ell}\left(\frac{1}{\ell}\sum_{j=1}^{\ell}\beta_{j}K(x_{i},x_{j})\right) & A\\
\sum_{i=1}^{\ell}y_{i}\ge & \sum_{i,j=1}^{\ell}y_{j}\beta_{j}K(x_{i},x_{j})-\mathcal{R}^{*} & B\\
-\sum_{i=1}^{\ell}y_{i}\ge & -\sum_{i,j=1}^{\ell}y_{j}\beta_{j}K(x_{i},x_{j})-\mathcal{R} & C\\
0\ge & -\beta_{i}\quad & D_{i},i=1,...,\ell\\
0\ge & -\mathcal{R} & E\\
0\ge & -\mathcal{R}^{*} & F\\
0= & \mathcal{R}-\mathcal{R}^{*} & G
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We have to keep 
\begin_inset Formula $\mathcal{R}$
\end_inset

 as a variable to keep this as a LP problem, otherwise we'd have to take
 the square loss to avoid the absolute value bit.
 The constraints are labeled so we can clarify the optimization matrices
\end_layout

\begin_layout Standard
\begin_inset Formula $[\beta_{i},\mathcal{R},\mathcal{R}^{*}]$
\end_inset


\end_layout

\begin_layout Subsection
A
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
A^{A}\beta= & \sum_{i=1}^{\ell}\left(\frac{1}{\ell}\sum_{j=1}^{\ell}\beta_{j}K(x_{i},x_{j})\right)\\
= & \sum_{j=1}^{\ell}\beta_{j}\sum_{i=1}^{\ell}\frac{1}{\ell}K(x_{i},x_{j})\\
A_{1,j}^{A}= & \sum_{i=1}^{\ell}\frac{1}{\ell}K(x_{i},x_{j})\\
A_{1,\mathcal{R}}^{A},G_{1,\mathcal{R}^{*}}^{A}= & 0\\
b_{1}^{A}= & 1
\end{align*}

\end_inset


\end_layout

\begin_layout Subsection
B
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
G^{B}\beta= & \sum_{i,j=1}^{\ell}y_{j}\beta_{j}K(x_{i},x_{j})-\mathcal{R}^{*}\\
= & \sum_{j=1}^{\ell}\beta_{j}\sum_{i=1}^{\ell}y_{j}K(x_{i},x_{j})-\mathcal{R}^{*}\\
G_{1,j}^{B}= & y_{j}\sum_{i=1}^{\ell}K(x_{i},x_{j})\\
G_{1,\mathcal{R}^{*}}^{B}= & -1\\
G_{1,\mathcal{R}}^{B}= & 0\\
h_{1}^{B}= & \sum_{i=1}^{\ell}y_{i}
\end{align*}

\end_inset


\end_layout

\begin_layout Subsection
C
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
G^{C}\beta= & -\sum_{i,j=1}^{\ell}y_{j}\beta_{j}K(x_{i},x_{j})-\mathcal{R}\\
= & -\sum_{j=1}^{\ell}\beta_{j}\sum_{i=1}^{\ell}y_{j}K(x_{i},x_{j})-\mathcal{R}\\
G_{1,j}^{C}= & -y_{j}\sum_{i=1}^{\ell}K(x_{i},x_{j})\\
G_{1,\mathcal{R}}^{C}= & -1\\
G_{1,\mathcal{R}^{*}}^{C}= & 0\\
h_{1}^{C}= & -\sum_{i=1}^{\ell}y_{i}
\end{align*}

\end_inset


\end_layout

\begin_layout Subsection
D
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
G_{1,i}^{D_{i}}\beta= & -\beta_{i}\\
G_{1,i}^{D_{i}}= & -1\\
G_{1,\cdot}^{D_{i}}= & 0\\
G_{1,\mathcal{R}}^{C}= & 0\\
G_{1,\mathcal{R}^{*}}^{C}= & 0\\
h_{1}^{D_{i}}= & 0
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The rest of the simple constraints should be obvious from the last example
\end_layout

\begin_layout Section
Simplified QP
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\varphi(x)= & \sum_{i=1}^{\ell}y_{i}\beta_{i}K(x,x_{i})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\text{Minimize}\quad W(\beta)= & \sum_{i=1}^{\ell}\left(y_{i}-\sum_{j=1}^{\ell}y_{j}\beta_{j}K(x_{i},x_{j})\right)^{2}\\
\text{Subject To}\qquad1= & \sum_{i=1}^{\ell}\left(\frac{1}{\ell}\sum_{j=1}^{\ell}\beta_{j}K(x_{i},x_{j})\right)\\
\beta_{i}\ge & 0\quad i=1,...,\ell
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\sum_{i=1}^{\ell}\left(y_{i}-\sum_{j=1}^{\ell}y_{j}\beta_{j}K(x_{i},x_{j})\right)= & \sum_{i=1}^{\ell}\left[y_{i}^{2}-2y_{i}\sum_{j=1}^{\ell}y_{j}\beta_{j}K(x_{i},x_{j})+\left(\sum_{j=1}^{\ell}y_{j}\beta_{j}K(x_{i},x_{j})\right)^{2}\right]\\
= & \sum_{i=1}^{\ell}y_{i}^{2}-2\sum_{i,j=1}^{\ell}y_{i}y_{j}\beta_{j}K(x_{i},x_{j})+\sum_{i,j,k=1}^{\ell}y_{j}y_{k}\beta_{j}\beta_{k}K(x_{i},x_{j})K(x_{i},x_{k})\\
\approx & \sum_{j,k=1}^{\ell}\beta_{j}\beta_{k}\sum_{i=1}^{\ell}y_{j}y_{k}K(x_{i},x_{j})K(x_{i},x_{k})-2\sum_{j=1}^{\ell}\beta_{j}\sum_{i=1}^{\ell}y_{i}y_{j}K(x_{i},x_{j})\\
= & \sum_{i,j=1}^{\ell}\beta_{i}\beta_{j}\sum_{k=1}^{\ell}y_{i}y_{j}K(x_{k},x_{i})K(x_{k},x_{j})-2\sum_{i=1}^{\ell}\beta_{i}\sum_{j=1}^{\ell}y_{i}y_{j}K(x_{j},x_{i})\\
= & \sum_{i,j=1}^{\ell}\beta_{i}\beta_{j}\sum_{k=1}^{\ell}y_{i}y_{j}K(x_{i},x_{k})K(x_{j},x_{k})-2\sum_{i=1}^{\ell}\beta_{i}\sum_{j=1}^{\ell}y_{i}y_{j}K(x_{i},x_{j})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
This give us the following optimization problem
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\text{Minimize }\quad W(\beta)= & \frac{1}{2}\beta^{T}P\beta+q^{T}\beta\\
\text{Subject to}\quad\qquad G\beta & \preceq h\\
A\beta & =b
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
P_{i,j}= & \sum_{k=1}^{\ell}y_{i}y_{j}K(x_{i},x_{k})K(x_{j},x_{k})\\
q_{i}= & -\sum_{j=1}^{\ell}y_{i}y_{j}K(x_{i},x_{j})\\
G= & -\mathbf{I}_{\ell\times\ell}\\
h_{i}= & 0\\
A_{i}= & \sum_{j=1}^{\ell}\frac{1}{\ell}K(x_{i},x_{j})\\
b= & 1
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
I think that if we use an RKHS we can simplify the kernel product 
\begin_inset Formula $K(x_{i},x_{k})K(x_{j},x_{k})$
\end_inset

 to 
\begin_inset Formula $K(x_{i},x_{j})$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\text{Minimize }\quad W(\beta)= & \frac{1}{2}\beta^{T}P\beta+q^{T}\beta\\
\text{Subject to}\quad\qquad G\beta & \preceq h\\
A\beta & =b
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
P_{i,j}= & \sum_{k=1}^{\ell}y_{i}y_{j}K(x_{i},x_{j})\\
q_{i}= & -\sum_{j=1}^{\ell}y_{i}y_{j}K(x_{i},x_{j})\\
G= & -\mathbf{I}_{\ell\times\ell}\\
h_{i}= & 0\\
A_{i}= & \sum_{j=1}^{\ell}\frac{1}{\ell}K(x_{i},x_{j})\\
b= & 1
\end{align*}

\end_inset


\end_layout

\begin_layout Section
Modified Density (via Vapnik)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\varphi(x)= & \sum_{i=1}^{\ell}y_{i}\beta_{i}K(x,x_{i})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\text{Minimize}\quad W(\beta)= & \sum_{i,j=1}^{\ell}\beta_{i}\beta_{j}K(x_{j},x_{i})K(y_{j,}y_{i})\\
\text{Subject To}\qquad1= & \sum_{i=1}^{\ell}\left(\frac{1}{\ell}\sum_{j=1}^{\ell}\beta_{j}K(x_{i},x_{j})\right)\\
\sigma^{*}\le & \left[y_{j}-\sum_{i=1}^{\ell}y_{i}\beta_{i}K(x_{j},x_{i})\right]\le\sigma\quad j=1,...,\ell
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\sigma^{*}\le & y_{j}-\sum_{i=1}^{\ell}y_{i}\beta_{i}K(x_{j},x_{i})\\
\sum_{i=1}^{\ell}y_{i}\beta_{i}K(x_{j},x_{i})\le & y_{j}-\sigma^{*}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
y_{j}-\sum_{i=1}^{\ell}y_{i}\beta_{i}K(x_{j},x_{i})\le & \sigma\\
-\sum_{i=1}^{\ell}y_{i}\beta_{i}K(x_{j},x_{i})\le & \sigma-y_{j}
\end{align*}

\end_inset


\end_layout

\begin_layout Section
Bare Bones SVR
\end_layout

\begin_layout Standard
This is the typical SVR formulation, but without using 
\begin_inset Formula $\varepsilon$
\end_inset

-insensitive zones or 
\begin_inset Formula $w\cdot w\le c_{n}$
\end_inset

.
 The idea is that if this works, we can control accuracy using the MCIC
 principle
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\varphi(x)= & \sum_{i=1}^{\ell}(\alpha_{i}-\alpha_{i}^{*})K(x,x_{i})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\text{Minimize}\quad W(\beta)= & \sum_{i=1}^{\ell}y_{i}(\alpha_{i}^{*}-\alpha_{i})-\frac{1}{2}\sum_{i,j=1}^{\ell}(\alpha_{i}^{*}-\alpha_{i})(\alpha_{j}^{*}-\alpha_{j})K(x_{i},x_{j})\\
\\
\text{Subject To}\qquad\sum_{i=1}^{\ell}\alpha_{i}^{*}= & \sum_{i=1}^{\ell}\alpha_{i}\\
0\le\alpha_{i}\le1 & \quad i=1,...,\ell\\
0\le\alpha_{i}^{*}\le1 & \quad i=1,...,\ell
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
W(\beta)= & \sum_{i=1}^{\ell}y_{i}(\alpha_{i}^{*}-\alpha_{i})-\frac{1}{2}\sum_{i,j=1}^{\ell}(\alpha_{i}^{*}-\alpha_{i})(\alpha_{j}^{*}-\alpha_{j})K(x_{i},x_{j})\\
= & \sum_{i=1}^{\ell}y_{i}\alpha_{i}^{*}-\sum_{i=1}^{\ell}y_{i}\alpha_{i}-\frac{1}{2}\sum_{i,j=1}^{\ell}(\alpha_{i}^{*}\alpha_{j}^{*}-\alpha_{i}^{*}\alpha_{j}-\alpha_{i}\alpha_{j}^{*}+\alpha_{i}\alpha_{j})K(x_{i},x_{j})
\end{align*}

\end_inset


\end_layout

\begin_layout Part
Discussion
\end_layout

\begin_layout Standard
None of this is going well.
 My hope was that I could find a way to directly minimize the risk, subject
 to constraints which would ensure a valid target functional.
 None of these attempts are working, either based on finding a hyperplane
 or finding a PDF-based regression function.
 
\end_layout

\begin_layout Standard
One remaining possibility is to modify the PDF function directly (rather
 than trying to use the regression shortcut) and rather than taking a fixed
 
\begin_inset Formula $\sigma$
\end_inset

, we use the modulus of continuity.
 Even this seems difficult, as I'm not sure why the regularizer is being
 used in place of the risk functional or how it can be modified.
\end_layout

\begin_layout Standard
Let's try starting at the beginning.
 Since we're not interested in creating a PDF (for now), there's no reason
 to put an upper bound on our optimization variables.
 One of the original formulations of the target functional was 
\begin_inset Formula $f(x)=\sum y_{i}\beta_{i}K(x,x_{i})$
\end_inset

 - this can be accomplished without retaining 
\begin_inset Formula $y_{i}$
\end_inset

 if 
\begin_inset Formula $\beta_{i}$
\end_inset

 isn't constrained.
 So unless there's a good reason to somehow constrain 
\begin_inset Formula $\beta$
\end_inset

, I think we can use the following functional
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
f(x)= & \sum_{i=1}^{\ell}\beta_{i}K(x,x_{i})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
This essentially encodes the regression as a series of 
\begin_inset Formula $\beta$
\end_inset

 whose value can take any value in the range of 
\begin_inset Formula $y$
\end_inset

.
\end_layout

\begin_layout Standard
Next, we need to consider our optimization problem.
 We know that we're trying to reduce the empirical risk (for now), and there's
 a decent chance that the MCIC requires the 
\begin_inset Formula $L_{1}$
\end_inset

 norm, so for now let's frame this as a simple task of minimizing the risk
 
\begin_inset Formula $R$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\text{Minimize}\qquad W(\beta)= & R\\
\text{Subject to}\qquad\quad R\ge & \sum_{i=1}^{\ell}\left|y_{i}-\sum_{l=1}^{\ell}\beta_{i}K(x_{i},x_{j})\right|_{L_{1}}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The brings us to our first challenge, which is that splitting this into
 two summations (over 
\begin_inset Formula $y$
\end_inset

 and 
\begin_inset Formula $f$
\end_inset

) reduces the optimization problem to a problem of matching means, rather
 than minimizing localized risk.
 My initial response was to use the 
\begin_inset Formula $L_{2}$
\end_inset

 norm, as this does a better job separating out pairwise dynamics from aggregate
 dynamics.
 Since our interest is in minimizing the aggregate local risk (loss), perhaps
 there's a way to examine that directly.
\end_layout

\begin_layout Standard
Moving to the most local formulation possible, we would want to consider
 the loss at each point, but somehow either calculate or combine that loss
 in such a way that it couldn't be reduced to matching means.
 Vapnik's approach is to limit the local loss directly and optimize over
 other variables.
 Perhaps we could do something similar; we restrict the loss for each observatio
n to an optimization variable 
\begin_inset Formula $R_{i}$
\end_inset

, then minimize 
\begin_inset Formula $R\cdot R$
\end_inset

.
 This approach takes inspiration from the original SVM formulation in which
 sparseness is achieved by including 
\begin_inset Formula $w\cdot w$
\end_inset

 in the optimization term.
\end_layout

\begin_layout Standard
Let's consider the dynamics of 
\begin_inset Formula $R\cdot R$
\end_inset

.
 Let's assume that 
\begin_inset Formula $\sum R=c$
\end_inset

, some constant.
 If 
\begin_inset Formula $R_{i}=R_{j}$
\end_inset

, then 
\begin_inset Formula $R\cdot R=\ell(c/\ell)^{2}=c^{2}/\ell$
\end_inset

.
 On the other hand, if 
\begin_inset Formula $R_{0}=c$
\end_inset

, then 
\begin_inset Formula $R\cdot R=c^{2}$
\end_inset

 (since only 
\begin_inset Formula $R_{0}\times R_{0}$
\end_inset

 is non-zero).
 This shows that at least for these two examples, the smaller value of 
\begin_inset Formula $R\cdot R$
\end_inset

 is the value in which risk is distributed.
 Thanks to wikipedia, I can state that 
\begin_inset Formula $R\cdot R=\left\Vert R\right\Vert ^{2}$
\end_inset

 - the dot product is proportional to the magnitude of 
\begin_inset Formula $R$
\end_inset

, which means that for any 
\begin_inset Formula $\sum R=c$
\end_inset

, the most even distribution along each axis should be minimal (extension
 increases the sum exponentially, while distribution increases the sum linearly).
\end_layout

\begin_layout Standard
OK, so I just ran the math and tried it, and it turns out that 
\begin_inset Formula $P$
\end_inset

 is singular - because I'm not using 
\begin_inset Formula $\beta$
\end_inset

 in the optimization function there's no array 
\begin_inset Formula $A$
\end_inset

 for which 
\begin_inset Formula $AP=\mathbf{I}$
\end_inset

.
 This gives use two options, either we need to add 
\begin_inset Formula $\beta$
\end_inset

 to the 
\begin_inset Formula $W$
\end_inset

, or we need to reformulate 
\begin_inset Formula $W$
\end_inset

 to not use 
\begin_inset Formula $R$
\end_inset

.
 I think the latter is the better option.
 Notice that 
\begin_inset Formula $R\cdot R=\sum R^{2}$
\end_inset

, and we've set 
\begin_inset Formula $R_{i}\ge L(f(x_{i}),y_{i})$
\end_inset

, so we should be able to eliminate the intermediate step and simply optimize
 the following
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\text{Minimize}\qquad W(\beta)= & \sum_{i=1}^{\ell}\left(\varphi(x_{i})-y_{i}\right)^{2}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
This has no constraints, which may be a problem (which would have been a
 problem to begin with, since it's the equivalent problem).
 In any case, let's see how that shakes out
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
W(\beta)= & \sum_{i=1}^{\ell}\varphi(x_{i})-y_{i}\\
= & \sum_{i=1}^{\ell}\left(y_{i}-\sum_{j=1}^{\ell}\beta_{j}K(x_{i,}x_{j})\right)^{2}\\
= & \sum_{i=1}^{\ell}\left[y_{i}^{2}-2y_{i}\sum_{j=1}^{\ell}\beta_{j}K(x_{i},x_{j})+\sum_{j,k=1}^{\ell}\beta_{j}\beta_{k}K(x_{i},x_{j})K(x_{i},x_{k})\right]\\
\approx & \sum_{i,j,k=1}^{\ell}\beta_{j}\beta_{k}K(x_{i},x_{j})K(x_{i},x_{k})-\sum_{i,j=1}^{\ell}2y_{i}\beta_{j}K(x_{i},x_{j})\\
= & \sum_{i,j,k=1}^{\ell}\beta_{i}\beta_{j}K(x_{k},x_{i})K(x_{k},x_{j})-\sum_{i,j=1}^{\ell}2y_{j}\beta_{i}K(x_{j},x_{i})\\
= & \sum_{i,j=1}^{\ell}\beta_{i}\beta_{j}\sum_{k=1}^{\ell}K(x_{i},x_{k})K(x_{j},x_{k})-\sum_{i=1}^{\ell}\beta_{i}\sum_{j=1}^{\ell}2y_{j}K(x_{i},x_{j})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Let's put thin in QP terms
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\text{Minimize }\quad W(\beta)= & \frac{1}{2}\beta^{T}P\beta+q^{T}\beta\\
\text{Subject to}\quad\qquad G\beta & \preceq h\\
A\beta & =b
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
P_{i,j}= & \sum_{k=1}^{\ell}K(x_{i},x_{k})K(x_{j},x_{k})\\
q_{i}= & -\sum_{j=1}^{\ell}y_{j}K(x_{i},x_{j})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
This just simply doesn't work.
 No fucking clue why not, but I think we need to re-think the objective
 function, maybe start with an actual kernel regression form.
 One option would be the Nadaraya-Watson estimator
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\phi(x)= & \frac{\sum_{i=1}^{\ell}y_{i}K(x,x_{i})}{\sum_{i=1}^{\ell}K(x,x_{i})}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
This can be interpreted as a weighted average of 
\begin_inset Formula $y_{i}$
\end_inset

, which takes the generalized form
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\bar{x}= & \frac{\sum w_{i}x_{i}}{\sum w_{i}}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
This means that if we're going to use a multiplier in the numerator it'll
 need to be included in the denominator as well, giving us the following
 target functional
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\varphi(x)= & \frac{\sum_{i=1}^{\ell}y_{i}\beta_{i}K(x,x_{i})}{\sum_{i=1}^{\ell}\beta_{i}K(x,x_{i})}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
This is a problem, as we have optimization variables in the denominator.
 The simple solution is to constrain the problem (this is sounding more
 and more like Vapnik) so the denominator goes away - something along these
 lines
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
1= & \sum_{i=1}^{\ell}\beta_{i}K(x,x_{i})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The other option would be to use Vapnik's CPDF algorithm, although I'm not
 sure how that could be modified to minimize risk directly.
 Look into the ill-posed problem regularizers - if you can find a regularizer
 that's in the constraints rather than the objective function, it might
 solve the problem with the formalism above.
\end_layout

\begin_layout Section
Regularization
\end_layout

\begin_layout Standard
Looking back over Vapnik, it ooks like we might need to be working on this
 from the regularization perspective.
 He shows three possible approaches
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\text{Minimize}\qquad W_{T}(f)= & \left\Vert Af-F\right\Vert _{E_{2}}^{2}+\gamma\Omega(f)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\text{Minimize}\qquad\qquad\quad W_{P}(f)= & \Omega(f)\\
\text{Subject to}\qquad\left\Vert Af-F\right\Vert _{E_{2}}\le & \mu
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\text{Minimize}\qquad W_{I}(f)= & \left\Vert Af-F\right\Vert _{E_{2}}\\
\text{Subject to}\qquad\Omega(f)\le & C
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
In all of these cases, we have to use a function 
\begin_inset Formula $\Omega$
\end_inset

 which has the following properties;
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\Omega(f)$
\end_inset

 is nonnegative convex functional, that is for any 
\begin_inset Formula $0\le\lambda\le1$
\end_inset

 the inequality
\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\begin{align*}
\Omega(\lambda f_{1}+(1-\lambda)f_{2})\le & \lambda\Omega(f_{1})+(1-\lambda)\Omega(f_{2}),\qquad f_{1},f_{2}\in\mathcal{F}
\end{align*}

\end_inset


\begin_inset Newline newline
\end_inset

is valid.
 
\end_layout

\begin_layout Enumerate
The following equality holds:
\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\begin{align*}
\Omega(0)= & 0
\end{align*}

\end_inset


\end_layout

\begin_layout Enumerate
For each fixed 
\begin_inset Formula $f$
\end_inset

 the function
\begin_inset Newline newline
\end_inset


\begin_inset Formula $ $
\end_inset


\begin_inset Formula 
\begin{align*}
r(\gamma)= & \Omega(\gamma f)
\end{align*}

\end_inset


\begin_inset Newline newline
\end_inset

is a strictly increasing function of 
\begin_inset Formula $\gamma$
\end_inset

.
 In other words, multiplying 
\begin_inset Formula $f$
\end_inset

 by some constant should result in an increase in 
\begin_inset Formula $\Omega$
\end_inset

, and this increase should be proportional to the constant.
\end_layout

\begin_layout Standard
We would like to express the problem of minimizing 
\begin_inset Formula $R$
\end_inset

 as a regularized minimization task based on 
\begin_inset Formula $R_{\text{emp}}$
\end_inset

 and 
\begin_inset Formula $\omega$
\end_inset

.
 Since our objective is to minimize both the empirical risk (through selecting
 
\begin_inset Formula $\beta$
\end_inset

) and the modulus of continuity (through minimizing 
\begin_inset Formula $\omega$
\end_inset

 subject to a set of constraints), we'll probably need to use the 
\begin_inset Formula $W_{T}$
\end_inset

 regularizer, as the free parameter 
\begin_inset Formula $\lambda$
\end_inset

 is the only one which doesn't impose a choice of minimizing one or the
 other.
\end_layout

\begin_layout Standard
The challenge here is that the MCIC is defined over the 
\begin_inset Formula $L_{1}$
\end_inset

 norm.
 The exponent in the regularizer will require either reformulating the MCIC
 in 
\begin_inset Formula $L_{2}$
\end_inset

 or expanding it.
 Let's see what happens if we expand it
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
M(f_{n})= & R_{\text{emp}}(f_{n})+\frac{\omega(f_{n},h_{0})}{3}\sqrt{\frac{1}{2\ell}\ln\frac{2}{\delta}}\\
M(f_{n})^{2}= & R_{\text{emp}}(f_{n})^{2}+\frac{2R_{\text{emp}}(f_{n})\omega(f_{n},h_{0})}{3}\sqrt{\frac{1}{2\ell}\ln\frac{2}{\delta}}+\frac{\omega(f_{n},h_{0})^{2}}{9}\left(\frac{1}{2\ell}\ln\frac{2}{\delta}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Using this approach will require us to define our regularizer as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\Omega(f_{n})= & \frac{2R_{\text{emp}}(f_{n})\omega(f_{n},h_{0})}{3}\sqrt{\frac{1}{2\ell}\ln\frac{2}{\delta}}+\frac{\omega(f_{n},h_{0})^{2}}{9}\left(\frac{1}{2\ell}\ln\frac{2}{\delta}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
To demonstrate this is a valid regularizer, we can eliminate terms which
 are constant for a given optimization problem, giving us
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\Omega^{\prime}(f_{n})= & R_{\text{emp}}(f_{n})\omega(f_{n},h_{0})+\omega(f_{n},h_{0})^{2}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
I'm fairly certain that 
\begin_inset Formula $\omega$
\end_inset

 is nonnegative convex, as is 
\begin_inset Formula $f(x)=x^{2}$
\end_inset

, which should mean that the second term is nonnegative convex.
 
\begin_inset Formula $\omega$
\end_inset

 also satisfies the second condition, which means that 
\begin_inset Formula $\omega^{2}$
\end_inset

 does as well.
 Finally, the third condition is satsified for 
\begin_inset Formula $\omega$
\end_inset

, since 
\begin_inset Formula $\omega(af,h)\le\left|a\right|\omega(f,h)$
\end_inset

, which should result in 
\begin_inset Formula $\omega^{2}$
\end_inset

 having the same property.
\end_layout

\begin_layout Standard
The risk term isn't as clear though.
 The second condition is satisfied because the 
\begin_inset Formula $\omega$
\end_inset

 term will cancel out the risk.
\end_layout

\begin_layout Standard
Before we get into convexity, lets consider the third condition.
 We'll need to define our target functional first, so for now let's use
 the regression form Vapnik uses
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\varphi(x)= & \sum_{i=1}^{\ell}y_{i}\beta_{i}K(x,x_{i})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Since 
\begin_inset Formula $f_{n}$
\end_inset

 is parameterized by 
\begin_inset Formula $\beta$
\end_inset

, we can express our regularizer in terms of 
\begin_inset Formula $\beta$
\end_inset

, and explore the effect of multiplying by 
\begin_inset Formula $\gamma$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\Omega^{\prime}(\gamma\varphi_{\beta})= & R_{\text{emp}}(\gamma\varphi_{\beta})\omega(\gamma\varphi_{\beta},h_{0})+\omega(\gamma\varphi_{\beta},h_{0})^{2}\\
= & \left|\sum_{i=1}^{\ell}y_{i}-\gamma\sum_{i,j=1}^{\ell}y_{j}\beta_{j}K(x_{i},x_{j})\right|\omega(\gamma\varphi_{\beta},h_{0})+\omega(\gamma\varphi_{\beta},h_{0})^{2}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We would like to show that this quantity is strictly increasing with 
\begin_inset Formula $\gamma$
\end_inset

.
 We can expand this out some - here we'll concentrate on the risk portion
 of the first term
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\left|\sum_{i=1}^{\ell}y_{i}-\gamma\sum_{i,j=1}^{\ell}y_{j}\beta_{j}K(x_{i},x_{j})\right|= & \left|\gamma\sum_{i,j=1}^{\ell}y_{j}\beta_{j}K(x_{i},x_{j})-\sum_{i=1}^{\ell}y_{i}\right|
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
This seems to show that the loss is strictly increasing with 
\begin_inset Formula $\gamma$
\end_inset

, and since we know that 
\begin_inset Formula $\omega$
\end_inset

 is as well, I think we can state that conditions 2 and 3 are both met.
\end_layout

\begin_layout Standard
This leaves us with convexity.
 We'll begin by using the 
\begin_inset Formula $\beta$
\end_inset

-parameterization, the substitute our target functional (again, ignoring
 the 2nd term
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\Omega(\lambda\varphi_{\beta^{1}}+(1-\lambda)\varphi_{\beta^{2}})\le & \lambda\Omega(\varphi_{\beta^{1}})+(1-\lambda)\Omega(\varphi_{\beta^{2}}),\qquad\beta^{1},\beta^{2}\in\mathbb{R}^{\ell}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\Omega^{\prime}(\lambda\varphi_{\beta^{1}}+(1-\lambda)\varphi_{\beta^{2}})= & R_{\text{emp}}\left(\lambda\varphi_{\beta^{1}}+(1-\lambda)\varphi_{\beta^{2}}\right)\omega\left(\left[\lambda\varphi_{\beta^{1}}+(1-\lambda)\varphi_{\beta^{2}}\right],h_{0}\right)\\
= & \left|\sum_{i=1}^{\ell}y_{i}-\left(\lambda\sum_{j=1}^{\ell}y_{j}\beta_{j}^{1}K(x_{i},x_{j})+(1-\lambda)\sum_{j=1}^{\ell}y_{j}\beta_{j}^{2}K(x_{i},x_{j})\right)\right|\\
= & \left|\sum_{i=1}^{\ell}y_{i}-\left(\sum_{j=1}^{\ell}\lambda y_{j}\beta_{j}^{1}K(x_{i},x_{j})+(1-\lambda)y_{j}\beta_{j}^{2}K(x_{i},x_{j})\right)\right|\\
= & \left|\sum_{i=1}^{\ell}y_{i}-\left(\sum_{j=1}^{\ell}\left(\lambda\beta_{j}^{1}+(1-\lambda)\beta_{j}^{2}\right)y_{j}K(x_{i},x_{j})\right)\right|
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\lambda\Omega(\varphi_{\beta^{1}})= & \lambda\left|\sum_{i=1}^{\ell}y_{i}-\sum_{j=1}^{\ell}y_{j}\beta_{j}^{1}K(x_{i},x_{j})\right|\\
= & \left|\lambda\sum_{i=1}^{\ell}y_{i}-\lambda\sum_{j=1}^{\ell}y_{j}\beta_{j}^{1}K(x_{i},x_{j})\right|\\
(1-\lambda)\Omega(\varphi_{\beta^{2}})= & (1-\lambda)\left|\sum_{i=1}^{\ell}y_{i}-\sum_{j=1}^{\ell}y_{j}\beta_{j}^{2}K(x_{i},x_{j})\right|\\
= & \left|(1-\lambda)\sum_{i=1}^{\ell}y_{i}-(1-\lambda)\sum_{j=1}^{\ell}y_{j}\beta_{j}^{2}K(x_{i},x_{j})\right|
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
For our inequality to hold, we can state that the following equality must
 hold (ignoring the MC for now)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\left|\sum_{i=1}^{\ell}y_{i}-\left(\sum_{j=1}^{\ell}\left(\lambda\beta_{j}^{1}+(1-\lambda)\beta_{j}^{2}\right)y_{j}K(x_{i},x_{j})\right)\right|\le & \left|\lambda\sum_{i=1}^{\ell}y_{i}-\lambda\sum_{j=1}^{\ell}y_{j}\beta_{j}^{1}K(x_{i},x_{j})\right|+\left|(1-\lambda)\sum_{i=1}^{\ell}y_{i}-(1-\lambda)\sum_{j=1}^{\ell}y_{j}\beta_{j}^{2}K(x_{i},x_{j})\right|
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Who the fuck knows.
 This may be better to just try empirically.
 Probably won't work.
\end_layout

\begin_layout Section
Formalism
\end_layout

\begin_layout Standard
Before getting too far, let's make sure this is adequately constrained.
 We'll work backwards from the generalized regression form
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\varphi(x)= & \frac{\sum_{i=1}^{\ell}y_{i}\beta_{i}K(x,x_{i})}{\sum_{i=1}^{\ell}\beta_{i}K(x,x_{i})}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
To work with this we'll need a way to make the denominator go away, at least
 in our optimization problem.
 This will have to happen in the risk functional, defined as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
R_{\text{emp}}(\varphi)= & \sum_{i=1}^{\ell}\left|y_{i}-\varphi(x_{i})\right|\\
= & \sum_{i=1}^{\ell}\left|y_{i}-\frac{\sum_{j=1}^{\ell}y_{j}\beta_{j}K(x_{i},x_{j})}{\sum_{j=1}^{\ell}\beta_{j}K(x_{i},x_{j})}\right|
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
And here's the problem - unless the summation over 
\begin_inset Formula $i$
\end_inset

 can be distributed to the denominator, we can only eliminate the denominator
 by constraining 
\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\begin{align*}
1= & \sum_{j=1}^{\ell}\beta_{j}K(x_{i},x_{j}) & i=1,...,\ell
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Which is to say we introduce 
\begin_inset Formula $\ell$
\end_inset

 constrains rather than one.
 We also have no guarantee that the constraint will hold for new observations
 
\begin_inset Formula $\bar{x}$
\end_inset

, which means we might still need to include the denominator in the target
 functional.
 In any case, this isn't an unreasonable burden, especially considering
 the number of constraints the MCIC is going to impose.
 I think we're ready to begin framing the optimization task.
 In general terms, we want to optimize
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\text{Minimize}\quad W(f_{n},\omega)= & R_{\text{emp}}(f_{n})^{2}+\frac{2R_{\text{emp}}(f_{n})\omega}{3}\sqrt{\frac{1}{2\ell}\ln\frac{2}{\delta}}+\frac{\omega^{2}}{9}\left(\frac{1}{2\ell}\ln\frac{2}{\delta}\right)\\
\text{Subject to}\qquad1= & \sum_{j=1}^{\ell}\beta_{j}K(x_{i},x_{j}) & i=1,...,\ell\\
\omega\ge & \left|f_{n}(x_{i})-f_{n}(x_{j})\right| & \forall\ 0\le x_{i}-x_{j}\le h_{0}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
This needs some expansion before we can do the optimization
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
W(f_{n},\omega)= & R_{\text{emp}}(f_{n})^{2}+\frac{2R_{\text{emp}}(f_{n})\omega}{3}\sqrt{\frac{1}{2\ell}\ln\frac{2}{\delta}}+\frac{\omega^{2}}{9}\left(\frac{1}{2\ell}\ln\frac{2}{\delta}\right)\\
W(\beta,\omega)= & \left(\sum_{i=1}^{\ell}\left|y_{i}-\sum_{j=1}^{\ell}y_{j}\beta_{j}K(x_{i},x_{j})\right|\right)^{2}+\sum_{i=1}^{\ell}\left|y_{i}-\sum_{j=1}^{\ell}y_{j}\beta_{j}K(x_{i},x_{j})\right|\frac{2\omega}{3}\sqrt{\frac{1}{2\ell}\ln\frac{1}{\delta}}+\frac{\omega^{2}}{9}\left(\frac{1}{2\ell}\ln\frac{2}{\delta}\right)\\
= & \left(\sum_{i=1}^{\ell}y_{i}-\sum_{i,j=1}^{\ell}y_{j}\beta_{j}K(x_{i},x_{j})\right)^{2}+\sum_{i=1}^{\ell}\left|y_{i}-\sum_{j=1}^{\ell}y_{j}\beta_{j}K(x_{i},x_{j})\right|\frac{2\omega}{3}\sqrt{\frac{1}{2\ell}\ln\frac{1}{\delta}}+\frac{\omega^{2}}{9}\left(\frac{1}{2\ell}\ln\frac{2}{\delta}\right)\\
= & \sum_{i,j=1}^{\ell}y_{i}y_{j}-2\sum_{i,j,k=1}^{\ell}y_{k}y_{j}\beta_{j}K(x_{i},x_{j})+\sum_{i,j,k,l=1}^{\ell}y_{j}y_{l}\beta_{j}\beta_{l}K(x_{i},x_{j})K(x_{k},x_{l})\\
 & \qquad\qquad+\sum_{i=1}^{\ell}\left|y_{i}-\sum_{j=1}^{\ell}y_{j}\beta_{j}K(x_{i},x_{j})\right|\frac{2\omega}{3}\sqrt{\frac{1}{2\ell}\ln\frac{1}{\delta}}+\frac{\omega^{2}}{9}\left(\frac{1}{2\ell}\ln\frac{2}{\delta}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
This is a problem.
 The absolute value term can't be eliminated (in the past we've split it
 into positive and negative components and used it as a constraint).
 What I think this means is that the only way for this to work is if we
 can formulate the MCIC in 
\begin_inset Formula $L_{2}$
\end_inset

.
 That's still going to leave the ugly first term, but it will avoid the
 second term, and hopefully we'll be able to prove convexivity.
\end_layout

\begin_layout Section
LP II
\end_layout

\begin_layout Standard
This is an attemp to modify traditional SVR to use MCIC as regularizer rather
 than epsilon
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
F(\xi,\xi^{*},\omega) & =\sum_{i=1}^{\ell}\xi_{i}^{*}+\sum_{i=1}^{\ell}\xi_{i}+\frac{\omega}{3}\sqrt{\frac{1}{2N}\ln\frac{1}{\delta}}\\
y_{i}-\sum_{a}(w^{a}\cdot x_{i}^{a})-b & \le\xi_{i}^{*} & i=1,...,\ell\\
\sum_{a}(w^{a}\cdot x_{i}^{a})+b-y_{i} & \le\xi_{i} & i=1,...,\ell\\
\omega_{h}\ge & \sum_{a}(w^{a}\cdot x_{i}^{a})-\sum_{a}(w^{a}\cdot x_{j}^{a})\ge-\omega_{h} & 0\le x_{i}-x_{j}\le h\\
\xi_{i} & \ge\ 0 & i=1,...,\ell\\
\xi_{i}^{*} & \ge\ 0 & i=1,...,\ell
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Note that under the KKT conditions, 
\begin_inset Formula $g(x)\le c\rightarrow c-g(x)$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
F(w,\xi,\xi^{*};\ \alpha,\alpha^{*},C,\mathcal{C},\gamma,\gamma^{*})= & \sum_{i=1}^{\ell}\xi_{i}^{*}+\sum_{i=1}^{\ell}\xi_{i}+\frac{\omega}{3}\sqrt{\frac{1}{2N}\ln\frac{1}{\delta}}\\
 & \qquad-\sum_{i}\alpha_{i}^{a*}\left(\sum_{a}(w^{a}\cdot x_{i}^{a})+b-y_{i}+\xi_{i}^{*}\right)\\
 & \qquad-\sum_{i}\alpha_{i}^{a}\left(y_{i}-\sum_{a}(w^{a}\cdot x_{i}^{a})-b+\xi_{i}\right)\\
 & \qquad+\sum_{i,j}\Omega_{i,j}\left(\omega_{h}-\sum_{a}(w^{a}\cdot x_{i}^{a})+\sum_{a}(w^{a}\cdot x_{j}^{a})\right)\\
 & \qquad+\sum_{i,j}\Omega_{i,j}^{*}\left(\omega_{h}-\sum_{a}(w^{a}\cdot x_{j}^{a})+\sum_{x}(w^{a}\cdot x_{i}^{a}\right)\\
 & \qquad-\sum_{i}\gamma_{i}\xi_{i}-\sum_{i}\gamma_{i}^{*}\xi_{i}^{*}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
In this case, we have 
\begin_inset Formula $w,\xi,\xi^{*}$
\end_inset

 as free variables and the rest are Lagrange multipliers.
 For this next bit, it turns out that 
\begin_inset Formula $\frac{d}{dx}\left(a\cdot b\right)=a'\cdot b+a\cdot b'$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\frac{\partial F}{\partial w^{a}}=0= & -\frac{\partial}{\partial w^{a}}\left[\sum_{i}\alpha_{i}^{a*}\left(\sum_{a}(w^{a}\cdot x_{i}^{a})+b-y_{i}+\xi_{i}^{*}\right)\right]\\
 & \qquad-\frac{\partial}{\partial w^{a}}\left[\sum_{i}\alpha_{i}^{a}\left(y_{i}-\sum_{a}(w^{a}\cdot x_{i}^{a})-b+\xi_{i}\right)\right]\\
 & \qquad+\frac{\partial}{\partial w^{a}}\left[\sum_{i,j}\Omega_{i,j}\left(\omega_{h}-\sum_{a}(w^{a}\cdot x_{i}^{a})+\sum_{a}(w^{a}\cdot x_{j}^{a})\right)\right]\\
 & \qquad+\frac{\partial}{\partial w^{a}}\left[\sum_{i,j}\Omega_{i,j}\left(\omega_{h}-\sum_{a}(w^{a}\cdot x_{j}^{a})+\sum_{a}(w^{a}\cdot x_{i}^{a})\right)\right]\\
= & -\sum_{i}\alpha_{i}^{a*}\frac{\partial}{\partial w^{a}}\left(w^{a}\cdot x_{i}^{a}\right)+\sum_{i}\alpha_{i}^{a}\frac{\partial}{\partial w^{a}}\left(w^{a}\cdot x_{i}^{a}\right)+\sum_{i,j}\Omega_{i,j}\frac{\partial}{\partial w^{a}}\left(w^{a}\cdot x_{i}^{a}+w^{a}\cdot x_{j}^{a}\right)\\
= & -\sum_{i}\alpha_{i}^{a*}\left(\left(1\cdot x_{i}^{a}\right)+\left(w^{a}\cdot0\right)\right)+\sum_{i}\alpha_{i}^{a}\left(\left(1\cdot x_{i}^{a}\right)+\left(w^{a}\cdot0\right)\right)\\
 & \qquad+\sum_{i,j}\Omega_{i,j}\left(\left(1\cdot x_{i}^{a}\right)+\left(w^{a}\cdot0\right)+\left(1\cdot x_{j}^{a}\right)+\left(w^{a}\cdot0\right)\right)\\
= & -\sum_{i}\alpha_{i}^{a*}x_{i}^{a}+\sum_{i}\alpha_{i}^{a}x_{i}^{a}+\sum_{i,j}\Omega_{i,j}\left(x_{i}^{a}+x_{j}^{a}\right)\\
-\sum_{i,j}\Omega_{i,j}\left(x_{i}^{a}+x_{j}^{a}\right)= & -\sum_{i}\alpha_{i}^{a*}x_{i}^{a}+\sum_{i}\alpha_{i}^{a}x_{i}^{a}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
THAT's the correct derivation.
 Fucking A.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\frac{\partial F}{\partial\xi}=0= & \frac{\partial}{\partial\xi}\left[\sum_{i}\xi_{i}\right]-\frac{\partial}{\partial\xi}\left[\sum_{i}\alpha_{i}^{a}\left(y_{i}-\sum_{a}(w^{a}\cdot x_{i}^{a})-b+\varepsilon+\xi_{i}\right)\right]-\frac{\partial}{\partial\xi}\left[\sum_{i}\gamma_{i}\xi_{i}\right]\\
= & \ell-\sum_{i}\alpha_{i}^{a}-\sum_{i}\gamma_{i}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\frac{\partial F}{\partial\xi^{*}}=0= & \frac{\partial}{\partial\xi^{*}}\left[\sum_{i}\xi_{i}^{*}\right]-\frac{\partial}{\partial\xi^{*}}\left[\sum_{i}\alpha_{i}^{a*}\left(\sum_{a}(w^{a}\cdot x_{i}^{a})+b-y_{i}+\varepsilon+\xi_{i}^{*}\right)\right]-\frac{\partial}{\partial\xi^{*}}\left[\sum_{i}\gamma_{i}^{*}\xi_{i}^{*}\right]\\
= & \ell-\sum_{i}\alpha_{i}^{a*}-\sum_{i}\gamma_{i}^{*}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\ell-\sum_{i}\alpha_{i}^{a}-\sum_{i}\gamma_{i}= & \ell-\sum_{i}\alpha_{i}^{a*}-\sum_{i}\gamma_{i}^{*}\\
-\sum_{i}\alpha_{i}^{a}-\sum_{i}\gamma_{i}= & \sum_{i}\alpha_{i}^{a*}-\sum_{i}\gamma_{i}^{*}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Same as before, but watch this!
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\frac{\partial F}{\partial b}=0= & -\frac{\partial}{\partial b}\left[\sum_{i}\alpha_{i}^{a*}\left(\sum_{a}(w^{a}\cdot x_{i}^{a})+b-y_{i}+\varepsilon+\xi_{i}^{*}\right)\right]\\
 & \qquad-\frac{\partial}{\partial b}\left[\sum_{i}\alpha_{i}^{a}\left(y_{i}-\sum_{a}(w^{a}\cdot x_{i}^{a})-b+\varepsilon+\xi_{i}\right)\right]\\
= & -\sum_{i}\alpha_{i}^{a*}+\sum_{i}\alpha_{i}^{a}\\
\sum_{i}\alpha_{i}^{a}= & \sum_{i}\alpha_{i}^{a*}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
YEAH BITCHES.
 Putting that into the previous one:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
-\sum_{i}\alpha_{i}^{a}-\sum_{i}\gamma_{i}= & \sum_{i}\alpha_{i}^{a*}-\sum_{i}\gamma_{i}^{*}\\
-\sum_{i}\alpha_{i}^{a}-\sum_{i}\gamma_{i}= & \sum_{i}\alpha_{i}^{a}-\sum_{i}\gamma_{i}^{*}\\
\sum_{i}\gamma_{i}^{*}-\sum_{i}\gamma_{i}= & 2\sum_{i}\alpha_{i}^{a}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Now for 
\begin_inset Formula $C^{a}$
\end_inset

.
 This can probably be expressed with or without the summation.
 The result is the same, it's really a semantic issue.
 Either you have a single constraing defined over a summation or you have
 multiple constraints defined for 
\begin_inset Formula $a$
\end_inset

, in the Lagrange functional the two are interchangeable.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\frac{\partial F}{\partial C^{a}}=0= & -\frac{\partial}{\partial C^{a}}\left[\sum_{a}C^{a}\left(c_{n}^{a}-\left(w^{a}\cdot w^{a}\right)\right)\right]\\
= & -\sum_{a}\left(c_{n}^{a}-\left(w^{a}\cdot w^{a}\right)\right)\\
\sum_{a}c_{n}^{a}= & \sum_{a}w^{a}\cdot w^{a}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Vapnik mentions 'taking into account the Kuhn Tucker conditions' when eliminatin
g terms from 
\begin_inset Formula $L$
\end_inset

.
\end_layout

\end_body
\end_document
