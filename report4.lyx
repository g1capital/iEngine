#LyX 1.6.2 created this file. For more info see http://www.lyx.org/
\lyxformat 345
\begin_document
\begin_header
\textclass amsbook
\begin_preamble



\usepackage{amsfonts}
\end_preamble
\use_default_options false
\begin_modules
theorems-ams
eqs-within-sections
figs-within-sections
\end_modules
\language english
\inputencoding auto
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\paperfontsize 10
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 0
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\leftmargin 1.5in
\topmargin 1in
\rightmargin 1.5in
\bottommargin 1.25in
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\author "" 
\author "" 
\end_header

\begin_body

\begin_layout Title
Acting Machines
\end_layout

\begin_layout Author
Ryan Michael
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset nomencl_print
LatexCommand printnomenclature

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Section*
Introduction
\end_layout

\begin_layout Standard
Wikipedia defines 
\begin_inset Quotes eld
\end_inset

Machine Learning
\begin_inset Quotes erd
\end_inset

 as 
\emph on
a scientific discipline that is concerned with the design and development
 of algorithms that allow computers to learn based on data
\emph default
.
 Machine learning deals with the development of Learning Machines (LM's)
 capable of autonomously generating probabalistic models using sets of observati
ons.
 This text builds upon the foundation of Machine Learning to develop an
 Acting Machine (AM); a learning machine capable of interacting with with
 its environment in a motivated way.
 
\end_layout

\begin_layout Chapter
Motivation
\end_layout

\begin_layout Standard
Our objective is to design an system capable of making informed choices
 about the actions it takes in its environment.
 In the most general sense, the system is composed of three types of data;
 a set of 
\emph on
actions
\emph default
 which the system has taken in the past, a set of 
\emph on
internal states
\emph default
 whose values motivate the system and a set of 
\emph on
observations
\emph default
 describing the system's environment.
 The system can be viewed as a process which takes this information describing
 it's experience up to the present and generates a new set of actions which
 are expected to have a specific effect on the system's internal states.
\end_layout

\begin_layout Standard
We would like to design a system which makes as few assumptions about the
 nature and behavior of its environment as possible, and we would like to
 show that the system provides the best possible performance under those
 assumptions.
 In cases where computational resources determine the best possible performance
 of the system, we would like to use controlling parameters which are intuitive
 and directly relevant to the underlying processes.
\end_layout

\begin_layout Chapter
Epsilon-Machines
\end_layout

\begin_layout Standard
The most basic task the system must perform is to correlate causes with
 their effects.
 To accomplish this task we begin with a concept developed in computational
 mechanics called the 
\begin_inset Formula $\epsilon$
\end_inset

-machine.
 The discussion of these concepts will be heavily influenced by 
\begin_inset CommandInset citation
LatexCommand cite
key "Shal01"

\end_inset

.
\end_layout

\begin_layout Standard
An 
\begin_inset Formula $\epsilon$
\end_inset

-machine is an approach to discovering patterns within a data set.
 The general idea of a pattern is that an object 
\begin_inset Formula $\mathcal{O}$
\end_inset

 has a pattern 
\begin_inset Formula $\mathcal{P}$
\end_inset

 if and only if we can use 
\begin_inset Formula $\mathcal{P}$
\end_inset

 to compress or predict 
\begin_inset Formula $\mathcal{O}$
\end_inset

 in some way.
 In this sense, a pattern is a representation of some underlying structure,
 regularity, symmetry, etc contained in a data set.
 It is this underlying structure which allows us to make justifiable statements
 about the causal relations underlying a data-set.
 The notion of causality which we will use is highly restricted; we will
 refer to an event 
\begin_inset Formula $A$
\end_inset

 causing an event 
\begin_inset Formula $B$
\end_inset

 if 
\begin_inset Formula $B$
\end_inset

 generally follows 
\begin_inset Formula $A$
\end_inset

.
\end_layout

\begin_layout Section
Histories and Futures
\end_layout

\begin_layout Standard
We will begin our discussion of patterns by assuming discrete-valued, discrete-t
ime mixing processes.
 This essentially means that our data set is arranged into a set of 
\begin_inset Formula $t$
\end_inset

-indexed observations 
\begin_inset Formula $S{}_{t},\ S\in\mathcal{A}$
\end_inset

 where each observation takes values from in some finite set 
\begin_inset Formula $\mathcal{A}$
\end_inset

, and the dependence between observations decreases with time; 
\begin_inset Formula $\lim_{t\rightarrow\infty}\Pr(S_{t}|S_{i})=\Pr(S_{t})\quad\forall S_{i}$
\end_inset

.
 
\end_layout

\begin_layout Standard
Define the set of observations leading up to and including 
\begin_inset Formula $s_{i}$
\end_inset

 as a 
\emph on
history
\emph default
 denoted 
\begin_inset Formula $\overleftarrow{S_{i}}$
\end_inset

, and the set of observations after as a 
\emph on
future
\emph default
 denoted 
\begin_inset Formula $\overrightarrow{S_{i}}$
\end_inset

.
 Our goal is to predict the future
\begin_inset Formula $ $
\end_inset


\begin_inset Formula $\overrightarrow{S}$
\end_inset

 based on the past 
\begin_inset Formula $\overleftarrow{S}$
\end_inset

.
 We begin by taking the set 
\begin_inset Formula $\overleftarrow{\mathbf{S}}$
\end_inset

 of all pasts and splitting it into mutually exclusive subsets 
\begin_inset Formula $\boldsymbol{\mathcal{R}}$
\end_inset

.
 Each 
\begin_inset Formula $\rho\in\boldsymbol{\mathcal{R}}$
\end_inset

 will be called a 
\emph on
state
\emph default
.
 When the current history 
\begin_inset Formula $\overleftarrow{s}$
\end_inset

 is included in the set 
\begin_inset Formula $\rho$
\end_inset

 we refer to the process as being 
\emph on
in state
\emph default
 
\begin_inset Formula $\rho$
\end_inset

.
 Thus, we define a mapping 
\begin_inset Formula $\eta$
\end_inset

 from histories to effective states:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\eta:\overleftarrow{\mathbf{S}} & \longmapsto\boldsymbol{\mathcal{R}}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
A specific individual history 
\begin_inset Formula $\overleftarrow{s}\in\overleftarrow{\mathbf{S}}$
\end_inset

 maps to a specific state 
\begin_inset Formula $\rho\in\boldsymbol{\mathcal{R}}$
\end_inset

.
 
\end_layout

\begin_layout Section
Information Entropy
\end_layout

\begin_layout Standard
We would like to discuss the uncertainty of the future.
 We will use the concept of information entropy 
\begin_inset Formula $H[X]$
\end_inset

 to describe the amount of uncertainty a given future contains.
 The information entropy describes the unpredictability of a random process.
 For example, a series of coin tosses with a fair coin has maximum entropy,
 since there is no way to predict what will come next.
 A string of coin tosses with a two-headed coin has zero entropy, since
 the coin will always come up heads.
 The information entropy of a process 
\begin_inset Formula $X$
\end_inset

 taking values in 
\begin_inset Formula $\mathcal{A}$
\end_inset

 is defined as 
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
H[X] & \equiv-\sum_{x\in\mathcal{A}}\Pr(X=x)\log\Pr(X=x)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We can likewise describe the joint information entropy; the unpredictability
 of joint observations of two processes 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 taking values in 
\begin_inset Formula $\mathcal{A}$
\end_inset

 and 
\begin_inset Formula $\mathcal{B}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
H[X,Y] & \equiv-\sum_{(x,y)\in\mathcal{A}\times\mathcal{B}}\Pr(X=x,Y=y)\log\Pr(X=x,Y=y)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
as well as the conditional information entropy; the unpredictability of
 
\begin_inset Formula $X$
\end_inset

 given that the state of 
\begin_inset Formula $Y$
\end_inset

 is known
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
H[X|Y] & \equiv H[X,Y]-H[Y]\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Finally, we can describe the mutual information 
\begin_inset Formula $I[X;Y]$
\end_inset

 which describes the average reduction in uncertainty about 
\begin_inset Formula $X$
\end_inset

 produced by knowing 
\begin_inset Formula $Y$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $ $
\end_inset


\begin_inset Formula \begin{align*}
I[X;Y] & \equiv H[X]-H[X|Y]\end{align*}

\end_inset


\end_layout

\begin_layout Section
Causal States
\end_layout

\begin_layout Standard
We can now define the causal states of a process.
 A process 
\begin_inset Formula $X$
\end_inset

 has a set of causal states 
\begin_inset Formula $\epsilon(\overleftarrow{s})$
\end_inset

 defined as 
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\epsilon(\overleftarrow{x}) & \equiv\left\{ \overleftarrow{x\prime}|\ \Pr\left(\overrightarrow{X}\in F|\overleftarrow{X}=\overleftarrow{x}\right)=\Pr\left(\overrightarrow{X}\in F|\overleftarrow{X}=\overleftarrow{x\prime}\right),\ \forall F\in\sigma(\overrightarrow{X}),\overleftarrow{x\prime}\in\overleftarrow{X}\right\} \end{align*}

\end_inset


\end_layout

\begin_layout Standard
In other words, a causal state 
\begin_inset Formula $\epsilon(\overleftarrow{x})$
\end_inset

 is the set of all histories 
\begin_inset Formula $\overleftarrow{x\prime}$
\end_inset

 for which the probability of the future 
\begin_inset Formula $\overrightarrow{X}$
\end_inset

 taking some value 
\begin_inset Formula $F$
\end_inset

 is the same.
 A causal state encodes the set of paths through time leading to a given
 outcome.
 We denote a give causal state 
\begin_inset Formula $\mathcal{X}_{i}$
\end_inset

 and the set of all causal states for a process 
\begin_inset Formula $\boldsymbol{\mathcal{X}}$
\end_inset

.
 
\end_layout

\begin_layout Standard
Causal states are of interest to us because they fully describe the causal
 structure of the process from which they are derived.
 We describe the causal states as a 
\emph on
sufficient statistic
\emph default
 for the process 
\begin_inset Formula $\mathbf{S}$
\end_inset

, which is to say that the mutual information between the future and the
 past is the same as the between the future and the causal states
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
I[\overrightarrow{X};\mathcal{X}] & =I[\overrightarrow{X};\overleftarrow{X}]\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Anything you can learn about 
\begin_inset Formula $\overrightarrow{S}$
\end_inset

 from the past can also be learned from the causal states.
\end_layout

\begin_layout Standard
Another attractive property of the causal states is that they are the most
 compact sufficient statistic possible.
 We describe the causal states as a 
\emph on
minimal
\emph default
.
 It can be shown
\begin_inset CommandInset citation
LatexCommand cite
key "Shal01"

\end_inset

 that the entropy of the causal states 
\begin_inset Formula $H[\boldsymbol{\mathcal{S}}]$
\end_inset

 is smaller than any other partitioning of the past which is equally predictive
 of the future 
\begin_inset Formula $\hat{\boldsymbol{\mathcal{R}}}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
H[\overrightarrow{X}|\hat{\mathcal{R}}] & =H[\overrightarrow{X}|\mathcal{X}]\\
H[\boldsymbol{\mathcal{X}}] & \le H[\hat{\boldsymbol{\mathcal{R}}}]\end{align*}

\end_inset


\end_layout

\begin_layout Standard
This means that the causal states describe the future of 
\begin_inset Formula $\mathbf{S}$
\end_inset

 using the least possible amount if information.
\end_layout

\begin_layout Section
Reconstruction
\end_layout

\begin_layout Itemize
Reconstruction
\end_layout

\begin_layout Itemize
Transition Matrix
\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Standard
We have established the foundations of an acting machine by describing a
 generalized problem domain and showing the domain has several useful properties.
 Having shown that the causal states fully and minimally describe the underlying
 data-generating process we can now proceed to both refine our understanding
 of causal states and develop an algorithm to reconstruct them from a set
 of observations.
 
\end_layout

\begin_layout Standard
Unfortunately the task of reconstruction remains undefined, and for infinite
 sequences is computationally intractable.
 In the next chapter we will begin making the 
\begin_inset Formula $\epsilon$
\end_inset

-machine reconstruction task more tractable by considering finite sequences
 and developing a general algorithm to implement reconstruction.
\end_layout

\begin_layout Chapter
Hierarchy
\end_layout

\begin_layout Standard
Extension / Layering
\end_layout

\begin_layout Section
Mixing
\end_layout

\begin_layout Standard
The concept of mixing processes has been briefly mentioned, however it deserves
 more discussion.
 The original formulation of 
\begin_inset Formula $\epsilon$
\end_inset

-machines assumed that 
\begin_inset Formula $X$
\end_inset

 was stationary; that 
\begin_inset Formula $\Pr(X_{i}=x)=\Pr(X_{j}=x),\ \forall i,j$
\end_inset

.
 While this assumption makes some of the math simpler, it is a relatiely
 rigid constraint; many real-world processes which we would like to model
 do not satisfy the constraint that their probability distribution is time-invar
iant
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
Notice that while the distribution of a stationary process is constant,
 the paths traced by an observation of such a process can still have structure,
 and that structure can change over time.
 This is to say that while 
\begin_inset Formula $\Pr(S)$
\end_inset

 is constant, 
\begin_inset Formula $\Pr(S_{i}|S_{j})$
\end_inset

 need not be.
\end_layout

\end_inset

.
 We therefore use the weaker assumption of a mixing process.
\end_layout

\begin_layout Standard
Mixing processes are stochastic processes in which the dependance of the
 future on the past decreases with time.
 We will make the two assumptions about the process 
\begin_inset Formula $S$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\lim_{\tau\rightarrow\infty}\Pr(\overrightarrow{X_{i+\tau}}|\overleftarrow{X_{i}}) & =\Pr(\overrightarrow{X_{i}})\\
\int_{o}^{\infty}\Pr(\overrightarrow{X_{i+\tau}}|\overleftarrow{X_{i}})-\Pr(\overrightarrow{X_{i}}) & <\infty\end{align*}

\end_inset


\end_layout

\begin_layout Standard
In other words, the future is independant of the distant past, and the correlati
on between the past and the future decays fast enough that the influence
 of the past is bounded.
 
\end_layout

\begin_layout Itemize
Address the implications of mixing on CM & proofs of minimality & sufficiency
\end_layout

\begin_layout Section
Finite Sequences
\end_layout

\begin_layout Standard
It is now time to move away from infinite pasts, and thus from maximal reduction
s in uncertainty.
 We have shown that the causal states 
\begin_inset Formula $\boldsymbol{\mathcal{X}}$
\end_inset

 represent the maximal reduction in uncertainty about 
\begin_inset Formula $X$
\end_inset

 possible.
 We are unlikely to deal with infinite sequences of data in daily life;
 any analysis of real-world data will necessarily be finite.
 We can model this in our algorithm by adding an error term 
\begin_inset Formula $\xi$
\end_inset

 which describes either the maximal uncertainty we are willing to 'trade'
 for shorter sequences in 
\begin_inset Formula $\mathbf{X}$
\end_inset

 (when working from an infinite set) or the uncertainty resulting from a
 given set of sequences 
\begin_inset Formula $\mathbf{X}$
\end_inset

 (when working from actual data).
 
\end_layout

\begin_layout Standard
We can define the causal states 
\begin_inset Formula $\boldsymbol{\mathcal{X}}^{L}$
\end_inset

 of the set of finite sequences of length 
\begin_inset Formula $L$
\end_inset

; the uncertainty of the finite causal states differs from the uncertainty
 of the infinite causal states by at most the error term 
\begin_inset Formula $\xi$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
H[\boldsymbol{\mathcal{X}}^{L}]+\xi & \le H[\boldsymbol{\mathcal{X}}]\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Fortunately it can be shown that under the assumption of sequences of length
 
\begin_inset Formula $L$
\end_inset

 the properties of minimality and sufficiency are retained.
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
I[\overrightarrow{X}^{L};\mathcal{X}^{L}] & =I[\overrightarrow{X}^{L};\overleftarrow{X}^{L}]\\
H[\overrightarrow{X}^{L}|\hat{\mathcal{R}}^{L}] & =H[\overrightarrow{X}^{L}|\mathcal{X}^{L}]\\
H[\boldsymbol{\mathcal{X}}^{L}] & \le H[\hat{\boldsymbol{\mathcal{R}}}^{L}]\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Another useful property of finite causal states is that reconstructing the
 causal states for sequences of length 
\begin_inset Formula $L$
\end_inset

 constitutes a partial reconstruction of sequences of length 
\begin_inset Formula $L+1$
\end_inset

.
 Intuitively, if a causal state 
\begin_inset Formula $\mathcal{X}^{L}$
\end_inset

 of length 
\begin_inset Formula $L$
\end_inset

 contains all the sequences preceeding some value 
\begin_inset Formula $x$
\end_inset

, than it must be true that the sequence defined by the last 
\begin_inset Formula $L$
\end_inset

 elements of any sequence in 
\begin_inset Formula $\mathcal{X}^{L+1}$
\end_inset

 are members of 
\begin_inset Formula $\mathcal{X}$
\end_inset

.
 If we consider the causal states as a partioning of 
\begin_inset Formula $\mathbf{X}$
\end_inset

, we can state that the causal states 
\begin_inset Formula $\mathcal{X}^{L+1}$
\end_inset

 will be partitions of 
\begin_inset Formula $\mathcal{X}^{L}$
\end_inset

, in the same way that 
\begin_inset Formula $\mathcal{X}^{L}$
\end_inset

 will be partitions of 
\begin_inset Formula $\mathbf{X}$
\end_inset

.
 
\end_layout

\begin_layout Section
Reconstruction Deconstruction
\end_layout

\begin_layout Standard
We approach the reconstruction problem by deconstructing it into a set of
 smaller sub-problems; specifically we will describe an algorithm which
 begins with 
\begin_inset Formula $L=1$
\end_inset

 and recursively expands to describe arbitrary sequence lengths.
 We know that the infinite causal states are maximally predictive of 
\begin_inset Formula $X$
\end_inset

, we now begin to address the issue of computational complexity, and we
 will develop some heuristics to optimize a problem's computational complexity
 based on the structure of 
\begin_inset Formula $X$
\end_inset

.
\end_layout

\begin_layout Standard
We will deal with two methods of building from 
\begin_inset Formula $L=1$
\end_inset

 to some more useful length; 
\emph on
extension
\emph default
 and 
\emph on
translation
\emph default
.
 Extension allows us to extend the sequence length of a given set of causal
 states, while translation builds a hierarchical layer which combines causal
 states.
\end_layout

\begin_layout Standard
Extension takes a set of causal states 
\begin_inset Formula $\mathcal{X}^{L}$
\end_inset

 and determines the causal states of sequences one element longer.
 We'll refer to the process of reconstruction through extension as 
\begin_inset Formula $\psi:\mathcal{X}^{L}\times\mathbf{X}\longmapsto\mathcal{X}^{L+1}$
\end_inset

.
 Each iteration of the extension process produces a longer set of causal
 states.
\end_layout

\begin_layout Standard
Translation on the other hand creates a new problem space defined over the
 existing causal states.
 Translation allows us to build a hierarchical system in which each layer
 builds sequences of 
\emph on
causal states
\emph default
, rather than sequences of 
\emph on
observations
\emph default
.
 We refer to the process of translation as a mapping 
\begin_inset Formula $\mathcal{T}:\boldsymbol{\mathcal{X}}^{L}\longmapsto Y,\quad Y\in\mathcal{A}^{L}$
\end_inset

.
 While extension adds a single element to a sequence, translation allows
 us to add entire sequences together.
\end_layout

\begin_layout Itemize
Heuristics
\end_layout

\begin_layout Section
Transition Probabilities
\end_layout

\begin_layout Itemize
each layer both encodes the transition probabilities of lower layers and
 provides marginal probabilities for sequences with the layer
\end_layout

\begin_layout Section
Expected Values
\end_layout

\begin_layout Itemize
Conditional expected value can be calculated from marginal expected value
 of sequences
\end_layout

\begin_layout Chapter
Discretization
\end_layout

\begin_layout Standard
Pearson / KL divergence
\end_layout

\begin_layout Chapter
Probabilistic Data
\end_layout

\begin_layout Standard
Hidden process w/ noise, SVM compression
\end_layout

\begin_layout Chapter
Choice
\end_layout

\begin_layout Standard
Motivation, internal states, path exploration, non-discrete input space
\end_layout

\begin_layout Chapter
Performance
\end_layout

\begin_layout Standard
Rates of convergence, algorithmic complexity, distributed architecture
\end_layout

\begin_layout Chapter
Discussion
\end_layout

\begin_layout Itemize
Transform invariance other than time/shift
\end_layout

\begin_layout Itemize
Robustness of generalization ability
\end_layout

\begin_layout Itemize
Complex, context-sensitive planning
\end_layout

\begin_layout Itemize
Motivational creep
\end_layout

\begin_layout Itemize
Lateral inhibition
\end_layout

\begin_layout Itemize
Testing scenarios; quantifying success
\end_layout

\begin_layout Chapter
\start_of_appendix
Results
\end_layout

\begin_layout Section
Eunite Competition Data
\end_layout

\begin_layout Section
Santa Fe Data
\end_layout

\begin_layout Section
CATS Benchmark Data
\end_layout

\begin_layout Section
Results Summary
\end_layout

\begin_layout Chapter
Discussion of Existing Work
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Expectation-maximization_algorithm
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Stationary_process
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Ergodicity
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Mixing_(mathematics)
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Lyapunov_exponent
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Recurrence_quantification_analysis
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Recurrence_plot
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Autoregressive_fractionally_integrated_moving_averag
e
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Autocorrelation
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

%
\backslash
url{http://en.wikipedia.org/wiki/Linear_discriminant_analysis}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Hidden Markov Model
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

% cannot account for future states - only capable of prediction
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

% weak, short-term memory
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Box-Jenkins
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

% intended for simplistic processes with well-understood stationarity and
 periodicity
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

% http://en.wikipedia.org/wiki/Box-Jenkins
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Spectral Analysis
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

% assumes some type of frequency-domain decomposition.
  Frequency-domain signal representations do not do a very good job predicting
 time-domain values.
 
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Shrinking-
\begin_inset Formula $\epsilon$
\end_inset

 SVM Regression
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

% theoretical foundation weak; only compensates for the relevance of recent
 data.
  See Markhov problem
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Chapter
SVM Details & Optimizations
\end_layout

\begin_layout Section
Quadratic Optimization Problem
\end_layout

\begin_layout Section
Support Vector Decomposition 
\end_layout

\begin_layout Standard
\begin_inset Marginal
status collapsed

\begin_layout Plain Layout
This section is probably better as an appendix
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Due to the simplicity of the constraints in the optimization problem, it
 is possible to use the decomposition method of Osuna to reduce the memory
 requirements of the Parzen-SVM algorithm.
\end_layout

\begin_layout Subsection
Sub-Problem Definition 
\end_layout

\begin_layout Standard
The decomposition algorithm breaks 
\begin_inset Formula $X$
\end_inset

 into two working sets 
\begin_inset Formula $B,N$
\end_inset

, and attempts to optimize 
\begin_inset Formula $B$
\end_inset

 while keeping 
\begin_inset Formula $N$
\end_inset

 fixed.
 This results in the following iterative optimization problem where 
\begin_inset Formula $\boldsymbol{\beta}^{k}$
\end_inset

 denotes the result of the previous iteration:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
W(\boldsymbol{\beta}_{B}) & =\frac{1}{2}\begin{bmatrix}\boldsymbol{\beta}_{B}^{T} & (\boldsymbol{\beta}_{N}^{k})^{T}\end{bmatrix}\begin{bmatrix}P_{BB} & P_{BN}\\
P_{NB} & P_{NN}\end{bmatrix}\begin{bmatrix}\boldsymbol{\beta}_{B}\\
\boldsymbol{\beta}_{N}^{k}\end{bmatrix}-\begin{bmatrix}q_{B}^{T} & q_{N}^{T}\end{bmatrix}\begin{bmatrix}\boldsymbol{\beta}_{B}\\
\boldsymbol{\beta}_{N}^{k}\end{bmatrix}\\
 & =\frac{1}{2}\boldsymbol{\beta}_{B}^{T}P_{BB}\boldsymbol{\beta}_{B}-(-q_{B}+P_{BN}\boldsymbol{\beta}_{N}^{k})^{T}\boldsymbol{\beta}_{B}\\
 & =\frac{1}{2}\begin{bmatrix}\beta_{i} & \beta_{j}\end{bmatrix}\begin{bmatrix}P_{ii} & P_{ij}\\
P_{ij} & P_{jj}\end{bmatrix}\begin{bmatrix}\beta_{i}\\
\beta_{j}\end{bmatrix}-(-q_{B}+P_{BN}\boldsymbol{\beta}_{N}^{k})^{T}\begin{bmatrix}\beta_{i}\\
\beta_{j}\end{bmatrix}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\text{subject to}\quad0\le\beta_{i},\beta_{j},\quad\beta_{i}+\beta_{j}=1-\mathbf{1}^{T}\boldsymbol{\beta}_{N}^{k}\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
 Working Set Selection 
\end_layout

\begin_layout Standard
Select
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align}
 & i\in\text{arg}\max_{t}\left\{ -\nabla f(\boldsymbol{\beta}^{k})_{t}\ |\quad t\in I(\boldsymbol{\beta}^{k})\right\} \\
 & j\in\text{arg}\min_{t}\left\{ -\frac{b_{it}^{2}}{a_{it}}\ |\quad t\in I(\boldsymbol{\beta}^{k}),\quad-\nabla f(\boldsymbol{\beta}^{k})_{t}<-\nabla f(\boldsymbol{\beta}^{k})_{i}\right\} \end{align}

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset ERT
status open

\begin_layout Plain Layout

% This needs to be checked for the new optimization scenario
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align}
I(\boldsymbol{\beta}) & \equiv\{t\ |\quad\beta_{t}<1\quad\text{or}\quad\beta_{t}>0\}\\
 & a_{it}=P_{ii}+P_{tt}-2P_{it}\\
 & \bar{a}_{it}=\begin{cases}
a_{it} & \text{if}\ a_{it}>0\\
\delta & \text{otherwise}\end{cases}\\
 & b_{it}=-\nabla f(\boldsymbol{\beta}^{k})_{i}+\nabla f(\boldsymbol{\beta}^{k})_{t}\\
\nabla f(\boldsymbol{\beta})_{i} & \equiv P_{i}\boldsymbol{\beta}-q_{i}\end{align}

\end_inset


\end_layout

\begin_layout Subsection
Stopping Condition 
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\max_{i\in I(\boldsymbol{\alpha}^{k})}-\nabla f(\boldsymbol{\alpha})_{i}+\min_{j\in I(\boldsymbol{\alpha}^{k})}\nabla f(\boldsymbol{\alpha})_{j}\le\epsilon\end{equation}

\end_inset


\end_layout

\begin_layout Section
Cascade SVM
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "Hazan08"

\end_inset


\end_layout

\begin_layout Section
Parallel SVM Decomposition
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "Graf04"

\end_inset


\end_layout

\begin_layout Chapter
Further Research
\end_layout

\begin_layout Section
Data Pre-Processing
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

% logistic function using mean and sd to put most training points between
 .1 and .9
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

% logistic function from delta using mean and sd in same way if data non-station
ary
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

% iterative integration process until stationary data found ?
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Performance
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "Research/research.bib"
options "bibtotoc,plain"

\end_inset


\end_layout

\end_body
\end_document
