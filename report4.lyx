#LyX 1.6.5 created this file. For more info see http://www.lyx.org/
\lyxformat 345
\begin_document
\begin_header
\textclass amsbook
\begin_preamble



\usepackage{amsfonts}
\end_preamble
\use_default_options false
\begin_modules
theorems-ams
eqs-within-sections
figs-within-sections
\end_modules
\language english
\inputencoding auto
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\paperfontsize 10
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 0
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\leftmargin 1.5in
\topmargin 1in
\rightmargin 1.5in
\bottommargin 1.25in
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\author "" 
\author "" 
\end_header

\begin_body

\begin_layout Title
Acting Machines
\end_layout

\begin_layout Author
Ryan Michael
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset nomencl_print
LatexCommand printnomenclature

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Section*
Introduction
\end_layout

\begin_layout Standard
Wikipedia defines 
\begin_inset Quotes eld
\end_inset

Machine Learning
\begin_inset Quotes erd
\end_inset

 as 
\emph on
a scientific discipline that is concerned with the design and development
 of algorithms that allow computers to learn based on data
\emph default
.
 Machine learning deals with the development of Learning Machines (LM's)
 capable of autonomously generating probabalistic models using sets of observati
ons.
 This text builds upon the foundation of Machine Learning to develop an
 Acting Machine (AM); a learning machine capable of interacting with with
 its environment in a motivated way.
 
\end_layout

\begin_layout Chapter
Motivation
\end_layout

\begin_layout Standard
Our objective is to design an system capable of making informed choices
 about the actions it takes in its environment.
 In the most general sense, the system is composed of three types of data;
 a set of 
\emph on
actions
\emph default
 which the system has taken in the past, a set of 
\emph on
internal states
\emph default
 whose values motivate the system and a set of 
\emph on
observations
\emph default
 describing the system's environment.
 The system can be viewed as a process which takes this information describing
 it's experience up to the present and generates a new set of actions which
 are expected to have a specific effect on the system's internal states.
\end_layout

\begin_layout Standard
We would like to design a system which makes as few assumptions about the
 nature and behavior of its environment as possible, and we would like to
 show that the system provides the best possible performance under those
 assumptions.
 In cases where computational resources determine the best possible performance
 of the system, we would like to use controlling parameters which are intuitive
 and directly relevant to the underlying processes.
\end_layout

\begin_layout Chapter
Epsilon-Machines
\end_layout

\begin_layout Standard
The most basic task the system must perform is to correlate causes with
 their effects.
 To accomplish this task we begin with a concept developed in computational
 mechanics called the 
\begin_inset Formula $\epsilon$
\end_inset

-machine.
 The discussion of these concepts will be heavily influenced by 
\begin_inset CommandInset citation
LatexCommand cite
key "Shal01"

\end_inset

.
\end_layout

\begin_layout Standard
An 
\begin_inset Formula $\epsilon$
\end_inset

-machine is an approach to discovering patterns within a data set.
 The general idea of a pattern is that an object 
\begin_inset Formula $\mathcal{O}$
\end_inset

 has a pattern 
\begin_inset Formula $\mathcal{P}$
\end_inset

 if and only if we can use 
\begin_inset Formula $\mathcal{P}$
\end_inset

 to compress or predict 
\begin_inset Formula $\mathcal{O}$
\end_inset

 in some way.
 In this sense, a pattern is a representation of some underlying structure,
 regularity, symmetry, etc contained in a data set.
 It is this underlying structure which allows us to make justifiable statements
 about the causal relations underlying a data-set.
 The notion of causality which we will use is highly restricted; we will
 refer to an event 
\begin_inset Formula $A$
\end_inset

 causing an event 
\begin_inset Formula $B$
\end_inset

 if 
\begin_inset Formula $B$
\end_inset

 generally follows 
\begin_inset Formula $A$
\end_inset

.
\end_layout

\begin_layout Section
Histories and Futures
\end_layout

\begin_layout Standard
We will begin our discussion of patterns by assuming discrete-valued, discrete-t
ime mixing processes.
 This essentially means that our data set is arranged into a set of 
\begin_inset Formula $t$
\end_inset

-indexed observations 
\begin_inset Formula $S{}_{t},\ S\in\mathcal{A}$
\end_inset

 where each observation takes values from in some finite set 
\begin_inset Formula $\mathcal{A}$
\end_inset

, and the dependence between observations decreases with time; 
\begin_inset Formula $\lim_{t\rightarrow\infty}\Pr(S_{t}|S_{i})=\Pr(S_{t})\quad\forall S_{i}$
\end_inset

.
 
\end_layout

\begin_layout Standard
Define the set of observations leading up to and including 
\begin_inset Formula $s_{i}$
\end_inset

 as a 
\emph on
history
\emph default
 denoted 
\begin_inset Formula $\overleftarrow{S_{i}}$
\end_inset

, and the set of observations after as a 
\emph on
future
\emph default
 denoted 
\begin_inset Formula $\overrightarrow{S_{i}}$
\end_inset

.
 Our goal is to predict the future
\begin_inset Formula $ $
\end_inset


\begin_inset Formula $\overrightarrow{S}$
\end_inset

 based on the past 
\begin_inset Formula $\overleftarrow{S}$
\end_inset

.
 We begin by taking the set 
\begin_inset Formula $\overleftarrow{\mathbf{S}}$
\end_inset

 of all pasts and splitting it into mutually exclusive subsets 
\begin_inset Formula $\boldsymbol{\mathcal{R}}$
\end_inset

.
 Each 
\begin_inset Formula $\rho\in\boldsymbol{\mathcal{R}}$
\end_inset

 will be called a 
\emph on
state
\emph default
.
 When the current history 
\begin_inset Formula $\overleftarrow{s}$
\end_inset

 is included in the set 
\begin_inset Formula $\rho$
\end_inset

 we refer to the process as being 
\emph on
in state
\emph default
 
\begin_inset Formula $\rho$
\end_inset

.
 Thus, we define a mapping 
\begin_inset Formula $\eta$
\end_inset

 from histories to effective states:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\eta:\overleftarrow{\mathbf{S}} & \longmapsto\boldsymbol{\mathcal{R}}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
A specific individual history 
\begin_inset Formula $\overleftarrow{s}\in\overleftarrow{\mathbf{S}}$
\end_inset

 maps to a specific state 
\begin_inset Formula $\rho\in\boldsymbol{\mathcal{R}}$
\end_inset

.
 
\end_layout

\begin_layout Section
Information Entropy
\end_layout

\begin_layout Standard
We would like to discuss the uncertainty of the future.
 We will use the concept of information entropy 
\begin_inset Formula $H[X]$
\end_inset

 to describe the amount of uncertainty a given future contains.
 The information entropy describes the unpredictability of a random process.
 For example, a series of coin tosses with a fair coin has maximum entropy,
 since there is no way to predict what will come next.
 A string of coin tosses with a two-headed coin has zero entropy, since
 the coin will always come up heads.
 The information entropy of a process 
\begin_inset Formula $X$
\end_inset

 taking values in 
\begin_inset Formula $\mathcal{A}$
\end_inset

 is defined as 
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
H[X] & \equiv-\sum_{x\in\mathcal{A}}\Pr(X=x)\log\Pr(X=x)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We can likewise describe the joint information entropy; the unpredictability
 of joint observations of two processes 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 taking values in 
\begin_inset Formula $\mathcal{A}$
\end_inset

 and 
\begin_inset Formula $\mathcal{B}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
H[X,Y] & \equiv-\sum_{(x,y)\in\mathcal{A}\times\mathcal{B}}\Pr(X=x,Y=y)\log\Pr(X=x,Y=y)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
as well as the conditional information entropy; the unpredictability of
 
\begin_inset Formula $X$
\end_inset

 given that the state of 
\begin_inset Formula $Y$
\end_inset

 is known
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
H[X|Y] & \equiv H[X,Y]-H[Y]\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Finally, we can describe the mutual information 
\begin_inset Formula $I[X;Y]$
\end_inset

 which describes the average reduction in uncertainty about 
\begin_inset Formula $X$
\end_inset

 produced by knowing 
\begin_inset Formula $Y$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $ $
\end_inset


\begin_inset Formula \begin{align*}
I[X;Y] & \equiv H[X]-H[X|Y]\end{align*}

\end_inset


\end_layout

\begin_layout Section
Causal States
\end_layout

\begin_layout Standard
We can now define the causal states of a process.
 A process 
\begin_inset Formula $X$
\end_inset

 has a set of causal states 
\begin_inset Formula $\epsilon(\overleftarrow{x})$
\end_inset

 defined as 
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\epsilon(\overleftarrow{x}) & \equiv\left\{ \overleftarrow{x\prime}|\ \Pr\left(\overrightarrow{X}\in F|\overleftarrow{X}=\overleftarrow{x}\right)=\Pr\left(\overrightarrow{X}\in F|\overleftarrow{X}=\overleftarrow{x\prime}\right),\ \forall F\in\sigma(\overrightarrow{X}),\overleftarrow{x\prime}\in\overleftarrow{X}\right\} \end{align*}

\end_inset


\end_layout

\begin_layout Standard
In other words, a causal state 
\begin_inset Formula $\epsilon(\overleftarrow{x})$
\end_inset

 is the set of all histories 
\begin_inset Formula $\overleftarrow{x\prime}$
\end_inset

 for which the probability of the future 
\begin_inset Formula $\overrightarrow{X}$
\end_inset

 taking some value 
\begin_inset Formula $F$
\end_inset

 is the same.
 A causal state encodes the set of paths through time leading to a given
 outcome.
 We denote a give causal state 
\begin_inset Formula $\mathcal{X}_{i}$
\end_inset

 and the set of all causal states for a process 
\begin_inset Formula $\boldsymbol{\mathcal{X}}$
\end_inset

.
 
\end_layout

\begin_layout Standard
Causal states are of interest to us because they fully describe the causal
 structure of the process from which they are derived.
 We describe the causal states as a 
\emph on
sufficient statistic
\emph default
 for the process 
\begin_inset Formula $\mathbf{S}$
\end_inset

, which is to say that the mutual information between the future and the
 past is the same as the between the future and the causal states
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
I[\overrightarrow{X};\mathcal{X}] & =I[\overrightarrow{X};\overleftarrow{X}]\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Anything you can learn about 
\begin_inset Formula $\overrightarrow{S}$
\end_inset

 from the past can also be learned from the causal states.
\end_layout

\begin_layout Standard
Another attractive property of the causal states is that they are the most
 compact sufficient statistic possible.
 We describe the causal states as a 
\emph on
minimal
\emph default
.
 It can be shown
\begin_inset CommandInset citation
LatexCommand cite
key "Shal01"

\end_inset

 that the entropy of the causal states 
\begin_inset Formula $H[\boldsymbol{\mathcal{S}}]$
\end_inset

 is smaller than any other partitioning of the past which is equally predictive
 of the future 
\begin_inset Formula $\hat{\boldsymbol{\mathcal{R}}}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
H[\overrightarrow{X}|\hat{\mathcal{R}}] & =H[\overrightarrow{X}|\mathcal{X}]\\
H[\boldsymbol{\mathcal{X}}] & \le H[\hat{\boldsymbol{\mathcal{R}}}]\end{align*}

\end_inset


\end_layout

\begin_layout Standard
This means that the causal states describe the future of 
\begin_inset Formula $\mathbf{S}$
\end_inset

 using the least possible amount if information.
\end_layout

\begin_layout Section
Reconstruction
\end_layout

\begin_layout Itemize
Reconstruction
\end_layout

\begin_layout Itemize
Transition Matrix
\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Standard
We have established the foundations of an acting machine by describing a
 generalized problem domain and showing the domain has several useful properties.
 Having shown that the causal states fully and minimally describe the underlying
 data-generating process we can now proceed to both refine our understanding
 of causal states and develop an algorithm to reconstruct them from a set
 of observations.
 
\end_layout

\begin_layout Standard
Unfortunately the task of reconstruction remains undefined, and for infinite
 sequences is computationally intractable.
 In the next chapter we will begin making the 
\begin_inset Formula $\epsilon$
\end_inset

-machine reconstruction task more tractable by considering finite sequences
 and developing a general algorithm to implement reconstruction.
\end_layout

\begin_layout Chapter
Hierarchy
\end_layout

\begin_layout Standard
Extension / Layering
\end_layout

\begin_layout Section
Mixing
\end_layout

\begin_layout Standard
The concept of mixing processes has been briefly mentioned, however it deserves
 more discussion.
 The original formulation of 
\begin_inset Formula $\epsilon$
\end_inset

-machines assumed that 
\begin_inset Formula $X$
\end_inset

 was stationary; that 
\begin_inset Formula $\Pr(X_{i}=x)=\Pr(X_{j}=x),\ \forall i,j$
\end_inset

.
 While this assumption makes some of the math simpler, it is a relatiely
 rigid constraint; many real-world processes which we would like to model
 do not satisfy the constraint that their probability distribution is time-invar
iant
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
Notice that while the distribution of a stationary process is constant,
 the paths traced by an observation of such a process can still have structure,
 and that structure can change over time.
 This is to say that while 
\begin_inset Formula $\Pr(S)$
\end_inset

 is constant, 
\begin_inset Formula $\Pr(S_{i}|S_{j})$
\end_inset

 need not be.
\end_layout

\end_inset

.
 We therefore use the weaker assumption of a mixing process.
\end_layout

\begin_layout Standard
Mixing processes are stochastic processes in which the dependance of the
 future on the past decreases with time.
 We will make the two assumptions about the process 
\begin_inset Formula $S$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\lim_{\tau\rightarrow\infty}\Pr(\overrightarrow{X_{i+\tau}}|\overleftarrow{X_{i}}) & =\Pr(\overrightarrow{X_{i}})\\
\int_{o}^{\infty}\Pr(\overrightarrow{X_{i+\tau}}|\overleftarrow{X_{i}})-\Pr(\overrightarrow{X_{i}}) & <\infty\end{align*}

\end_inset


\end_layout

\begin_layout Standard
In other words, the future is independant of the distant past, and the correlati
on between the past and the future decays fast enough that the influence
 of the past is bounded.
 
\end_layout

\begin_layout Itemize
Address the implications of mixing on CM & proofs of minimality & sufficiency
\end_layout

\begin_layout Section
Finite Sequences
\end_layout

\begin_layout Standard
It is now time to move away from infinite pasts, and thus from maximal reduction
s in uncertainty.
 We have shown that the causal states 
\begin_inset Formula $\boldsymbol{\mathcal{X}}$
\end_inset

 represent the maximal reduction in uncertainty about 
\begin_inset Formula $X$
\end_inset

 possible.
 We are unlikely to deal with infinite sequences of data in daily life;
 any analysis of real-world data will necessarily be finite.
 We can model this in our algorithm by adding an error term 
\begin_inset Formula $\xi$
\end_inset

 which describes either the maximal uncertainty we are willing to 'trade'
 for shorter sequences in 
\begin_inset Formula $\mathbf{X}$
\end_inset

 (when working from an infinite set) or the uncertainty resulting from a
 given set of sequences 
\begin_inset Formula $\mathbf{X}$
\end_inset

 (when working from actual data).
 
\end_layout

\begin_layout Standard
We can define the causal states 
\begin_inset Formula $\boldsymbol{\mathcal{X}}^{L}$
\end_inset

 of the set of finite sequences of length 
\begin_inset Formula $L$
\end_inset

; the uncertainty of the finite causal states differs from the uncertainty
 of the infinite causal states by at most the error term 
\begin_inset Formula $\varepsilon$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
H[\boldsymbol{\mathcal{X}}^{L}]+\varepsilon & \le H[\boldsymbol{\mathcal{X}}]\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Fortunately it can be shown that under the assumption of sequences of length
 
\begin_inset Formula $L$
\end_inset

 the properties of minimality and sufficiency are retained.
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
I[\overrightarrow{X}^{L};\mathcal{X}^{L}] & =I[\overrightarrow{X}^{L};\overleftarrow{X}^{L}]\\
H[\overrightarrow{X}^{L}|\hat{\mathcal{R}}^{L}] & =H[\overrightarrow{X}^{L}|\mathcal{X}^{L}]\\
H[\boldsymbol{\mathcal{X}}^{L}] & \le H[\hat{\boldsymbol{\mathcal{R}}}^{L}]\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Another useful property of finite causal states is that reconstructing the
 causal states for sequences of length 
\begin_inset Formula $L$
\end_inset

 constitutes a partial reconstruction of sequences of length 
\begin_inset Formula $L+1$
\end_inset

.
 Intuitively, if a causal state 
\begin_inset Formula $\mathcal{X}^{L}$
\end_inset

 of length 
\begin_inset Formula $L$
\end_inset

 contains all the sequences preceeding some value 
\begin_inset Formula $x$
\end_inset

, than it must be true that the sequence defined by the last 
\begin_inset Formula $L$
\end_inset

 elements of any sequence in 
\begin_inset Formula $\mathcal{X}^{L+1}$
\end_inset

 are members of 
\begin_inset Formula $\mathcal{X}$
\end_inset

.
 If we consider the causal states as a partioning of 
\begin_inset Formula $\mathbf{X}$
\end_inset

, we can state that the causal states 
\begin_inset Formula $\mathcal{X}^{L+1}$
\end_inset

 will be partitions of 
\begin_inset Formula $\mathcal{X}^{L}$
\end_inset

, in the same way that 
\begin_inset Formula $\mathcal{X}^{L}$
\end_inset

 will be partitions of 
\begin_inset Formula $\mathbf{X}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Relevance of mixing to sequence length
\end_layout

\begin_layout Itemize
Monotonicity of entropy convergence as L increases
\end_layout

\begin_layout Section
Reconstruction Deconstruction
\end_layout

\begin_layout Standard
We approach the reconstruction problem by deconstructing it into a set of
 smaller sub-problems; specifically we will describe an algorithm which
 begins with 
\begin_inset Formula $L=1$
\end_inset

 and recursively expands to describe arbitrary sequence lengths.
 We know that the infinite causal states are maximally predictive of 
\begin_inset Formula $X$
\end_inset

, we now begin to address the issue of computational complexity, and we
 will develop some heuristics to optimize a problem's computational complexity
 based on the structure of 
\begin_inset Formula $X$
\end_inset

.
\end_layout

\begin_layout Standard
We will deal with two methods of building from 
\begin_inset Formula $L=1$
\end_inset

 to some more useful length; 
\emph on
extension
\emph default
 and 
\emph on
translation
\emph default
.
 Extension allows us to extend the sequence length of a given set of causal
 states, while translation builds a hierarchical layer which combines causal
 states.
\end_layout

\begin_layout Standard
Extension takes a set of causal states 
\begin_inset Formula $\mathcal{X}^{L}$
\end_inset

 and determines the causal states of sequences one element longer.
 We'll refer to the process of reconstruction through extension as 
\begin_inset Formula $\psi:\mathcal{X}^{L}\times\mathbf{X}\longmapsto\mathcal{X}^{L+1}$
\end_inset

.
 Each iteration of the extension process produces a longer set of causal
 states.
\end_layout

\begin_layout Standard
Translation on the other hand creates a new problem space defined over the
 existing causal states.
 Translation allows us to build a hierarchical system in which each layer
 builds sequences of 
\emph on
causal states
\emph default
, rather than sequences of 
\emph on
observations
\emph default
.
 We refer to the process of translation as a mapping 
\begin_inset Formula $\mathcal{T}:\boldsymbol{\mathcal{X}}^{L}\longmapsto Y,\quad Y\in\mathcal{A}^{L}$
\end_inset

.
 While extension adds a single element to a sequence, translation allows
 us to add entire sequences together.
\end_layout

\begin_layout Itemize
Heuristics (later section - part of optimization?)
\end_layout

\begin_layout Section
Transition Probabilities
\end_layout

\begin_layout Standard
The traditional 
\begin_inset Formula $ $
\end_inset


\begin_inset Formula $\epsilon$
\end_inset

-machine is a monoid
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
A monoid is a set with an identity function
\end_layout

\end_inset

 
\begin_inset Formula $(\epsilon,T)$
\end_inset

 describing the set of causal states and the transition probability between
 states.
 The transition probability is concieved as the probability that the next
 observation of the sequance 
\begin_inset Formula $\overleftarrow{X}_{t}$
\end_inset

 will induce the causal state 
\begin_inset Formula $\mathcal{X}_{t+1}$
\end_inset

 given the current causal state 
\begin_inset Formula $\mathcal{X}_{t}$
\end_inset

.
 For our purposes however, the transition probability function 
\begin_inset Formula $T$
\end_inset

 will describe the probability that the next full sequence of length 
\begin_inset Formula $L$
\end_inset

 will induce the causal state 
\begin_inset Formula $\mathcal{X}_{t+L}$
\end_inset

 given the current causal state 
\begin_inset Formula $\mathcal{X}_{t}$
\end_inset

.
 That is, while the traditional formulation describes the probability of
 observing a point 
\begin_inset Formula $x\in\mathcal{A}$
\end_inset

 after having observed 
\begin_inset Formula $\mathcal{X}_{t}$
\end_inset

, our approach will describe the probability of observing a causal state
 
\begin_inset Formula $\mathcal{X}^{L}\in\boldsymbol{\mathcal{X}}^{L}$
\end_inset

 after having observed 
\begin_inset Formula $\mathcal{X}_{t}^{L}$
\end_inset

.
 
\end_layout

\begin_layout Standard
This approach allows us to treat marginal probabilities in the domain 
\begin_inset Formula $Y$
\end_inset

 as conditional probabilities in the domain 
\begin_inset Formula $X$
\end_inset

.
 Consider the case where causal states 
\begin_inset Formula $\mathcal{Y}$
\end_inset

 are defined as binary pairs of causal states (in other words, 
\begin_inset Formula $L=2$
\end_inset

) drawn from 
\begin_inset Formula $\boldsymbol{\mathcal{X}}$
\end_inset

 where the marginal probability of 
\begin_inset Formula $\boldsymbol{\mathcal{X}}$
\end_inset

 is known.
 In this case determining the marginal probability 
\begin_inset Formula $\Pr(\mathcal{Y})$
\end_inset

 where 
\begin_inset Formula $\mathcal{Y}=\{\mathcal{X}_{i},\mathcal{X}_{j}\}$
\end_inset

 tells us the conditional probability 
\begin_inset Formula $\Pr(\mathcal{X}_{j}|\mathcal{X}_{i})$
\end_inset

.
 Working from the definition of conditional probability
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\Pr(A|B) & =\frac{\Pr(A\cap B)}{\Pr(B)}\\
\Pr(\mathcal{X}_{j}|\mathcal{X}_{i}) & =\frac{\Pr(\mathcal{X}_{j}\cap\mathcal{X}_{i})}{\Pr(\mathcal{X}_{i})}\\
 & =\frac{\Pr(\mathcal{Y})}{\Pr(\mathcal{X}_{i})}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Clearly this can be extended to sequences in 
\begin_inset Formula $Y$
\end_inset

 of arbitrary length
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
Notice that this equation holds for any 'missing' element of the set - we
 could just as easily describe the conditional probability of the first
 element give the rest, or any of the intermediate elements given their
 compliment.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\Pr(\mathcal{X}_{n}|\mathcal{X}_{1},...,\mathcal{X}_{n-1}) & =\frac{\Pr(\mathcal{Y})}{\Pr(\mathcal{X}_{1},...,\mathcal{X}_{n-1})},\quad\mathcal{Y}=\{\mathcal{X}_{1},...,\mathcal{X}_{n}\}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
This shows that as we translate the problem domain into higher dimensions
 we extend our prediction horizion by increasing the amount of information
 available to the system about the process.
\end_layout

\begin_layout Section
Expected Values
\end_layout

\begin_layout Itemize
Conditional expected value can be calculated from marginal expected value
 of sequences
\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Standard
In the previous chapter we developed a general approach to learning about
 the future from the past; in this chapter we have begun to develop an algorithm
 for accomplishing that task.
 We have broken the problem of 
\begin_inset Formula $\epsilon$
\end_inset

-machine reconstruction into two basic operations; extension and translation.
 By combining these two operations we can simultaneously increase sequence
 length and generate transition probabilites.
 With this hierarchical description of a process in hand, we can make contextual
ized predictions of the processes' evolution over time.
\end_layout

\begin_layout Standard
We have introduced the system's first control variable; the sequence length
 
\begin_inset Formula $L$
\end_inset

.
 Under the mixing assumption we know that there exists some value of 
\begin_inset Formula $L$
\end_inset

 for which the entropy of the finite causal states is 'acceptably' close
 to the entropy of the infinite causal states, and that increasing 
\begin_inset Formula $L$
\end_inset

 monotonically decreases the difference between the two.
\end_layout

\begin_layout Standard
One aspect of the system which needs more consideration is the domain 
\begin_inset Formula $\mathcal{A}$
\end_inset

 of the process.
 We have restricted this domain to countable sets, however this restriction
 requires some 'cheating' to work on many classes of real-world data.
 In the next section we will develop a method to translate a broader class
 of processes into the domain 
\begin_inset Formula $\mathcal{A}$
\end_inset

.
\end_layout

\begin_layout Chapter
Probabalistic Data
\end_layout

\begin_layout Standard
The discrete nature of 
\begin_inset Formula $\mathcal{A}$
\end_inset

 would seem to limit the types of processes the system can handle.
 In this chapter we will use concepts from machine learning to show that
 a broader class of data can be treated as equivalent to 
\begin_inset Formula $\mathcal{A}$
\end_inset

 for our purposes.
 We will focus on real-valued discrete-time data of arbitrary dimension,
 however we will show later that any metric space will suffice.
 
\end_layout

\begin_layout Standard
To accomplish this we will introduce a noise parameter to 
\begin_inset Formula $X$
\end_inset

, allowing us to treat unique observations as the same if they are sufficiently
 similar.
 We will then show how support vector machines (SVM's) can be applied to
 the reconstruction task to retain the properties of uniqueness, minimality
 and sufficiency for our causal states.
\end_layout

\begin_layout Section
Noise
\end_layout

\begin_layout Standard
We have been assuming that the process we are trying to model is directly
 observable - that our observations 
\begin_inset Formula $x_{i}$
\end_inset

 are direct realizations of the process 
\begin_inset Formula $X$
\end_inset

.
 Let us consider the case where our observations are drawn from a hidden
 process 
\begin_inset Formula $Z$
\end_inset

 with added noise
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
X & =Z+\xi\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Clearly, the nature of 
\begin_inset Formula $\xi$
\end_inset

 is important; if it is unbounded and purely random it is possible that
 the dynamics of 
\begin_inset Formula $\xi$
\end_inset

 will entirely dominate the dynamics of 
\begin_inset Formula $X$
\end_inset

.
 We will assume that 
\begin_inset Formula $\xi$
\end_inset

 is bounded and normally distributed.
 We will see later that the magnitude of 
\begin_inset Formula $\xi$
\end_inset

 will affect the rate at which the system is able to learn about 
\begin_inset Formula $Z$
\end_inset

.
\end_layout

\begin_layout Standard
While we are still trying to describe the dynamics of 
\begin_inset Formula $Z$
\end_inset

, we must do so with corrupted observations.
 We must take a probabalistic approach not only to the transition probabilities
 between causal states, but to the definition of causal states themselves.
 That is we must assume that a given sequence of observations has a specific
 likelihood of exhibiting the dynamics of a given causal state.
\end_layout

\begin_layout Standard
While the loss of certainty might seem unfortunate, it opens to door to
 a richer class of data sets.
 Without uncertainty we could only learn about a process if exact repetitions
 existed in the set of observations - these repetitions allow us to correlate
 the histories leading up to them and generate a set of causal states which
 could be more compact than 
\begin_inset Formula $\mathbf{S}$
\end_inset

.
 With uncertainty, even if two observations have the same value we cannot
 know if they were drawn from the same underlying values in 
\begin_inset Formula $Z$
\end_inset

.
 The best we can do is consider regions of the input space.
 Using this approach, all the observations in a region of space are likely
 to be drawn from the same value of 
\begin_inset Formula $z$
\end_inset

, and thus are likely to have similar dynamics.
 This shift of focus from values to regions will allow us to essentially
 work with intervals over the reals.
\end_layout

\begin_layout Section
Emprirical Risk Minimization
\end_layout

\begin_layout Standard
Support Vector Machines were developed as a way to directly estimate probability
 distributions given a set of empirical observations.
 The main benefits of SVM's is that the optimization problems they solve
 have unique solutions, and those solutions can be shown to be the minimal
 in solving the given problem.
 SVM's have well-defined computational complexity and error bounds.
\end_layout

\begin_layout Standard
The problem which an SVM solves is referred to as 
\emph on
empirical risk minimization
\emph default
 (ERM).
 As it's name implies, empirical risk minimization attempts to minimize
 the 
\emph on
risk
\emph default
 of a set of empirical observations of some underlying random variable.
 The term 'risk' refers to the uncertainty associated with a model with
 respect to the uncertainty of the underlying process.
 
\end_layout

\begin_layout Standard
Following (Vapnik); given a random process 
\begin_inset Formula $\Phi(x)\longmapsto(x,y)$
\end_inset

 which generates values for 
\begin_inset Formula $y$
\end_inset

 based on some unknown conditional probability distribution 
\begin_inset Formula $\Pr(y|x)$
\end_inset

, our task is to construct a function 
\begin_inset Formula $\Lambda(x)\longmapsto(x,y)$
\end_inset

 which estimates the distribution of 
\begin_inset Formula $y$
\end_inset

 based on a set of observations 
\begin_inset Formula $(x_{1},y_{1}),\ldots,(x_{\ell},y_{\ell})$
\end_inset

.
 We will use a function called the loss 
\begin_inset Formula $L(\Phi,\Lambda)$
\end_inset

 which describes the difference between the predicted and actual value of
 
\begin_inset Formula $y$
\end_inset

 given 
\begin_inset Formula $x$
\end_inset

.
 The risk 
\begin_inset Formula $R$
\end_inset

 is defined as the sum of the loss function over the set of 
\begin_inset Formula $\ell$
\end_inset

 observations.
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
R(\Lambda) & =\sum_{i=1}^{\ell}L(y_{i},\Lambda(x_{i}))\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The ERM principle should now be clear; we wish to find the function 
\begin_inset Formula $\Lambda$
\end_inset

 with the smallest risk 
\begin_inset Formula $R$
\end_inset

.
 Another way of framing this problem is to treat 
\begin_inset Formula $\Lambda$
\end_inset

 as a 
\emph on
class
\emph default
 of estimators whose behavior is determined by some parameter 
\begin_inset Formula $\alpha$
\end_inset

; in this case we wish to find the value of 
\begin_inset Formula $\alpha$
\end_inset

 which minimizes the equivalent risk function
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
R(\alpha) & =\sum_{i=1}^{\ell}L(y_{i},\Lambda(x_{i},\alpha))\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The SVM task can now be stated as selecting a loss function 
\begin_inset Formula $L$
\end_inset

 and a class of estimators 
\begin_inset Formula $\Lambda$
\end_inset

 in such a way that the problem of optimizing 
\begin_inset Formula $\alpha$
\end_inset

 is computationally tractable and the resulting estimator has useful properties.
\end_layout

\begin_layout Section
Support Vector Machines
\end_layout

\begin_layout Standard
Support Vector Machines implement the ERM principle using two assumptions.
 The first assumption is that the estimator 
\begin_inset Formula $\Lambda$
\end_inset

 is paramaterized by a set of observations 
\begin_inset Formula $X=\{(x_{1},y_{1}),\ldots,(x_{\ell},y_{\ell})\}$
\end_inset

 and a set of associated weights 
\begin_inset Formula $\beta=\{\beta_{1},\ldots,\beta_{\ell}\}$
\end_inset

.
 The second assumption is that the set of observations 
\begin_inset Formula $X$
\end_inset

 can be mapped into some feature space 
\begin_inset Formula $\Psi$
\end_inset

 in which the optimization task is computationally tractable.
\end_layout

\begin_layout Standard
The first assumption means that the estimator 
\begin_inset Formula $\Lambda$
\end_inset

 generates estimates using the observations 
\begin_inset Formula $X$
\end_inset

 in some way, 
\emph on
and no other information
\emph default
.
 SVM's execute an optimization process to determine how to combine a set
 of observations in some pre-determined manner, and the end result of the
 optimization task is the set of weights 
\begin_inset Formula $\beta$
\end_inset

.
 The estimator is fully defined by the set 
\begin_inset Formula $(\beta,X)$
\end_inset

.
 It is possible that there exists a parameter 
\begin_inset Formula $\alpha$
\end_inset

 with lower entropy than this set.
\end_layout

\begin_layout Standard
The second assumption can be made more intuitive by considering the case
 of binary classification.
 Given a dataset where 
\begin_inset Formula $x\in\mathbb{R}^{2},\ y\in\{0,1\}$
\end_inset


\begin_inset Foot
status collapsed

\begin_layout Plain Layout
This describes a set of points in two-dimensional space with an associated
 value of either 0 or 1
\end_layout

\end_inset

, the task is to produce a function which predicts if a given value of 
\begin_inset Formula $x$
\end_inset

 will be 0 or 1.
 If it happens that a line can be drawn across 
\begin_inset Formula $\mathbb{R}^{2}$
\end_inset

 which separates the set 
\begin_inset Formula $\{y:\ y=0\}$
\end_inset

 from the set 
\begin_inset Formula $\{y:\ y=1\}$
\end_inset

 the optimization problem can be formulated as a simple linear optimization
 task (Vapnik 5.2).
 If this is not the case, however, selecting the optimal decision rule is
 NP-complete
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
The time required to find a solution increases exponentially with the number
 of observations.
 
\end_layout

\end_inset

.
 By mapping the set 
\begin_inset Formula $X$
\end_inset

 into a high-dimensional space defined by the product space 
\begin_inset Formula $X\times X$
\end_inset


\begin_inset Foot
status collapsed

\begin_layout Plain Layout
Imagine X as a set of 
\begin_inset Formula $\ell$
\end_inset

 real-valued observations arranged as a column vector.
 The product space would be the 
\begin_inset Formula $\ell\times\ell$
\end_inset

 matrix where each element 
\begin_inset Formula $\psi_{i,j}$
\end_inset

 describes the product 
\begin_inset Formula $x_{i}\cdot x_{j}$
\end_inset

.
\end_layout

\end_inset

, it is possible to formulate the problem as another linear optimization
 task (Vapnik 5.4).
 Conceptually, the NP-complete problem is defined by drawing boundaries
 in 
\begin_inset Formula $X$
\end_inset

, however we don't know how many partitions are needed or where they should
 be placed.
 Translating the set 
\begin_inset Formula $X$
\end_inset

 into its product space allows us to restrict our attention to the relationships
 between points - a much simpler problem
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
This may seem a little hand-wavey; the nature of the optimization problem
 in 
\begin_inset Formula $\Psi$
\end_inset

 depends on the class of estimator 
\begin_inset Formula $\Lambda$
\end_inset

 - we will develop this issue further soon.
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Standard
Various types of problems can be solved with SVM's; classification, regression,
 density estimation and clustering, to name a few.
 In general the problem is framed as either a linear or quadratic
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
A quadratic optimization problem is a problem which can be expressed in
 the form 
\begin_inset Formula $f(\mathbf{x})=\mathbf{x}^{T}Q\mathbf{x}+\mathbf{c}^{T}\mathbf{x}$
\end_inset

 subject to inequality constraints 
\begin_inset Formula $A\mathbf{x}\le\mathbf{b}$
\end_inset

 and inequality constraints 
\begin_inset Formula $E\mathbf{x}=\mathbf{d}$
\end_inset

, where 
\begin_inset Formula $\mathbf{x}$
\end_inset

 is a set of 
\begin_inset Formula $\ell$
\end_inset

 variables to be optimized, 
\begin_inset Formula $Q$
\end_inset

 is an 
\begin_inset Formula $\ell\times\ell$
\end_inset

 matrix, 
\begin_inset Formula $\mathbf{c},A,E$
\end_inset

 are 
\begin_inset Formula $1\times\ell$
\end_inset

 matrices and 
\begin_inset Formula $f$
\end_inset

 is the function to be minimized.
 Quadratic optimization problems are significant because their solutions
 are unique and methods exist to efficiently compute the minimal value of
 
\begin_inset Formula $\mathbf{x}$
\end_inset

.
\end_layout

\end_inset

 optimization problem where the product space 
\begin_inset Formula $\Psi$
\end_inset

 is defined using a kernel function 
\begin_inset Formula $K(x,y)$
\end_inset

 which follows the following rules
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
K(x,x) & =1\\
\int K(x,y)dy & =1\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Kernel functions can be though of as defining a neighborhood around a point
 
\begin_inset Formula $x$
\end_inset

 in the domain 
\begin_inset Formula $X$
\end_inset

; the points outside the neighborhood will have values approaching 0, while
 points within the neighborhood will have values describing their proximity
 to the 'center' of the neighborhood
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
This makes some assumptions about the specific kernel function, namely that
 it decreases with distance from 
\begin_inset Formula $x$
\end_inset

.
 We will generally be dealing with the radial basis kernel 
\begin_inset Formula $K(x,y)=e^{-\gamma/(x-y)}$
\end_inset

, for which these assumptions are true.
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
The specific class of SVM we will be using estimates probability densities
 from a set of observations.
 This class of SVM minimizes the function 
\end_layout

\begin_layout Standard
\begin_inset Formula $ $
\end_inset


\begin_inset Formula \begin{align}
W(\beta) & =\sum_{i,j=1}^{\ell}\beta_{i}\beta_{j}K(x_{i},x_{j})\label{eq:SVM_Density_Target_functional}\end{align}

\end_inset


\end_layout

\begin_layout Standard
subject to the constraints
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align}
F_{\ell}(x)- & \sum_{j=1}^{\ell}\beta_{j}\int_{-\infty}^{x}K(x_{j,}y)dy\ \le\ \sigma_{\ell}\label{eq:SVM_Density_Inequality_Constraint}\\
F_{\ell}(x) & =\frac{1}{\ell}\sum_{i=1}^{\ell}\theta(x-x_{i})\label{eq:SVM_Density_Emprical_CDF}\\
\theta(x) & =\begin{cases}
1 & \text{all coordinates of }x\text{ are positive}\\
0 & \text{otherwise}\end{cases}\label{eq:Step_Function}\\
\sigma_{\ell} & =\frac{q(X)}{\sqrt{\ell}}\label{eq:SVM_Residual_Equation}\end{align}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $q(X)$
\end_inset

 is the 50% quantile of 
\begin_inset Formula $X$
\end_inset


\begin_inset Foot
status collapsed

\begin_layout Plain Layout
The 50% quantile of 
\begin_inset Formula $X$
\end_inset

 is the value of 
\begin_inset Formula $x$
\end_inset

 for which the cumulative probability is .5.
\end_layout

\end_inset

.
 See (Vapnik 7.8) for a thorough discussion of SVMs for density estimation.
 In general terms, the objective is to minimize the similarity of the input
 vectors while keeping the resulting estimator as accurate as possible.
 The term 
\begin_inset Formula $\sigma_{\ell}$
\end_inset

 describes the expected error given 
\begin_inset Formula $\ell$
\end_inset

 observations.
 
\begin_inset Formula $F_{\ell}$
\end_inset

 is the empirical cumulative distribution, so the inequality constraint
 limits the difference between the empirical CDF and the estimator's CDF
 to the expected error for 
\begin_inset Formula $\ell$
\end_inset

 observations.
 Since the kernel distance 
\begin_inset Formula $K(x,y)$
\end_inset

 is essentially a weighted distance measure, the kernel value of two nearby
 points will be large.
 To minimize the sum of such kernel distances the multiplier 
\begin_inset Formula $\beta$
\end_inset

 must be small for one of the points.
 The optimization problem is to find the set of points for which reducing
 
\begin_inset Formula $\beta$
\end_inset

 minimizes 
\begin_inset Formula $W$
\end_inset

 without sacrificing predictive accuracy.
 These points are essentially points which are redundant.
 
\end_layout

\begin_layout Itemize
Most 
\begin_inset Formula $\beta$
\end_inset

 will go to 0, leaving the support vectors
\end_layout

\begin_layout Section
Kernel Functions
\end_layout

\begin_layout Itemize
General definition and conceptual function
\end_layout

\begin_layout Itemize
Polynomial, RBF
\end_layout

\begin_layout Itemize
tensor product for vectors
\end_layout

\begin_layout Itemize
width parameter and nature as control variable
\end_layout

\begin_layout Section
SVM Reconstruction
\end_layout

\begin_layout Standard
The 
\begin_inset Formula $\epsilon$
\end_inset

-machine reconstruction task requires that we determine the set of causal
 states 
\begin_inset Formula $\boldsymbol{\mathcal{X}}$
\end_inset

 of a set of histories 
\begin_inset Formula $\mathbf{X}$
\end_inset

.
 Following the work done in the hierarchy section, we will define the reconstruc
tion algorithm for finite causal states of length 
\begin_inset Formula $L$
\end_inset

.
 Earlier we considered the implications of adding a noise term 
\begin_inset Formula $\xi$
\end_inset

, namely that we could treat points 
\begin_inset Formula $x$
\end_inset

 in a given neighborhood as likely drawn from the same value 
\begin_inset Formula $z$
\end_inset

.
 Recall that specifying the marginal probabilities of 
\begin_inset Formula $\boldsymbol{\mathcal{X}},\boldsymbol{\mathcal{Y}}$
\end_inset

 is sufficient to determine the conditional probability 
\begin_inset Formula $\mathcal{X}_{j}|\mathcal{X}_{i}$
\end_inset

 for any 
\begin_inset Formula $i,j$
\end_inset

.
 It should be clear that the density estimating SVM discussed in the last
 section constitutes a candidate algorithm for hierarchical 
\begin_inset Formula $\epsilon$
\end_inset

-machine reconstruction.
\end_layout

\begin_layout Standard
Support vector machines assume that the set of observations be i.i.d.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
Independant and identically distributed
\end_layout

\end_inset

 As we are explicitly trying to model the conditional dynamics of time series
 data, this would seem to be a problem.
 To avoid this conflict, we use finite histories from the set 
\begin_inset Formula $\mathbf{X}^{L}$
\end_inset

 rather than individual points from the set 
\begin_inset Formula $X$
\end_inset

.
 We have already assumed that our estimator will contain some error margin
 
\begin_inset Formula $\varepsilon$
\end_inset

 resulting from using sequences of length 
\begin_inset Formula $L$
\end_inset

.
 Since 
\begin_inset Formula $\overleftarrow{X}^{L}$
\end_inset

 is a discrete-time sequence of length 
\begin_inset Formula $L$
\end_inset

, we can use a simple mapping from from the history 
\begin_inset Formula $\overleftarrow{X}^{L}$
\end_inset

 to a vector
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\phi(\overleftarrow{X}_{i}^{L}) & \longmapsto\dot{x}_{i}=(x_{i-L},\ldots,x_{i})\end{align*}

\end_inset


\end_layout

\begin_layout Standard
This mapping generates a set of vectors 
\begin_inset Formula $\dot{X}$
\end_inset

 which are amenable to SVM estimation
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\phi(\mathbf{X}^{L}) & \longmapsto\dot{X}\end{align*}

\end_inset


\end_layout

\begin_layout Itemize
Discuss how 
\begin_inset Formula $K(x,y)$
\end_inset

 handles vectors in 
\begin_inset Formula $\mathbb{R}^{d\times L}$
\end_inset


\end_layout

\begin_layout Standard
To generate a reconstruction using the SVM density approach we minimize
 the functional (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:SVM_Density_Target_functional"

\end_inset

) subject to the constraints of (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:SVM_Density_Inequality_Constraint"

\end_inset

) using the data set 
\begin_inset Formula $\dot{X}$
\end_inset

.
 Elements of 
\begin_inset Formula $\dot{X}$
\end_inset

 with non-trivial weights 
\begin_inset Formula $\beta$
\end_inset

 are the resulting causal states, and their marginal probabilities are given
 by the corresponding value of 
\begin_inset Formula $\beta$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
You might want to justify the assumption that 
\begin_inset Formula $\Pr(\dot{x}_{i})=\beta_{i}$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Itemize
How does this relate to 
\begin_inset Formula $\mathcal{A}$
\end_inset

? 
\end_layout

\begin_layout Section
Probabalistic Extension and Translation
\end_layout

\begin_layout Itemize
:(
\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Standard
This chapter extended the 
\begin_inset Formula $\epsilon$
\end_inset

-machine to the class of real-valued processes 
\begin_inset Formula $X:\ x\in\mathbb{R}^{d}$
\end_inset

 by treating 
\begin_inset Formula $X$
\end_inset

 as a noisy process generated by a hidden process 
\begin_inset Formula $Z$
\end_inset

.
 Several results from machine learning were adapted to accomplish the reconstruc
tion task posed under the extended 
\begin_inset Formula $\epsilon$
\end_inset

-machine definition.
 The concept of Empirical Risk Minimization was adopted as a general optimizatio
n goal, and Support Vector optimization was used to accomplish the optimization
 task.
 Finally it was shown that finite discrete-time sequences have obvious interpret
ations as vectors, and that these vectors are within the class of admissable
 observations for the SVM algorithm using a Radial Basis Function kernel.
 In the process we introduced the second control variable, the kernel width
 parameter 
\begin_inset Formula $\gamma$
\end_inset

 which controls the trade-off between model complexity and accuracy.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
What are the implications on 
\begin_inset Formula $\varepsilon$
\end_inset

 of combining elements of 
\begin_inset Formula $\mathcal{A}$
\end_inset

?
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Besides extending the class of admissable processes, the probabalistic framework
 for 
\begin_inset Formula $\epsilon$
\end_inset

-machine reconstruction has a significant benefit; it allows us to control
 the complexity of 
\begin_inset Formula $\Lambda$
\end_inset

.
 Without the control variable 
\begin_inset Formula $\gamma$
\end_inset

 our only way to control the computational cost of reconstruction and estimation
 is by limiting our prediction horizon.
 We can now control the sparseness of the estimator independant of the predictio
n horizon; in essence allowing us to balance the 
\emph on
accuracy
\emph default
 and 
\emph on
duration
\emph default
 of our estimates, assuming fixed computational resources and a structural
 complexity in 
\begin_inset Formula $X$
\end_inset

.
 
\end_layout

\begin_layout Standard
We have chosen the SVM process for reconstruction due to three features;
 the uniqueness of it solutions, its ability to handle high-dimensional
 data, and the simplicity of its control variables.
 SVM optimization is not susceptable to local minima because the optimization
 problem takes place in the feature space 
\begin_inset Formula $\Psi$
\end_inset

 which is constructed such that the target function to be minimized decreases
 monotinically
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
By contrast, other systems such as some types of neural networks require
 complicated heuristics such as annealing to ensure that a local solution
 is also a global solution.
 This is due to the fact that the target functional does not decrease monotonica
lly; it is possible to find a solution which is better than any similar
 solutions, but worse than a radically different solution.
\end_layout

\end_inset

.
 The SVM approach was developed in part to address the 'curse of dimensionality'
, the fact that the complexity of many optimization algorithms increases
 exponentially with the dimensionality of the data being optimized.
 The complexity of and SVM will generally increase linearly with the dimensional
ity of 
\begin_inset Formula $x$
\end_inset

, allowing us to extend the prediction horizon with minimal computational
 cost.
 Finally, the control parameter 
\begin_inset Formula $\gamma$
\end_inset

 provides a simple and intuitive method to control computational costs.
 
\end_layout

\begin_layout Standard
In the next chapter we will develop a method of handling data sets which
 are neither discrete-time nor uniformly sampled.
\end_layout

\begin_layout Chapter
Discretization
\end_layout

\begin_layout Standard
The assumption of discrete-time data is the next area we will examine.
 Many data sources do not have obvious discrete-time interpretations.
 Consider for example a system which gathers data from a distributed sensor
 network in which each sensor's sample rate is independent and varies over
 time.
\end_layout

\begin_layout Standard
The general approach we will take is to discretize the set of observations
 into histories defined by their duration 
\begin_inset Formula $\tau$
\end_inset

 rather than their sequence length 
\begin_inset Formula $L$
\end_inset

.
 Histories defined in this way accept observations with real-valued time
 stamps and do not impose any constraints on the timing of the observations,
 however several questions are raised.
 The first question is how to define a kernel function which operates over
 arbitrarily sized sets of observations? The second question is how to handle
 the set 
\begin_inset Formula $\mathbf{X}$
\end_inset

 of all histories? Because 
\begin_inset Formula $t$
\end_inset

 can concievably take infinitely many values, even if we specify the history
 duration 
\begin_inset Formula $\tau$
\end_inset

, the set 
\begin_inset Formula $\mathbf{X}^{\tau}$
\end_inset

 is uncountable.
\end_layout

\begin_layout Standard
We will begin by formalizing the notion of histories defined over time intervals
 and then answer these questions one at a time.
\end_layout

\begin_layout Section
Interval Histories
\end_layout

\begin_layout Standard
Rather than dealing with a sequence of values terminating at 
\begin_inset Formula $x_{i}$
\end_inset

, we will deal with a set of observations which take place in the time interval
 
\begin_inset Formula $(t-\tau,t]$
\end_inset

.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
Another potential approach is to use a soft-margin windowing function in
 which a point's inclusion in a given window is described by some function
 
\begin_inset Formula $f(\bar{x})$
\end_inset

, for example the radial basis function.
 We choose to use closed partitions for simplicity, and because it isn't
 clear what benefit this type of smearing provides.
\end_layout

\end_inset

 That is, the underlying process 
\begin_inset Formula $X$
\end_inset

 is defined as a set of time-stamped observations
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
X & =\left\{ \bar{x}_{1},\ldots,\bar{x}_{\ell}\right\} \\
\bar{x}_{i} & =(x_{i},t_{i})\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We define a 
\emph on
history
\emph default
 as the set of points 
\begin_inset Formula $ $
\end_inset

which fall in the interval 
\begin_inset Formula $(t-\tau,t]$
\end_inset

 and a 
\emph on
future
\emph default
 as the set of points in the interval 
\begin_inset Formula $(t,t+\tau)$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\overleftarrow{x}_{t}^{\tau} & =\left\{ \bar{x}_{i}:\ t-\tau<t_{i}\le t\right\} \\
\overrightarrow{x}_{t}^{\tau} & =\left\{ \bar{x}_{i}:\ t<t_{i}<t+\tau\right\} \end{align*}

\end_inset


\end_layout

\begin_layout Standard
We can re-state the causal states as the set
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\epsilon(\overleftarrow{x}_{t}^{\tau}) & \equiv\left\{ \overleftarrow{x}_{i}^{\tau}:\ \left|\Pr\left(\overrightarrow{X}^{\tau}\in F|\overleftarrow{X}^{\tau}=\overleftarrow{x}_{t}^{\tau}\right)-\Pr\left(\overrightarrow{X}^{\tau}\in F|\overleftarrow{X}^{\tau}=\overleftarrow{x}_{i}^{\tau}\right)\right|\le\varepsilon,\ \forall F\in\sigma(\overrightarrow{X}^{\tau}),\ \forall\overleftarrow{x}_{t}^{\tau}\in\overleftarrow{X}^{\tau}\right\} \end{align*}

\end_inset


\end_layout

\begin_layout Standard
We denote a give causal state 
\begin_inset Formula $\mathcal{X}_{i}^{\tau}$
\end_inset

 and the set of all causal states for a process 
\begin_inset Formula $\boldsymbol{\mathcal{X}}^{\tau}$
\end_inset

.
 
\end_layout

\begin_layout Section
Divergence Kernel
\end_layout

\begin_layout Standard
Kernel functions tranditionally operate over metric spaces.
 We saw in the last chapter that discrete sequences of uniform length could
 easily be mapped into a measurable vector space.
 Defining some notion of 'distance' between two sets of continuous-time
 observations of differing cardinality is less trivial.
 To do so requires that we treat the observations not as vectors, but as
 realizations of an underlying probability distribution.
 
\end_layout

\begin_layout Standard
The set of histories 
\begin_inset Formula $\mathbf{X}$
\end_inset

 has been discussed in the discrete-time context, but considering the set
 of interval histories doesn't preclude us from considering 
\begin_inset Formula $\mathbf{X}^{\tau}$
\end_inset

 as a random process from which individual histories 
\begin_inset Formula $\overleftarrow{x}_{i}^{\tau}$
\end_inset

 are drawn, or that we could examine a greater number of sample trajectories
 in some histories than in others.
 In a sense, while a discrete-time sequence is fully specified, we must
 consider a continuous-time history as a realization of the probability
 density of 
\begin_inset Formula $X$
\end_inset

 in the neighborhood of time 
\begin_inset Formula $t$
\end_inset

.
 Constructing a metric 
\begin_inset Formula $\mu$
\end_inset

 between interval histories is therefore a task of determining the similarity
 between the underlying distributions of the two histories given the set
 of observations available; the more similar two distributions are the 'closer'
 their corresponding histories.
\end_layout

\begin_layout Standard
Returning to the concept of information entropy, we would like to describe
 a property of two sets which follows the following guidelines:
\end_layout

\begin_layout Enumerate
The measure should be minimal when the joint entropy of the sets is the
 same as the marginal entropy of the sets (the sets together contain the
 same information as they would independently)
\end_layout

\begin_layout Enumerate
The measure of two sets should decrease as the uncertainty of the convergence
 of their distributions decreases: 
\end_layout

\begin_layout Standard
In general terms, we want the measure to tell us how certain we are that
 the histories are drawn from the same distribution.
 A candidate function for 
\begin_inset Formula $\mu$
\end_inset

 is the Kullback-Liebler (KL) divergence:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
D(p\Vert q) & =\int p(x)\log\frac{p(x)}{q(x)}dx\end{align*}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $p,q$
\end_inset

 are probability density functions of two random variables 
\begin_inset Formula $P,Q$
\end_inset

.
 The KL divergence can be expressed in terms of the joint and marginal entropy
 of the random functions
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
D(p\Vert q) & =H[P,Q]-H[Q]\end{align*}

\end_inset


\end_layout

\begin_layout Standard
From this we can see that objective [1] is satisfied; the divergence will
 be 0 if the joint and marginal entropy is the same.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
But [2] isn't.
 Find new divergence or re-consider the objectives
\end_layout

\end_inset


\end_layout

\begin_layout Section
Making X Finite
\end_layout

\begin_layout Standard
Pearson / KL divergence
\end_layout

\begin_layout Chapter
Choice
\end_layout

\begin_layout Standard
Motivation, internal states, path exploration, non-discrete input space
\end_layout

\begin_layout Chapter
Performance
\end_layout

\begin_layout Standard
Rates of convergence, algorithmic complexity, distributed architecture
\end_layout

\begin_layout Chapter
Discussion
\end_layout

\begin_layout Itemize
Transform invariance other than time/shift
\end_layout

\begin_layout Itemize
Robustness of generalization ability
\end_layout

\begin_layout Itemize
Complex, context-sensitive planning
\end_layout

\begin_layout Itemize
Motivational creep
\end_layout

\begin_layout Itemize
Lateral inhibition
\end_layout

\begin_layout Itemize
Testing scenarios; quantifying success
\end_layout

\begin_layout Chapter
\start_of_appendix
Results
\end_layout

\begin_layout Section
Eunite Competition Data
\end_layout

\begin_layout Section
Santa Fe Data
\end_layout

\begin_layout Section
CATS Benchmark Data
\end_layout

\begin_layout Section
Results Summary
\end_layout

\begin_layout Chapter
Discussion of Existing Work
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Expectation-maximization_algorithm
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Stationary_process
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Ergodicity
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Mixing_(mathematics)
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Lyapunov_exponent
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Recurrence_quantification_analysis
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Recurrence_plot
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Autoregressive_fractionally_integrated_moving_averag
e
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Autocorrelation
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

%
\backslash
url{http://en.wikipedia.org/wiki/Linear_discriminant_analysis}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Hidden Markov Model
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

% cannot account for future states - only capable of prediction
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

% weak, short-term memory
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Box-Jenkins
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

% intended for simplistic processes with well-understood stationarity and
 periodicity
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

% http://en.wikipedia.org/wiki/Box-Jenkins
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Spectral Analysis
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

% assumes some type of frequency-domain decomposition.
  Frequency-domain signal representations do not do a very good job predicting
 time-domain values.
 
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Shrinking-
\begin_inset Formula $\epsilon$
\end_inset

 SVM Regression
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

% theoretical foundation weak; only compensates for the relevance of recent
 data.
  See Markhov problem
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Chapter
SVM Details & Optimizations
\end_layout

\begin_layout Section
Quadratic Optimization Problem
\end_layout

\begin_layout Section
Support Vector Decomposition 
\end_layout

\begin_layout Standard
\begin_inset Marginal
status collapsed

\begin_layout Plain Layout
This section is probably better as an appendix
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Due to the simplicity of the constraints in the optimization problem, it
 is possible to use the decomposition method of Osuna to reduce the memory
 requirements of the Parzen-SVM algorithm.
\end_layout

\begin_layout Subsection
Sub-Problem Definition 
\end_layout

\begin_layout Standard
The decomposition algorithm breaks 
\begin_inset Formula $X$
\end_inset

 into two working sets 
\begin_inset Formula $B,N$
\end_inset

, and attempts to optimize 
\begin_inset Formula $B$
\end_inset

 while keeping 
\begin_inset Formula $N$
\end_inset

 fixed.
 This results in the following iterative optimization problem where 
\begin_inset Formula $\boldsymbol{\beta}^{k}$
\end_inset

 denotes the result of the previous iteration:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
W(\boldsymbol{\beta}_{B}) & =\frac{1}{2}\begin{bmatrix}\boldsymbol{\beta}_{B}^{T} & (\boldsymbol{\beta}_{N}^{k})^{T}\end{bmatrix}\begin{bmatrix}P_{BB} & P_{BN}\\
P_{NB} & P_{NN}\end{bmatrix}\begin{bmatrix}\boldsymbol{\beta}_{B}\\
\boldsymbol{\beta}_{N}^{k}\end{bmatrix}-\begin{bmatrix}q_{B}^{T} & q_{N}^{T}\end{bmatrix}\begin{bmatrix}\boldsymbol{\beta}_{B}\\
\boldsymbol{\beta}_{N}^{k}\end{bmatrix}\\
 & =\frac{1}{2}\boldsymbol{\beta}_{B}^{T}P_{BB}\boldsymbol{\beta}_{B}-(-q_{B}+P_{BN}\boldsymbol{\beta}_{N}^{k})^{T}\boldsymbol{\beta}_{B}\\
 & =\frac{1}{2}\begin{bmatrix}\beta_{i} & \beta_{j}\end{bmatrix}\begin{bmatrix}P_{ii} & P_{ij}\\
P_{ij} & P_{jj}\end{bmatrix}\begin{bmatrix}\beta_{i}\\
\beta_{j}\end{bmatrix}-(-q_{B}+P_{BN}\boldsymbol{\beta}_{N}^{k})^{T}\begin{bmatrix}\beta_{i}\\
\beta_{j}\end{bmatrix}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\text{subject to}\quad0\le\beta_{i},\beta_{j},\quad\beta_{i}+\beta_{j}=1-\mathbf{1}^{T}\boldsymbol{\beta}_{N}^{k}\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
 Working Set Selection 
\end_layout

\begin_layout Standard
Select
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align}
 & i\in\text{arg}\max_{t}\left\{ -\nabla f(\boldsymbol{\beta}^{k})_{t}\ |\quad t\in I(\boldsymbol{\beta}^{k})\right\} \\
 & j\in\text{arg}\min_{t}\left\{ -\frac{b_{it}^{2}}{a_{it}}\ |\quad t\in I(\boldsymbol{\beta}^{k}),\quad-\nabla f(\boldsymbol{\beta}^{k})_{t}<-\nabla f(\boldsymbol{\beta}^{k})_{i}\right\} \end{align}

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset ERT
status open

\begin_layout Plain Layout

% This needs to be checked for the new optimization scenario
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align}
I(\boldsymbol{\beta}) & \equiv\{t\ |\quad\beta_{t}<1\quad\text{or}\quad\beta_{t}>0\}\\
 & a_{it}=P_{ii}+P_{tt}-2P_{it}\\
 & \bar{a}_{it}=\begin{cases}
a_{it} & \text{if}\ a_{it}>0\\
\delta & \text{otherwise}\end{cases}\\
 & b_{it}=-\nabla f(\boldsymbol{\beta}^{k})_{i}+\nabla f(\boldsymbol{\beta}^{k})_{t}\\
\nabla f(\boldsymbol{\beta})_{i} & \equiv P_{i}\boldsymbol{\beta}-q_{i}\end{align}

\end_inset


\end_layout

\begin_layout Subsection
Stopping Condition 
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\max_{i\in I(\boldsymbol{\alpha}^{k})}-\nabla f(\boldsymbol{\alpha})_{i}+\min_{j\in I(\boldsymbol{\alpha}^{k})}\nabla f(\boldsymbol{\alpha})_{j}\le\epsilon\end{equation}

\end_inset


\end_layout

\begin_layout Section
Cascade SVM
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "Hazan08"

\end_inset


\end_layout

\begin_layout Section
Parallel SVM Decomposition
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "Graf04"

\end_inset


\end_layout

\begin_layout Chapter
Further Research
\end_layout

\begin_layout Section
Data Pre-Processing
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

% logistic function using mean and sd to put most training points between
 .1 and .9
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

% logistic function from delta using mean and sd in same way if data non-station
ary
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

% iterative integration process until stationary data found ?
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Performance
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "Research/research.bib"
options "bibtotoc,plain"

\end_inset


\end_layout

\end_body
\end_document
