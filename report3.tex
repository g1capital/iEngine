\documentclass[10pt]{article}

\usepackage{amsmath}
\usepackage{amsfonts}

\usepackage{hyperref}
\usepackage[all]{hypcap}

\setlength{\parskip}{.25cm plus4mm minus3mm}
\setlength{\parindent}{0in}
\setlength{\voffset}{-.5in}
\setlength{\hoffset}{-.5in}
\setlength{\textwidth}{6.0in}


\begin{document}
\title{Robust Timeseries Analysis using Heterogenous Prediction Windows}
%\subtitle{Analysis of Ergodic Processes with Multiple Stationary Behaviors using Data from Multiple Asynchronous Input Channels}
\author{Ryan Michael\\ \texttt{kerinin@gmail.com}}
%\date{???}
\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}
%\tableofcontents

\subsection{Existing Work}

\subsection{General Overview}

\section{Formal Description}


\subsection{Problem Setting}
We begin with a hidden random variable 

\[ X = (\Omega,\mathcal{F},\mathcal{P}) \]

Our knowledge of \( X \) comes from a set of independant sources which we treat as random processes generated by \( X \):

\[ X^n : \Omega \mapsto \Omega^n \in \mathbb{R}^d \times \mathbb{R}_+  \]
\[ X^n = (\Omega^n,\mathcal{F}^n,\mathcal{P}^n) \]
\[ \mathbf{X} = [X^0,...,X^c]   \]

We refer to each of these sources as a channel, and refer to each channel as the \( i^\text{th} \) element of the set \( \mathbf{X} \):

\[ C^n = [\Omega^n,\mathcal{F}^n,\mathcal{P}^n] \]

For each channel we are given a set of \( \ell \) observations of dimension \( d \):

\[ \mathbf{x}_i^n = (x_i^n, t_i^n ) \]
\[ X^n = \left[ \mathbf{x}_0^n,\hdots,\mathbf{x}_\ell^n \right] \in \mathbb{R}^d \times \mathbb{R}_+ \]

Given a set of time durations \( \theta \in \Theta \), we define a set of time widows:

\[  \mathcal{T}^n =  
\begin{bmatrix} 
(t_0^n,\theta_0) & \hdots & (t_0^n,\theta_z) \\
\vdots & \ddots & \vdots \\
(t_\ell^n, \theta_0) & \hdots & (t_\ell^n, \theta_z) \\
\end{bmatrix} 
\]

We assume that \( \mathcal{P} \) can be approximated in a given time window using a shifted set of weighted distributions defined over some set of time intervals.

\begin{equation} \mathcal{P}(t,\theta) = \sum_i \delta_i \cdot f_i(t - t_i) \end{equation}

where \( \delta_n \) is the weight corresponding to \( f_n \).  Finally, we assume that a similar mixture of distributions can be determined for each channel:

\begin{equation} \mathcal{P}^n(t,\theta) = \sum_i \delta_i^n \cdot f_i^n(t - t_i) \end{equation}

\subsection{Single Channel Setting}
We begin by considering the case where only one channel exists, so for now we will omit the superscript and refer to \(X^n = (\Omega^n,\mathcal{F}^n,\mathcal{P}^n) \) as \(X = (\Omega, \mathcal{F},\mathcal{P}) \).  Given a set of test data \( \hat{X} \), our goal is to estimate the probability distribution of \( X \) over some time window \( \mathcal{T}_{\hat{x}} \):

\begin{align*}
\mathbf{\hat{x}}_i &= (\hat{x}_i,t_i) \\
\hat{X} &= [ \mathbf{\hat{x}}_0,\hdots,\mathbf{\hat{x}}_k ] \\
\mathcal{T}_{\hat{x}} &= (t_{\hat{x}}, \theta_{\hat{x}} ), \quad t_{\hat{x}} < \min_t \hat{X} < \max_t \hat{X} < t_{\hat{x}} + \theta_{\hat{x}}
\end{align*}

We begin by defining the subset of the training observations which fall into each time window, scaled and shifted to the interval \( t \in [0,1) \):

\begin{equation} \label{eq:Ssingle} S_{t,\theta} = \left[ \left( x_i,\frac{t_i - t}{\theta} \right) \Big| \quad x_i \in X, \ t \le t_i < t+\theta \right] \end{equation}
\[ \mathcal{S} = 
\begin{bmatrix} 
S_{t_0,\theta_0} & \hdots & S_{t_0,\theta_z} \\
\vdots & \ddots & \vdots \\
S_{t_\ell, \theta_0} & \hdots & S_{t_\ell, \theta_z} \\
\end{bmatrix}  
\]

We treat \( \mathcal{S} \) as a random process:

\[ \mathcal{S} = [ \Omega^\mathcal{S}, \mathcal{F}^\mathcal{S},\mathcal{P}^\mathcal{S}] \]

 and observe that \( \hat{X} \) can be treated as an observation of \( \mathcal{S} \):

\begin{equation} \label{eq:SySingle} 
S_{\hat{x}} =  \left[ \left( x_i,\frac{t_i - t_{\hat{x}}}{\theta_{\hat{x}}} \right) \Big| \quad (x_i,t_i) \in \hat{X} \right] 
\end{equation}

We can now frame the task of estimating the probability distribution of \( \hat{X} \) as a task of estimating a probability density function \( \varphi \) for the random process \( \mathcal{S} \): 

\begin{equation} \label{eq:PrS}
\Pr( X = \mathbf{x} \ | \ \hat{X} ) \mapsto \ \Pr( \mathcal{S} = \{ S_{\hat{x}} \cup \mathbf{x} \} ) \ \simeq \ \varphi( \mathbf{x}, S_{\hat{x}} )
\end{equation}


\subsection{Multiple Channel Setting}

In order to extend this result to settings in which multiple channels exist, we return to \ref{eq:Ssingle} and extend the scope to multiple channels:

\begin{equation} \label{eq:Smultiple} S_{t,\theta} = \left[ \left( x_i^n,\frac{t_i - t}{\theta} \right) \Big| \quad x_i^n \in \mathbf{X}, \ t \le t_i < t+\theta \right] \end{equation}

In this case, we treat each channel as an orthonormal basis of the abstract space \( \Omega^\mathcal{S} \).  Equation \ref{eq:SySingle} is likewise extended in the same manner:

\begin{equation} \label{eq:SyMultiple} 
S_{\hat{x}} =  \left[ \left( x_i^n,\frac{t_i - t_{\hat{x}}}{\theta_{\hat{x}}} \right) \Big| \quad (x_i^n,t_i) \in \mathbf{\hat{X}} \right] 
\end{equation}

\section{ Parzen Window Estimation}

One method of evaluating \ref{eq:PrS} is by using the Parzen Window method.  We choose the Parzen Window method because it allows us to estimate probabilities of unordered sets, provided they have an addition operation and a kernel function exists to provide a distance metric.  

\subsection{ Single Channel Parzen Window}

We will again begin by considering the single-channel case, then extend the resulting equations as necessary.  The Parzen Window method requires the definition of a metric over the abstract space \( \Omega^\mathcal{S} \).  Such a metric can be defined using the symmetric Kullbeck Liebler divergence with probability measures \( \phi_n(\mathbf{x}) \simeq Pr(X = \mathbf{x} \ | \ S_n) \):

\begin{align} \label{eq:KLsingle}
D_{KL}(S_n\|S_m) &= \sum_{\mathbf{x} \in \{ S_n \cup S_m \} } \phi_n( \mathbf{x} ) \log \frac{ \phi_n(\mathbf{x}) }{ \phi_m( \mathbf{x} ) } \nonumber \\
&= -\sum_{\mathbf{x} \in \{ S_n \cup S_m \} } \phi_n(\mathbf{x}) \log \phi_m(\mathbf{x}) + \sum_{\mathbf{x} \in \{ S_n \cup S_m \} } \phi_n(\mathbf{x}) \log \phi_n(\mathbf{x}) \\
\|S_n - S_m\|_{KL} &= D_{KL}(S_n\|S_m) + D_{KL}(S_m\|S_n)
\end{align}
% Need to discuss why the union of n&m is sufficient to evaluate the KL divergence

The Parzen Window estimation of \( \mathcal{P}^\mathcal{S} \) is defined as:

\begin{equation} \label{eq:PrSParzenSingle}
\Pr( \mathcal{S} = S ) \simeq \sum_{i \in \mathcal{S}}  \frac{1}{|\mathcal{S}| } K_\gamma( S, S_i ) 
\end{equation}

where \( | \cdot | \) denotes the cardinality of \( ( \cdot ) \) and \( K \) is some kernel function, for instance the Radial Basis Function:

\begin{equation} \label{eq:KRBFparzen}
K_\gamma( S_i, S_j ) =  \frac{1}{\gamma \sqrt{2\pi} } e^{-\frac{1}{\gamma} \|S_i - S_j \|_{KL}^2}
\end{equation}

The same method can be used to estimate \( \phi_n(\mathbf{x}) \) for a given subset \( S_n \):

\begin{equation} \label{eq:PhiSingle}
\phi_n( \mathbf{x} ) = \sum_{\mathbf{x}_i \in S_n} \frac{1}{|S_n|}  K_\gamma (\mathbf{x}, \mathbf{x}_i ) 
\end{equation}

Substituting \ref{eq:PrSParzenSingle} into \ref{eq:PrS} our probability distribution estimate becomes:

\begin{equation} \label{eq:VarphiParzenSingle}
\varphi_P(\mathbf{x}, S) = \sum_{i \in \mathcal{S}} \frac{1}{|\mathcal{S}|} K_\gamma ( S_{\mathbf{x}}, S_i )
\end{equation}
\[ S_{\mathbf{x}} = \{ S \cup \mathbf{x} \}\]


\subsection{ Multiple Channel Parzen Window}

Extending the Parzen Window approach requires the realization that \ref{eq:KLsingle} requires that \( S_n \) and \( S_m \) both be defined over the same abstract space \footnote{ For instance if \( X_n \in \mathbb{R}^2 \) and \( X_m \in \mathbb{R}^3 \), it is impossible to calculate \( \phi_n\big( (x_m,t) \big) \) because the quantituy \( \| x_m - x_n \|^2 \) from using \ref{eq:PhiSingle} is ambiguous.}.  As mentioned earlier, in the Multiple Channel context each channel is treated as an orthonormal basis of \( \Omega^\mathcal{S} \).  An obvious approach to defining a measure over \( \Omega^\mathcal{S} \) for mutliple channels is to use the Euclidean norm of the Kullbeck Liebler divergence of each channel considered independently:

\[ S_n^c = [ \mathbf{x} \ | \ \mathbf{x} \in \{ S_n \cap X^c \} ] \]

This requires the following minor extension of \ref{eq:KRBFparzen}:

\begin{equation} \label{eq:KRBFParzenMultiple}
K_\gamma( S_i, S_j ) =  \prod_c \frac{1}{\gamma \sqrt{2\pi} } e^{-\frac{1}{\gamma} \|S_i^c - S_j^c \|_{KL}^2 }
\end{equation}


\section{ Support Vector Estimation}

The Parzen Window method is neither sparse nor computationally efficient, and as the number of observations grows, these deficiencies quickly become prohibitive.  We now investigate the use of Support Vector Machines to generate \( \varphi(\mathbf{x}, S) \).

\subsection{ Random Process Estimation }
Support Vector Machines are usually used to estimate probability distributions by solving the related problem of estimating the cumulative distribution function of the random variable in question.  This reduces the problem to one of estimating a non-linear mapping from observations to cumulative distribution values, which can be formulated as an optimization problem over a linear operator equation.  Unfortunately, these methods depend on the ability to calculate an empirical distribution for each observation:

\begin{equation} F_\ell(x) = \frac{1}{\ell} \sum_i \theta(x-x_i) \end{equation}

where \( \theta(x) \) is the indicator function.  To evaluate this function, the abstract space \( \Omega^\mathcal{S} \) must be ordered.  While we have described a distance metric over \( \Omega^\mathcal{S} \), it is not clear what a meaningful ordering relation would be.

Rather than calculating the cumulative probability distribution of \( \Omega^\mathcal{S} \), we begin with the assumption that the Parzen Window estimate of the probability distribution is accurate and attempt to minimize the square loss between the Support Vector estimate and the Parzen Window estimate.  Because we are hoping to generate a sparse representation of the probability distribution, we add a regularizing term \( \Omega \) which penalizes similar \( S \).  The Support Vector approach seeks a solution in the following form:

\begin{align} \label{eq:SVResultSingle}
\varphi_{SV}( \mathbf{x}, S : \beta) &= \sum_{i} \beta_i K_\gamma( S_\mathbf{x}, S_i ) \\
S_\mathbf{x} &= \{ S \cup \mathbf{x} \} \nonumber
\end{align}

So we can express the Support Vector optimization problem as:

\begin{align*}
W(\beta) &= \sum_{\mathbf{x} \in X } \Big( \varphi_{P}(\mathbf{x}, S) - \varphi_{SV}(\mathbf{x}, S : \beta ) \Big)^2 + \lambda \Omega(\beta,S) \\
&= \sum_{ \mathbf{x} \in X } \Big( \varphi_{SV}(\mathbf{x},S : \beta)^2 - 2 \varphi_{SV}(\mathbf{x},S : \beta) \varphi_P(\mathbf{x},S)  \Big)  + \lambda \Omega( \beta, S ) \\
&= \sum_{ \mathbf{x} \in X } \left( \Big( \sum_i \beta_i K_\gamma( S_{\mathbf{x}}, S_i ) \Big) \cdot \Big( \sum_j \beta_j K_\gamma( S_{\mathbf{x}}, S_j ) \Big) - 2 \Big( \sum_i \beta_i K_\gamma( S_{\mathbf{x}}, S_i ) \Big) \cdot \Big( \sum_j \frac{1}{|\mathcal{S}|} K_\gamma( S_{\mathbf{x}}, S_j ) \Big)   \right) + \lambda \Omega( \beta,S ) \\
&= \sum_{i,j} \beta_i \beta_j \sum_{\mathbf{x} \in X} K_\gamma(S_{\mathbf{x}},S_i) K_\gamma(S_{\mathbf{x}},S_j) + \sum_i \beta_i \sum_{\mathbf{x} \in X } \left( \lambda K_\gamma(S_{\mathbf{x}},S_i)^{-1} - \frac{2}{|\mathcal{S}|} \sum_j K_\gamma(S_{\mathbf{x}},S_i) K_\gamma(S_{\mathbf{x}},S_j) \right)
\end{align*}

\begin{equation} \label{eq:SVProcessSingle} \text{subject to} \quad \sum_i \beta_i = 1, \quad \beta_i \ge 0, \ i=1,\hdots,|\mathcal{S}|
\end{equation}

Notice the regularizer selected is defined as:

\begin{equation}
\Omega(\beta,S) = \sum_i \beta_i \sum_{\mathbf{x} \in X} K_\gamma(S_{\mathbf{x}},S_i)^{-1}
\end{equation}

Equation \ref{eq:SVProcessSingle} is a quadratic optimiztion problem defined as:

\begin{align*}
W(\beta) &= \frac{1}{2} \beta^T P \beta + q^T \beta \\
P_{i,j} &= \sum_{\mathbf{x} \in X } K_\gamma(S_{\mathbf{x}},S_i) K_\gamma(S_{\mathbf{x}},S_j) \\
q_i &= \sum_{\mathbf{x} \in X } \left( \lambda K_\gamma(S_{\mathbf{x}},S_i)^{-1} - \frac{1}{|\mathcal{S}|} \sum_j K_\gamma(S_{\mathbf{x}},S_i) K_\gamma(S_{\mathbf{x}},S_j) \right) \\
\end{align*}
\begin{align}
P &= \langle \mathbf{K}^T \cdot \mathbf{K} \rangle  \label{eq:SVPSingle} \\
q &= \lambda \langle \mathbf{K}^T \cdot \mathbf{1}_{( |K|, 1 )} \rangle^{-1} - \frac{1}{ | \mathcal{S} | } \langle \mathbf{K}^T \cdot \mathbf{K} \cdot \mathbf{1}_{( |K|, 1 )} \rangle
\end{align}

\subsection{  Random Process Kernel Definition }

When developing the Parzen Window algorithm, we used kernel function \ref{eq:KLsingle} which is based on the Kullbeck Liebler divergence of a probability estimate \ref{eq:PhiSingle} defined at the sets being evaluated.  Our motivation in developing a Support Vector approach is to generate sparse representations of \( \mathcal{P}^{\mathcal{S}} \), in part to reduce the computational demands of evaluating \( \varphi( \mathbf{x}, S) \) for test data sets.  Unfortunately, kernel function \ref{eq:KLsingle} requires probability estimates of both sets being compared - it would be helpful to develop a kernel function capable of evaluating a distance between a test set \( S_{\hat{X}} \) and a training set \( S_i \) without first calculating an estimate of the probability distribution of \( S_{\hat{X}} \).

Note that \ref{eq:SVProcessSingle} is formulated in such a way that the first kernel argument is always \( S_{\mathbf{x}} \), which allows us to define a kernel which takes a set of points as its first argument and a probability distribution as its second.  The definition of \ref{eq:SVPSingle} also allows us to define non-symmetric kernels without sacrificing convexity or monotonicity in the optimization objective function.  We can therefore use any measure of the divergence between a set of points and a probability distribution.  We consider the Renyi entropy with \( \alpha = 1 \) and select the Shannon Entropy as our kernel metric, as it is equivalent the the Kullback Lieblier divergence.

\begin{equation}
\| X - \phi \|_{H} = \sum_{x \in X} \phi(x) \log \phi(x)
\end{equation}

Recalling that \( S_\mathbf{x} = \{ S \cup \mathbf{x} \} \), we observe that:

\begin{equation}
\| S_{\mathbf{x}} - S_i \|_{H} = \| S - S_i \|_{H} + \| \mathbf{x} - S_i \|_{H}
\end{equation}

We restate the kernel function as:

\begin{equation}
K_\gamma( X, S_i ) =  \frac{1}{\gamma \sqrt{2\pi} } e^{-\frac{1}{\gamma} \|X - S_i \|_{H}^2}
\end{equation}

In situations where probabilities are being calculated for a uniform set of points in \( \Omega^{\mathcal{S}} \), this can considerably reduce the computational time needed to evaluate \( \mathbf{K} \), as \( \|\mathbf{x} - S_i \|_H \) can be computed once and then added to each \( \|S - S_i\|_H, S \in \mathcal{S} \).


\subsection{ Random Vector Estimation }
 
Evaluating \ref{eq:SVProcessSingle} over a training set \( X \) produces a set \( \mathcal{S}_{SV} \in \mathcal{S} \) referred to as Support Vectors.  While \ref{eq:SVProcessSingle} produces a sparse representation of \( \mathcal{P}^\mathcal{S} \) by eliminating some \( S \), further sparseness can be achieved by refining the definition of \( \phi(\mathbf{x}) \) to allow the elimination of some \( \mathbf{x} \) in each \( S \in \mathcal{S}_{SV} \).  In the context of computing the kernel matrices used in optimizing \ref{eq:SVProcessSingle} the Parzen Window definition of \( \phi(\mathbf{x}) \) is computationally acceptable, as it results in a good approximation with a minimal amount of computation \footnote{ The Parzen Window estimate at a point requires distance computations for each observation and a summation.  By contrast, the SV estimate requires the same number of distance computations as well as the solving of a quadratic optimization problem whose complexity increases exponentially with the number of observations. }.  In the context of evaluating \ref{eq:SVResultSingle} over a testing set \( \hat{X} \), the Parzen Window algorithm for \( \phi(\mathbf{x}) \) is sub-optimal due to the fact that it requires the full set of observations for each \( S \in \mathcal{S}_{SV} \).  In this context, a sparse algorithm for \( \phi(\mathbf{x}) \) would reduce both the data required to store \( \mathcal{S}_{SV} \) and the computational resources required to evaluate \ref{eq:DKLemp}.

We return now to the empirical cumulative distribution function method of Support Vector density estimation and search for a solution in the following form, which is able to use multiple non-symmetric kernel functions simultaneously:

\begin{equation} \label{eq:SVDensity}
\phi(x) = \sum_{i=1}^\ell \left( \beta_i^1 \mathcal{K}_1(x_i,x) + ... + \beta_i^\kappa \mathcal{K}_\kappa( x_i,x) \right)
\end{equation}

We select values of \( \beta \) which minimize the square loss of the empirical cumulative probability distribution using some regularizer:

\begin{align}
&\min \left( \sum_{i=1}^\ell \left( y_i - \sum_{j=1}^\ell \sum_{n=1}^\kappa \beta_j^n k_n(x_i, x_j) \right)^2 + \lambda \sum_{i=1}^\ell \sum_{n=1}^\kappa \frac{1}{\gamma_n} \beta_i^n \right) \\
&\text{subject to} \quad \sum_{i=1}^\ell \sum_{n=1}^\kappa \beta_i^n = 1, \quad \beta_i \ge 0 \\
\end{align}

given a kernel function \( k(x,x') \) from the sigmoid family to approximate the cumulative probability distribution and it's derivative which we refer to as the cross-kernel \( \mathcal{K}(x,x') \) which we use to construct an estimate of the probability distribution:

\begin{align}
&k(x,x') = \frac{1}{1+e^{-\gamma(x-x')} } \\
&\mathcal{K}(x,x') = -\frac{\gamma}{2 + e^{\gamma(x-x')} + e^{-\gamma(x-x')} } 
\end{align}


\section{Results}
The architecture has been tested against several data sets.  In all cases the system parameters are left unchanged to eliminate the possbility of optimizing the system performance to best match the known outcomes.

\subsection{Eunite Competition Data}


\subsection{Santa Fe Data}


\subsection{CATS Benchmark Data}


\subsection{Results Summary}


\section{Further Research}

\subsection{Data Retention}
% this can probably be worked into the other 3 sections, but for now I'll put it here


\subsection{Ensemble System}
% more analysis of recent data, some type of transfer from short to long-term 'memory'

% heirarchical system?

% what parameters change?


\subsection{Weighted Influence}
% a weight term could be added to the regularizer to control the influence of specific sets.  This could provide more information in 'important' situations, which could be useful in motivated learning.


\section{Conclusion}
Eat it, bitches

\bibliographystyle{plain}
\bibliography{Research/research.bib}

\end{document}
